---
# You can also start simply with 'default'
theme: seriph
# random image from a curated Unsplash collection by Anthony
# like them? see https://unsplash.com/collections/94734566/slidev
background: https://cover.sli.dev
# some information about your slides (markdown enabled)
title: Welcome to Slidev
info: |
  ## Slidev Starter Template
  Presentation slides for developers.

  Learn more at [Sli.dev](https://sli.dev)
# apply unocss classes to the current slide
class: text-center
# https://sli.dev/features/drawing
drawings:
  persist: false
# slide transition: https://sli.dev/guide/animations.html#slide-transitions
transition: slide-left
# enable MDC Syntax: https://sli.dev/features/mdc
mdc: true

fonts:
  # basically the text
  sans: Robot
  # use with `font-serif` css class from UnoCSS
  serif: Robot Slab
  # for code blocks, inline code, etc.
  mono: Fira Code
---

# Introduction to LLMs and Their Applications

ZEYU LYU

<div @click="$slidev.nav.next" class="mt-12 py-1" hover:bg="white op-10">
  Press Space for next page <carbon:arrow-right />
</div>

<div class="abs-br m-6 text-xl"> 
  <button @click="$slidev.nav.openInEditor" title="Open in Editor" class="slidev-icon-btn">
    <carbon:edit />
  </button>
  <a href="https://github.com/lvzeyu/Tohoku_AIE_PBL" target="_blank" class="slidev-icon-btn">
    <carbon:logo-github />
  </a>
</div>

<!--
The last comment block of each slide will be treated as slide notes. It will be visible and editable in Presenter Mode along with the slide. [Read more in the docs](https://sli.dev/guide/syntax.html#notes)
-->

---
transition: fade-out
---

# Overview of Lectures

LLMs Basics

<v-clicks depth="2">

- **Lecture 1**

    - üí° High-level explanations of the fundamental concepts behind LLMs
    - ‚≠êÔ∏è Insights into the transformer architecture
    - üßë‚Äçüíª Application of LLMs using Transformers library and HuggingFace Hub

- **Lecture 2**

    - üìù Prompting and In-context Learning of LLMs
    - üîß Fine-tuning LLMs

- **Lecture 3** 
   
    - ü§ñ Applications of LLMs Agents

</v-clicks>

 

---
transition: slide-up
level: 2
---

# LLMs Basics

Journey of Language Models

<div class="flex justify-center">
  <img src="./image/NLP_history.png" width="800" />

</div>

<!--
Ëá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜÔºàNLPÔºâ„ÅØ„ÄÅ„Ç≥„É≥„Éî„É•„Éº„Çø„Åå‰∫∫Èñì„ÅÆË®ÄË™û„ÇíÁêÜËß£„ÄÅÁîüÊàê„ÄÅÂàÜÊûê„Åô„Çã„Åü„ÇÅ„ÅÆ‰∏ÄÈÄ£„ÅÆÊäÄË°ì„Åß„Åô„ÄÇÊñáÁ´†„Å™„Å©„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÅØ„Åô„Åπ„Å¶ÈùûÊßãÈÄ†Âåñ„Éá„Éº„Çø„Å´ÂàÜÈ°û„Åï„Çå„Åæ„Åô„ÄÇ„Ç≥„É≥„Éî„É•„Éº„Çø„Å´„Å®„Å£„Å¶„ÄÅ„Åì„ÅÆÁ®Æ„ÅÆ„Éá„Éº„Çø„ÅØË°®„ÇÑ„Éá„Éº„Çø„Éô„Éº„Çπ„ÅÆ„Çà„ÅÜ„Å´ÊòéÁ¢∫„Å™ÊßãÈÄ†„ÇÑË¶èÂâá„ÇíÊåÅ„Åü„Å™„ÅÑ„Åü„ÇÅ„ÄÅÂá¶ÁêÜ„ÅåÈùûÂ∏∏„Å´Âõ∞Èõ£„Åß„Åô„ÄÇ

Ëá™ÁÑ∂Ë®ÄË™ûÊäÄË°ì„ÅÆÁô∫Â±ï„ÅÆÂàùÊúüÊÆµÈöé„Åß„ÅØ„ÄÅÈö†„Çå„Éû„É´„Ç≥„Éï„É¢„Éá„É´ÔºàHMMÔºâ„ÄÅÁ∑öÂΩ¢„Çµ„Éù„Éº„Éà„Éô„ÇØ„Çø„Éº„Éû„Ç∑„É≥ÔºàSVMÔºâ„ÄÅ„É≠„Ç∏„Çπ„ÉÜ„Ç£„ÉÉ„ÇØÂõûÂ∏∞„Å™„Å©„ÅÆÁµ±Ë®àÁöÑÊ©üÊ¢∞Â≠¶Áøí„ÅÆÊâãÊ≥ï„Åå‰∏ªÊµÅ„Åß„Åó„Åü„ÄÇ

„Åì„Çå„Çâ„ÅÆÊâãÊ≥ï„ÅÆÁâπÂæ¥„ÅØ„ÄÅ‰∫∫Èñì„ÅåÊâãÂãï„ÅßÁâπÂæ¥Èáè„ÇíË®≠Ë®à„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çã„Åì„Å®„Åß„Åô„ÄÇ„Å§„Åæ„Çä„ÄÅ„Åæ„Åö„É¢„Éá„É´„Å´„Äå„Å©„ÅÆÂçòË™û„ÅåÈáçË¶Å„Åã„Äç„Äå„Å©„ÅÆ„Çà„ÅÜ„Å™ÊßãÈÄ†„ÅåÊÑèÂë≥„ÇíÊåÅ„Å§„Åã„Äç„ÇíÊïô„Åà„ÄÅ„Åù„ÅÆ‰∏ä„Åß„É¢„Éá„É´„Å´Ë¶èÂâáÊÄß„ÇíÂ≠¶Áøí„Åï„Åõ„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åó„Åü„ÄÇ

„Åì„ÅÆÁ®Æ„ÅÆÊâãÊ≥ï„ÅØÂ∞èË¶èÊ®°„Å™„Çø„Çπ„ÇØ„Åß„ÅØÈùûÂ∏∏„Å´ÊúâÂäπ„Åß„Åó„Åü„Åå„ÄÅ‰ªäÊó•„ÅÆ„Çà„ÅÜ„Å™Â§ßË¶èÊ®°„ÅßË§áÈõëÊÄß„ÅÆÈ´ò„ÅÑË®ÄË™û„Éá„Éº„Çø„Å´Áõ¥Èù¢„Åô„Çã„Å®„ÄÅÊäÄË°ìÁöÑ„Å™„Éú„Éà„É´„Éç„ÉÉ„ÇØ„ÅåÈú≤Âëà„Åô„Çã„Åì„Å®„ÅåÂ§ö„Åè„ÅÇ„Çä„Åæ„Åó„Åü„ÄÇ

2010Âπ¥‰ª£„Å´ÂÖ•„Çã„Å®„ÄÅ„Éá„Ç£„Éº„Éó„É©„Éº„Éã„É≥„Ç∞ÊäÄË°ì„ÅÆ„Éñ„É¨„Éº„ÇØ„Çπ„É´„Éº„ÅåËá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ„ÅÆÂàÜÈáé„Å´Êñ∞„Åü„Å™Áô∫Â±ï„ÅÆÂ•ëÊ©ü„Çí„ÇÇ„Åü„Çâ„Åó„Åæ„Åó„Åü„ÄÇ„Éá„Ç£„Éº„Éó„É©„Éº„Éã„É≥„Ç∞„ÅØ„ÄÅ‰∫∫Èñì„Å´„Çà„ÇãÁâπÂæ¥ÈáèË®≠Ë®à„Å´‰æùÂ≠ò„Åõ„Åö„ÄÅÁîü„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„Åã„ÇâÁõ¥Êé•Ë°®ÁèæÔºàrepresentationÔºâ„ÇíËá™ÂãïÁöÑ„Å´Â≠¶Áøí„Åô„Çã„Åü„ÇÅ„ÄÅË§áÈõë„Å™Ë®ÄË™û„Çø„Çπ„ÇØ„ÇíÂá¶ÁêÜ„Åô„ÇãËÉΩÂäõ„ÅåËëó„Åó„ÅèÂêë‰∏ä„Åó„Åæ„Åó„Åü„ÄÇ

2018Âπ¥„ÄÅGoogle„ÅØÁîªÊúüÁöÑ„Å™„É¢„Éá„É´„Åß„ÅÇ„ÇãBERT„ÇíÁô∫Ë°®„Åó„Åæ„Åó„Åü„ÄÇ„Åì„Çå„ÅØ„ÄÅ„Åæ„ÅöÂ§ßË¶èÊ®°„Å™„Ç≥„Éº„Éë„Çπ„ÅßÊ±éÁî®ÁöÑ„Å™Ë®ÄË™ûËÉΩÂäõ„Çí„Äå‰∫ãÂâçÂ≠¶Áøí„Äç„Åó„ÄÅ„Åù„ÅÆÂæå„ÄÅÂÖ∑‰ΩìÁöÑ„Å™„Çø„Çπ„ÇØ„Å´Âøú„Åò„Å¶„Äå„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Äç„ÇíË°å„ÅÜ„ÇÇ„ÅÆ„Åß„Åô„ÄÇ„Åì„ÅÆ„Äå‰∫ãÂâçÂ≠¶Áøí-„Éï„Ç°„Ç§„É≥„ÉÅ„É•„Éº„Éã„É≥„Ç∞„Äç„Å®„ÅÑ„ÅÜÈñãÁô∫„Éë„É©„ÉÄ„Ç§„É†„ÅØ„ÄÅNLP„É¢„Éá„É´„ÅÆÊßãÁØâÊñπÊ≥ï„ÇíÊ†πÊú¨ÁöÑ„Å´Â§â„Åà„ÄÅË®ìÁ∑¥„Ç≥„Çπ„Éà„ÇíÂ§ßÂπÖ„Å´ÂâäÊ∏õ„Åó„ÄÅ„Çø„Çπ„ÇØÈñì„ÅÆËª¢ÁßªËÉΩÂäõ„ÇÇÂêë‰∏ä„Åï„Åõ„Åæ„Åó„Åü„ÄÇ„Åì„ÅÆÊÆµÈöé„Åã„Çâ„ÄÅNLP„É¢„Éá„É´„ÅØÊñáËÑà„ÇíÁêÜËß£„Åó„ÄÅÂâçÂæå„ÅÆÊÑèÂë≥„ÇíÊçâ„Åà„ÇãËÉΩÂäõ„ÇíÂÇô„Åà„Çã„Çà„ÅÜ„Å´„Å™„Çä„ÄÅ„Çà„ÇäË§áÈõë„Å™Êé®Ë´ñ„ÄÅË≥™ÁñëÂøúÁ≠î„ÄÅÊÑüÊÉÖË™çË≠ò„Å™„Å©„ÅÆ„Çø„Çπ„ÇØ„ÇíÂá¶ÁêÜ„Åß„Åç„Çã„Çà„ÅÜ„Å´„Å™„Çä„Åæ„Åó„Åü„ÄÇ

2019Âπ¥‰ª•Èôç„ÅØGPT„ÄÅT5„Å™„Å©„ÄÅÊßò„ÄÖ„Å™„Éê„É™„Ç®„Éº„Ç∑„Éß„É≥„ÅåÁôªÂ†¥„Åó„ÄÅ„Çà„ÇäÂ§ßË¶èÊ®°„Å™„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß‰∫ãÂâçÂ≠¶Áøí„Åï„Çå„ÄÅÊÄßËÉΩ„ÅåÈ£õË∫çÁöÑ„Å´Âêë‰∏ä„Åó„Åæ„Åó„Åü„ÄÇLLM„ÅØ„ÄÅ„Éë„É©„É°„Éº„ÇøÊï∞„Å®Â≠¶Áøí„Éá„Éº„Çø„ÅÆË¶èÊ®°„ÇíÊã°Â§ß„Åô„Çã„Åì„Å®„Åß„ÄÅÈ©ö„Åè„Åπ„ÅçËÉΩÂäõ„ÇíÁô∫ÊèÆ„Åô„Çã„Çà„ÅÜ„Å´„Å™„Çä„Åæ„Åó„Åü„ÄÇÊï∞ÂçÉÂÑÑ„ÄÅ„Åï„Çâ„Å´„ÅØÂÖÜÂçò‰Ωç„ÅÆ„Éë„É©„É°„Éº„Çø„ÇíÊåÅ„Å§„É¢„Éá„É´„ÅåÁôªÂ†¥„Åó„ÄÅ„Åì„Çå„Å´„Çà„Çä„ÄÅ„Çà„ÇäË§áÈõë„Å™Êé®Ë´ñ„ÄÅÂ§öË®ÄË™ûÂØæÂøú„ÄÅ„Éó„É≠„Ç∞„É©„Éü„É≥„Ç∞„Ç≥„Éº„Éâ„ÅÆÁîüÊàê„ÄÅÂâµÈÄ†ÁöÑ„Å™ÊñáÁ´†‰ΩúÊàê„Å™„Å©„ÄÅÂπÖÂ∫É„ÅÑ„Çø„Çπ„ÇØ„Åß‰∫∫Èñì„É¨„Éô„É´„ÄÅ„ÅÇ„Çã„ÅÑ„ÅØ„Åù„Çå„ÇíË∂Ö„Åà„ÇãÊÄßËÉΩ„ÇíÁ§∫„Åô„Çà„ÅÜ„Å´„Å™„Çä„Åæ„Åó„Åü„ÄÇ
-->



---
transition: slide-up
level: 2
---

# LLMs Basics

What is LLMs?

<v-clicks depth="2">

- LLMs are deep neural networks trained on massive amounts of text data, designed to understand, generate, and respond to human-like text.
    - ***Large* refers to both the model's size in terms of parameters and the dataset**
        - LLMs often involves billions of parameters
        - LLMs were trained on a large amount of texts
    - **LLMs are capable of *generating text***
        - LLMs are often referred to as a form of generative AI.
        - LLMs can handle various task including answering questions, writing essays, translating languages through text generation
    - **LLMs utilize an architecture called the *Transformer***
        - Transformer  enables efficient processing of language by capturing complex patterns and relationships across long text sequences

</v-clicks>

<!--
Â§ßË¶èÊ®°„Å™„ÉÜ„Ç≠„Çπ„Éà„Éá„Éº„Çø„ÅßË®ìÁ∑¥„Åï„Çå„ÅüÊ∑±Â±§„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ: LLM„ÅØ„ÄÅ„Ç§„É≥„Çø„Éº„Éç„ÉÉ„Éà‰∏ä„ÅÆËÜ®Â§ß„Å™Êõ∏Á±ç„ÄÅË®ò‰∫ã„ÄÅ„Ç¶„Çß„Éñ„Çµ„Ç§„Éà„Å™„Å©„ÄÅ„ÅÇ„Çâ„ÇÜ„ÇãÁ®ÆÈ°û„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„Éá„Éº„Çø„ÇíÂ≠¶Áøí„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åì„ÅÆ„ÄåÂ§ßË¶èÊ®°„Åï„Äç„Åå„ÄÅ‰∫∫Èñì„ÅÆ„Çà„ÅÜ„Å™„ÉÜ„Ç≠„Çπ„Éà„ÇíÁêÜËß£„Åó„ÄÅÁîüÊàê„Åó„ÄÅÂøúÁ≠î„Åô„ÇãËÉΩÂäõ„ÅÆÂü∫Áõ§„Å®„Å™„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ

„ÄåÂ§ßË¶èÊ®°ÔºàLargeÔºâ„Äç„ÅÆÊÑèÂë≥:

„Éë„É©„É°„Éº„ÇøÊï∞: „É¢„Éá„É´„ÅÆÂÜÖÈÉ®ÊßãÈÄ†„ÇíÊßãÊàê„Åô„Çã„Äå„Éë„É©„É°„Éº„Çø„Äç„ÅÆÊï∞„ÅåÈùûÂ∏∏„Å´Â§ö„ÅÑ„Åì„Å®„ÇíÊåá„Åó„Åæ„Åô„ÄÇÊï∞ÂÑÑ„ÄÅÊï∞ÂçÅÂÑÑ„ÄÅ„Åï„Çâ„Å´„ÅØÊï∞ÂÖÜ„Å´Âèä„Å∂„Éë„É©„É°„Éº„Çø„ÇíÊåÅ„Å§„É¢„Éá„É´„ÇÇÂ≠òÂú®„Åó„Åæ„Åô„ÄÇ„Éë„É©„É°„Éº„Çø„ÅåÂ§ö„ÅÑ„Åª„Å©„ÄÅ„Çà„ÇäË§áÈõë„Å™„Éë„Çø„Éº„É≥„ÇÑÈñ¢‰øÇÊÄß„ÇíÂ≠¶Áøí„Åß„Åç„Åæ„Åô„ÄÇ
Ë®ìÁ∑¥„Éá„Éº„Çø„Çª„ÉÉ„Éà: Â≠¶Áøí„Å´Áî®„ÅÑ„Çâ„Çå„Çã„ÉÜ„Ç≠„Çπ„Éà„Éá„Éº„Çø„ÅÆÈáè„ÅåËÜ®Â§ß„Åß„ÅÇ„Çã„Åì„Å®„ÇíÊåá„Åó„Åæ„Åô„ÄÇ„Åì„Çå„Å´„Çà„Çä„ÄÅÂ§öÊßò„Å™Ë®ÄË™ûË°®Áèæ„ÇÑÁü•Ë≠ò„ÇíÁç≤Âæó„Åó„Åæ„Åô„ÄÇ
„ÉÜ„Ç≠„Çπ„ÉàÁîüÊàêËÉΩÂäõÔºàGenerative AIÔºâ: LLM„ÅØ„ÄÅ‰∏é„Åà„Çâ„Çå„Åü„Éó„É≠„É≥„Éó„Éà„ÇÑÊñáËÑà„Å´Âü∫„Å•„ÅÑ„Å¶„ÄÅÊñ∞„Åó„ÅÑ„ÉÜ„Ç≠„Çπ„Éà„ÇíÁîüÊàê„Åô„ÇãËÉΩÂäõ„Å´ÂÑ™„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åù„ÅÆ„Åü„ÇÅ„ÄÅ„ÄåÁîüÊàêAIÔºàGenerative AIÔºâ„Äç„ÅÆ‰∏ÄÁ®Æ„Å®„Åó„Å¶Â∫É„ÅèË™çË≠ò„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇÊñáÁ´†‰ΩúÊàê„ÄÅË¶ÅÁ¥Ñ„ÄÅÁøªË®≥„ÄÅ„Ç≥„Éº„ÉâÁîüÊàê„Å™„Å©„ÄÅÂ§öÂ≤ê„Å´„Çè„Åü„Çã„ÉÜ„Ç≠„Çπ„ÉàÁîüÊàê„Çø„Çπ„ÇØ„ÇíÂÆüË°å„Åß„Åç„Åæ„Åô„ÄÇ

Transformer„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÅÆÂà©Áî®: LLM„ÅÆ„Åª„Å®„Çì„Å©„ÅØ„ÄÅ„ÄåTransformerÔºà„Éà„É©„É≥„Çπ„Éï„Ç©„Éº„Éû„ÉºÔºâ„Äç„Å®Âëº„Å∞„Çå„ÇãÁâπÂà•„Å™„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÇíÂü∫Áõ§„Å®„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇTransformer„ÅØ„ÄÅ„ÉÜ„Ç≠„Çπ„ÉàÂÜÖ„ÅÆÂçòË™ûÈñì„ÅÆÈñ¢‰øÇÊÄßÔºàÊñáËÑàÔºâ„ÇíÂäπÁéáÁöÑ„Å´Êçâ„Åà„Çã„Åì„Å®„Åå„Åß„Åç„Çã„Åü„ÇÅ„ÄÅÈï∑Êñá„ÅÆÁêÜËß£„ÇÑÁîüÊàê„Å´„Åä„ÅÑ„Å¶ÈùûÂ∏∏„Å´È´ò„ÅÑÊÄßËÉΩ„ÇíÁô∫ÊèÆ„Åó„Åæ„Åô„ÄÇBERT„ÇÑGPT„Å®„ÅÑ„Å£„ÅüÊúâÂêç„Å™„É¢„Éá„É´„ÇÇ„ÄÅ„Åì„ÅÆTransformer„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÇíÊé°Áî®„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ

Ë¶Å„Åô„Çã„Å´„ÄÅLLM„ÅØËÜ®Â§ß„Å™„ÉÜ„Ç≠„Çπ„Éà„Éá„Éº„Çø„Åã„ÇâÂ≠¶Áøí„Åó„ÄÅTransformer„Å®„ÅÑ„ÅÜÂäπÁéáÁöÑ„Å™ÊßãÈÄ†„Çí‰Ωø„Å£„Å¶„ÄÅ‰∫∫Èñì„ÅÆ„Çà„ÅÜ„Å´„ÉÜ„Ç≠„Çπ„Éà„ÇíÁêÜËß£„Åó„ÄÅ„Åù„Åó„Å¶ÂâµÈÄ†ÁöÑ„Å´„ÉÜ„Ç≠„Çπ„Éà„ÇíÁîüÊàê„Åô„ÇãËÉΩÂäõ„ÇíÊåÅ„Å§„ÄÅÈùûÂ∏∏„Å´Â§ßË¶èÊ®°„Å™AI„É¢„Éá„É´„Åß„ÅÇ„Çã„Å®Ë®Ä„Åà„Åæ„Åô„ÄÇ 
-->


---
transition: slide-up
level: 2
---

# LLMs Basics

What is GPT?

<v-clicks depth="2">

- G: **G**enerative model
    - GPT are trained to predict the next word in a sequence
- P: **P**re-trained
    - GPT models undergo an extensive training phase on a massive dataset of text
    - Pre-trained allows model to learn a vast amount of linguistic patterns, facts, reasoning abilities, and general knowledge.
- T: **T**ransformer
    - GPT models are built on the Transformer architecture


</v-clicks>

---
transition: slide-up
level: 2
---

# LLMs Basics

Language Models(LMs)

- LMs  designed to understand and generate human language by assigning probabilities to sequences of words
    - **Setup**: Assume a vocabulary of words  
  $$
  V = \{W_1, W_2, W_3, \ldots, W_n\}
  $$

    - **Data**: Given a training set of example sentences

    - **Goal**: Estimate a probability distribution  
  $$
  \sum_{x \in V^{*}} p(x) = 1
  $$

<!--
Language Models (LMs)„ÅØ„ÄÅ‰∫∫Èñì„ÅÆË®ÄË™û„ÇíÁêÜËß£„ÅóÁîüÊàê„Åô„Çã„Åü„ÇÅ„Å´„ÄÅÂçòË™û„ÅÆ„Ç∑„Éº„Ç±„É≥„Çπ„Å´Á¢∫Áéá„ÇíÂâ≤„ÇäÂΩì„Å¶„Çã„Çà„ÅÜ„Å´Ë®≠Ë®à„Åï„Çå„Åü„É¢„Éá„É´„Åß„Åô„ÄÇ

„ÅÇ„ÇãÂçòË™û„ÅÆÂæå„Å´„Å©„ÅÆ„Çà„ÅÜ„Å™ÂçòË™û„ÅåÁ∂ö„Åè„Åã„ÄÅ„ÅÇ„Çã„ÅÑ„ÅØ„ÅÇ„ÇãÂçòË™û„ÅÆ‰∏¶„Å≥ÂÖ®‰Ωì„Åå„Å©„Çå„Åè„Çâ„ÅÑ„ÅÆÁ¢∫Áéá„ÅßÁô∫Áîü„Åô„Çã„Åã„ÇíÊï∞Â≠¶ÁöÑ„Å´„É¢„Éá„É´Âåñ„Åó„Åæ„Åô„ÄÇ„Åì„Çå„Å´„Çà„Çä„ÄÅË®ÄË™û„ÅÆË¶èÂâáÊÄß„ÇÑ„Éë„Çø„Éº„É≥„ÇíÂ≠¶Áøí„Åó„ÄÅÊñ∞„Åó„ÅÑ„ÉÜ„Ç≠„Çπ„Éà„ÇíÁîüÊàê„Åó„Åü„Çä„ÄÅÊó¢Â≠ò„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„ÅÆÂ¶•ÂΩìÊÄß„ÇíË©ï‰æ°„Åó„Åü„Çä„Åô„Çã„Åì„Å®„ÅåÂèØËÉΩ„Å´„Å™„Çä„Åæ„Åô„ÄÇ

- Ë®ÄË™û„É¢„Éá„É´„ÅåÊâ±„ÅÜ„Åì„Å®„ÅÆ„Åß„Åç„Çã„Åô„Åπ„Å¶„ÅÆ„É¶„Éã„Éº„ÇØ„Å™ÂçòË™û„ÅÆÈõÜÂêà„ÇíÁî®ÊÑè„Åó„Åæ„Åô„ÄÇ
- Ë®ìÁ∑¥„Çª„ÉÉ„Éà„ÅÆ‰æãÊñá: „Åì„ÅÆ„Éá„Éº„Çø„Åã„Çâ„ÄÅ„Å©„ÅÆÂçòË™û„Åå„Å©„ÅÆÂçòË™û„ÅÆÂæå„Å´Áèæ„Çå„Çã„Åì„Å®„ÅåÂ§ö„ÅÑ„Åã„ÄÅ„Å©„ÅÆ„Çà„ÅÜ„Å™ÂçòË™û„ÅÆ‰∏¶„Å≥„ÅåËá™ÁÑ∂„Åã„ÄÅ„Å®„ÅÑ„Å£„ÅüÊÉÖÂ†±„ÇíÊäΩÂá∫„Åó„Åæ„Åô„ÄÇ
- ÁõÆÊ®ô„ÅØÁ¢∫ÁéáÂàÜÂ∏É„ÅÆÊé®ÂÆö: Ë®ÄË™û„É¢„Éá„É´„ÅÆÁ©∂Ê•µ„ÅÆÁõÆÊ®ô„ÅØ„ÄÅÂèØËÉΩ„Å™„Åô„Åπ„Å¶„ÅÆÂçòË™û„ÅÆ„Ç∑„Éº„Ç±„É≥„Çπ„Å´ÂØæ„Åô„ÇãÁ¢∫ÁéáÂàÜÂ∏É$p(x)$„ÇíÊé®ÂÆö„Åô„Çã„Åì„Å®„Åß„Åô„ÄÇË™ûÂΩô V „Åã„ÇâÊßãÊàê„Åï„Çå„Çã„Åô„Åπ„Å¶„ÅÆÂèØËÉΩ„Å™ÂçòË™û„ÅÆ„Ç∑„Éº„Ç±„É≥„Çπ„ÅÆÈõÜÂêà„ÇíË°®„Åó„Åæ„Åô„ÄÇ„Åì„ÅÆÂºè„ÅØ„ÄÅ„Åô„Åπ„Å¶„ÅÆÂèØËÉΩ„Å™ÂçòË™û„Ç∑„Éº„Ç±„É≥„Çπ„ÅÆÁ¢∫Áéá„ÇíÂêàË®à„Åô„Çã„Å®1„Å´„Å™„Çã„ÄÇ„Å§„Åæ„Çä„ÄÅ„É¢„Éá„É´„ÅØ„ÅÇ„Çâ„ÇÜ„ÇãÂçòË™û„ÅÆ‰∏¶„Å≥„Å´ÂØæ„Åó„Å¶„ÄÅ„Åù„Çå„Åå„Å©„Çå„Å†„Åë„Äå„ÅÇ„Çä„Åù„ÅÜ„Åã„Äç„Å®„ÅÑ„ÅÜÁ¢∫Áéá„ÇíÂâ≤„ÇäÂΩì„Å¶„Çã„Åì„Å®„Åå„Åß„Åç„Çã„Çà„ÅÜ„Å´„Å™„Çã„Åì„Å®„ÇíÁõÆÊåá„Åó„Åæ„Åô„ÄÇ
-->


---
transition: slide-up
level: 2
---

# LLMs Basics

Next-word Prediction Task

<div class="flex justify-center">
  <img src="./image/next.png" width="800" />
</div>


---
transition: slide-up
level: 2
---

# LLMs Basics

Text Generation

<div class="flex justify-center">
  <img src="./image/generative.png" width="550" />
</div>


---
transition: slide-up
level: 2
---

# LLMs Basics

Transformer

<div grid="~ cols-2 gap-4 items-start">

<div>

<v-clicks depth="2">

- The Transformer has significantly advanced the field of NLP and is applied across a wide range of large language models.
- **Self-attention** mechanism allows the model to compute the relevance of each element in a sequence and use this information to understand the context. 
    - Handling of **Long-Range Dependencies**
    - **Parallelization** for training truly "large" language models
</v-clicks>

</div>

<div class="flex justify-center">
  <img src="./image/transformer_frame.png" alt="„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÂõ≥" width="800" />
</div>

</div>

---
transition: slide-up
level: 2
---

# LLMs Basics

Seq2seq

<div grid="~ cols-2 gap-4 items-start">

<div>

<v-clicks depth="2">

- The Transformer is a type of **Seq2seq (Sequence-to-Sequence)** model
- A Seq2Seq model is an architecture designed to transform an input sequence into an output sequence

    - **Encoder**: Process the input sequence and compress its information into a fixed-size representation called the context vector
    
    - **Decoder**: Use encoded information (context vector) to generate the output sequence.

</v-clicks>

</div>

<div class="flex justify-center">
  <img src="./image/encoder-and-decoder-transformers-blocks.png" alt="„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÂõ≥" width="180" />
</div>

</div>


<!--
Transformer„ÅØSeq2Seq (Sequence-to-Sequence) „É¢„Éá„É´„ÅÆ‰∏ÄÁ®Æ„Åß„ÅÇ„Çã.

Seq2Seq„É¢„Éá„É´„ÅØ„ÄÅÂÖ•Âäõ„Ç∑„Éº„Ç±„É≥„Çπ„ÇíÂá∫Âäõ„Ç∑„Éº„Ç±„É≥„Çπ„Å´Â§âÊèõ„Åô„Çã„Çà„ÅÜ„Å´Ë®≠Ë®à„Åï„Çå„Åü„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„Åß„ÅÇ„Çã

- ÂæìÊù•„ÅÆSeq2Seq„É¢„Éá„É´Ôºà„É™„Ç´„É¨„É≥„Éà„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ (RNN) „ÇÑÁï≥„ÅøËæº„Åø„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ (CNN) „Çí„Éô„Éº„Çπ„Å®„Åó„Åü„ÇÇ„ÅÆÔºâ„Å®„ÅØÁï∞„Å™„Çã„ÄÅ„Çà„ÇäÂäπÁéáÁöÑ„ÅßÈ´òÊÄßËÉΩ„Å™„É°„Ç´„Éã„Ç∫„É†ÔºàËá™Â∑±Ê≥®ÊÑèÊ©üÊßã„Å™„Å©Ôºâ„ÇíÊé°Áî®„Åó„Å¶„ÅÑ„ÇãÁÇπ„ÅåÁâπÂæ¥„Åß„Åô„ÄÇ

- Encoder („Ç®„É≥„Ç≥„Éº„ÉÄ):ÂÖ•Âäõ„Ç∑„Éº„Ç±„É≥„Çπ„ÇíÂá¶ÁêÜ„Åó„ÄÅ„Åù„ÅÆÊÉÖÂ†±„ÇíÂõ∫ÂÆö„Çµ„Ç§„Ç∫„ÅÆË°®ÁèæÔºà„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„Éô„ÇØ„Éà„É´Ôºâ„Å´ÂúßÁ∏Æ„Åô„Çã „Ç®„É≥„Ç≥„Éº„ÉÄ„ÅÆÂΩπÂâ≤„ÅØ„ÄÅÂÖ•Âäõ„Åï„Çå„ÅüÊñáÁ´†„ÇÑ„Éá„Éº„ÇøÂÖ®‰Ωì„ÅÆÊÑèÂë≥ÂÜÖÂÆπ„ÇíÊçâ„Åà„ÄÅ„Åù„Çå„Çí„Éá„Ç≥„Éº„ÉÄ„ÅåÂà©Áî®„Åß„Åç„ÇãÂΩ¢„Å´Â§âÊèõ„Åô„Çã„Åì„Å®„Åß„Åô„ÄÇ„Åì„ÅÆ„ÄåÂõ∫ÂÆö„Çµ„Ç§„Ç∫„ÅÆË°®Áèæ„Äç„ÅØ„ÄÅÂÖ•Âäõ„Ç∑„Éº„Ç±„É≥„Çπ„ÅÆ„Åô„Åπ„Å¶„ÅÆÈáçË¶Å„Å™ÊÉÖÂ†±„ÅåÂáùÁ∏Æ„Åï„Çå„Åü„ÇÇ„ÅÆ„Å®Ë¶ã„Å™„Åï„Çå„Åæ„Åô„ÄÇÂæìÊù•„ÅÆSeq2Seq„É¢„Éá„É´„Åß„ÅØ„ÄÅ„É™„Ç´„É¨„É≥„Éà„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆÊúÄÂæå„ÅÆÈö†„ÇåÁä∂ÊÖã„Åå„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„Éô„ÇØ„Éà„É´„Å®„Åó„Å¶‰Ωø„Çè„Çå„Çã„Åì„Å®„ÅåÂ§ö„Åã„Å£„Åü„Åß„Åô„ÄÇ
Decoder („Éá„Ç≥„Éº„ÉÄ):

- „Ç®„É≥„Ç≥„Éº„Éâ„Åï„Çå„ÅüÊÉÖÂ†±Ôºà„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„Éô„ÇØ„Éà„É´Ôºâ„Çí‰ΩøÁî®„Åó„Å¶Âá∫Âäõ„Ç∑„Éº„Ç±„É≥„Çπ„ÇíÁîüÊàê„Åô„Çã „Éá„Ç≥„Éº„ÉÄ„ÅØ„ÄÅ„Ç®„É≥„Ç≥„Éº„ÉÄ„Åå‰ΩúÊàê„Åó„Åü„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„Éô„ÇØ„Éà„É´„ÇíÂü∫„Å´„ÄÅ„Çø„Éº„Ç≤„ÉÉ„ÉàË®ÄË™û„ÇÑÁõÆÁöÑ„Å®„Åô„ÇãÂá∫ÂäõÂΩ¢Âºè„ÅßÂçòË™û„Çí‰∏Ä„Å§„Åö„Å§ÁîüÊàê„Åó„Å¶„ÅÑ„Åç„Åæ„Åô„ÄÇÁîüÊàê„ÅÆÈöõ„Å´„ÅØ„ÄÅ„Åù„Çå„Åæ„Åß„Å´ÁîüÊàê„Åï„Çå„ÅüÂçòË™û„ÇÇËÄÉÊÖÆ„Å´ÂÖ•„Çå„ÄÅÊñáËÑà„Å´Âêà„Å£„ÅüÊ¨°„ÅÆÂçòË™û„ÇíÈÅ∏Êäû„Åó„Åæ„Åô„ÄÇ
-->


---
transition: slide-up
level: 2
---

# LLMs Basics

Seq2seq

<video controls width="700" style="display: block; margin-left: auto; margin-right: auto;">
  <source src="./image/seq2seq_training_with_target.mp4" type="video/mp4">
</video>



---
transition: slide-up
level: 2
---

# LMs before the Transformer

N-gram Language Model

- **üîÅ Markov assumption**: The probability of a word depends only on the previous \( N-1 \) words.  
  $$
  P(w_n \mid w_{1:n-1}) = P(w_n \mid w_{n-N+1:n-1})
  $$

- For a sentence *students opened their __[blank]__*  
  $$
  P(w) = \frac{\text{count(opened their } w)}{\text{count(opened their)}}
  $$

<div grid="~ cols-2 gap-4 items-start">

<div>

<v-clicks depth="2">

- An N-gram is a contiguous sequence of $N$ items from a given text.
- N-gram language model refers to a probabilistic model that can estimate the probability of a word given the $n-1$ previous words.
</v-clicks>

</div>

<div class="flex justify-center">
  <img src="./image/n-gram.png" alt="„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÂõ≥" width="800" />
</div>

</div>



<!--
„Éû„É´„Ç≥„Éï‰ªÆÂÆö (Markov assumption): 

$$P(w_n \mid w_{1:n-1}) = P(w_n \mid w_{n-N+1:n-1})$$
„Åì„ÅÆ‰ªÆÂÆö„ÅØ„ÄÅN-gram„É¢„Éá„É´„ÅÆÊ†πÂππ„Çí„Å™„Åô„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
- Â∑¶Ëæ∫ $P(w_n \mid w_{1:n-1})$ „ÅØ„ÄÅ„ÄåÁèæÂú®„ÅÆÂçòË™û $w_n$ „ÅÆÁ¢∫Áéá„Åå„ÄÅ„Åù„ÅÆÂâç„Å´Áèæ„Çå„Åü„Åô„Åπ„Å¶„ÅÆÂçòË™û $w_1, \ldots, w_{n-1}$ „Å´‰æùÂ≠ò„Åô„Çã„Äç„Å®„ÅÑ„ÅÜÁêÜÊÉ≥ÁöÑ„Å™Ôºà„Åó„Åã„ÅóË®àÁÆó„ÅåÈùûÂ∏∏„Å´Âõ∞Èõ£„Å™ÔºâÁä∂Ê≥Å„ÇíÁ§∫„Åó„Åæ„Åô„ÄÇ
- Âè≥Ëæ∫ $P(w_n \mid w_{n-N+1:n-1})$ „ÅØ„ÄÅ„ÄåÁèæÂú®„ÅÆÂçòË™û $w_n$ „ÅÆÁ¢∫Áéá„ÅØ„ÄÅ„Åù„ÅÆÁõ¥Ââç„ÅÆ $N-1$ ÂÄã„ÅÆÂçòË™û $w_{n-N+1}, \ldots, w_{n-1}$ „ÅÆ„Åø„Å´‰æùÂ≠ò„Åô„Çã„Äç„Å®„ÅÑ„ÅÜÁ∞°Áï•Âåñ„Åï„Çå„Åü‰ªÆÂÆö„Åß„Åô„ÄÇ

„Å§„Åæ„Çä„ÄÅ**ÈÅéÂéª„ÅÆ„Åô„Åπ„Å¶„ÅÆÊñáËÑà„ÇíËÄÉÊÖÆ„Åô„Çã„ÅÆ„Åß„ÅØ„Å™„Åè„ÄÅÁõ¥Ëøë„ÅÆ‰∏ÄÂÆöÊï∞„ÅÆÂçòË™û„ÅÆ„Åø„ÇíÊñáËÑà„Å®„Åó„Å¶ËÄÉÊÖÆ„Åô„Çã**„Å®„ÅÑ„ÅÜËÄÉ„ÅàÊñπ„Åß„Åô„ÄÇ„Åì„ÅÆ„Äå‰∏ÄÂÆö„ÅÆÊï∞„Äç„Åå $N-1$ „Åß„Åô„ÄÇ$N$ „ÅÆÂÄ§„ÅåÂ§ß„Åç„ÅÑ„Åª„Å©„ÄÅ„Çà„ÇäÂ§ö„Åè„ÅÆÊñáËÑà„ÇíËÄÉÊÖÆ„Åß„Åç„Åæ„Åô„Åå„ÄÅ„É¢„Éá„É´„ÅÆË§áÈõëÊÄß„Å®„Éá„Éº„Çø„Çπ„Éë„Éº„ÇπÊÄß„ÅÆÂïèÈ°å„ÅåÂ¢óÂ§ß„Åó„Åæ„Åô„ÄÇ

$$P(w) = \frac{\text{count(opened their } w)}{\text{count(opened their)}}$$

„Åì„Çå„ÅØ„ÄÅ**3-gramÔºà„Åæ„Åü„ÅØTrigramÔºâ„É¢„Éá„É´** „ÅÆÂ†¥Âêà„ÅÆ‰æã„Åß„Åô„ÄÇ„Åì„ÅÆÂºè„ÅØ„ÄÅÁ©∫ÁôΩ„Å´ÂΩì„Å¶„ÅØ„Åæ„ÇãÂçòË™û $w$ „ÅåÊù•„ÇãÁ¢∫Áéá„ÇíË®àÁÆó„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ

- $\text{count(opened their } w)$ „ÅØ„ÄÅÂ≠¶Áøí„Éá„Éº„Çø‰∏≠„Å´„Äåopened their„Äç„ÅÆÂæå„Å´ÁâπÂÆö„ÅÆÂçòË™û $w$ „ÅåÁ∂ö„ÅèÂõûÊï∞„ÇíÁ§∫„Åó„Åæ„Åô„ÄÇ‰æã„Åà„Å∞„ÄÅ„Äåopened their books„Äç„Å®„ÅÑ„ÅÜ„Éï„É¨„Éº„Ç∫„Åå‰ΩïÂõûÂá∫Áèæ„Åó„Åü„Åã„ÄÇ
- $\text{count(opened their)}$ „ÅØ„ÄÅÂ≠¶Áøí„Éá„Éº„Çø‰∏≠„Å´„Äåopened their„Äç„Å®„ÅÑ„ÅÜ„Éï„É¨„Éº„Ç∫„Åå‰ΩïÂõûÂá∫Áèæ„Åó„Åü„Åã„ÇíÁ§∫„Åó„Åæ„Åô„ÄÇ

„Åó„Åü„Åå„Å£„Å¶„ÄÅ„Åì„ÅÆÂºè„ÅØ„ÄÅ**„Äåopened their„Äç„Å®„ÅÑ„ÅÜÊñáËÑà„Åå‰∏é„Åà„Çâ„Çå„Åü„Å®„Åç„Å´„ÄÅÊ¨°„Å´ÂçòË™û $w$ „ÅåÁ∂ö„ÅèÊù°‰ª∂‰ªò„ÅçÁ¢∫Áéá** „ÇíÊé®ÂÆö„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
-->



---
transition: slide-up
level: 2
---

# LMs before the Transformer

N-gram Language Model

<v-clicks depth="2">

- ‚ö†Ô∏èComputational complexity 
    - As $N$ increases (to capture more context), the number of possible unique N-grams grows exponentially with the size of the vocabulary. 
    - Example: For a vocabulary of 50,000 words, a 5-gram model would theoretically need to store $(50,000)^5$ counts

- ‚ö†Ô∏èLow generalization 
    - Language often has dependencies that span many words: The fixed-size context is a fundamental limitation.
    - Inability to capture long-range dependencies means N-gram models cannot truly understand the deep syntactic or semantic relationships in sentences.
</v-clicks>

<!--

- **‚ö†Ô∏è Ë®àÁÆó‰∏ä„ÅÆË§áÈõëÊÄß (Computational complexity)**
    - **$N$„ÅåÂ¢óÂä†„Åô„Çã„Å®Ôºà„Çà„ÇäÂ§ö„Åè„ÅÆÊñáËÑà„ÇíÊçâ„Åà„Çã„Åü„ÇÅÔºâ„ÄÅÂèØËÉΩ„Å™„É¶„Éã„Éº„ÇØ„Å™N-gram„ÅÆÊï∞„ÅåË™ûÂΩô„Çµ„Ç§„Ç∫„Å´ÂØæ„Åó„Å¶ÊåáÊï∞Èñ¢Êï∞ÁöÑ„Å´Â¢óÂä†„Åô„Çã„ÄÇ**
        „Åì„Çå„ÅØN-gram„É¢„Éá„É´„ÅÆÊúÄ„ÇÇÊ∑±Âàª„Å™ÂïèÈ°å„ÅÆ‰∏Ä„Å§„Åß„Åô„ÄÇN-gram„ÅØÂü∫Êú¨ÁöÑ„Å´„ÄÅË®ìÁ∑¥„Éá„Éº„Çø‰∏≠„Å´Áèæ„Çå„ÇãÈÄ£Á∂ö„Åô„ÇãÂçòË™û„ÅÆ‰∏¶„Å≥ÔºàN-gramÔºâ„ÅÆÈ†ªÂ∫¶„ÇíÊï∞„Åà‰∏ä„Åí„Å¶Á¢∫Áéá„ÇíÊé®ÂÆö„Åó„Åæ„Åô„ÄÇ$N$„ÅåÂ§ß„Åç„Åè„Å™„Çã„Å®„ÄÅËÄÉ„Åà„Çâ„Çå„ÇãÂçòË™û„ÅÆÁµÑ„ÅøÂêà„Çè„Åõ„ÅåÁàÜÁô∫ÁöÑ„Å´Â¢ó„Åà„Çã„Åü„ÇÅ„ÄÅÊ¨°„ÅÆÂïèÈ°å„ÅåÁîü„Åò„Åæ„Åô„ÄÇ
    - **‰æãÔºöË™ûÂΩô„Çµ„Ç§„Ç∫„Åå50,000Ë™û„ÅÆÂ†¥Âêà„ÄÅ5-gram„É¢„Éá„É´„ÅØÁêÜË´ñ‰∏ä $(50,000)^5$ „ÅÆ„Ç´„Ç¶„É≥„Éà„Çí‰øùÂ≠ò„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çã„ÄÇ**
        „Åì„Çå„ÅØ $3.125 \times 10^{23}$ „Å®„ÅÑ„ÅÜÂ§©ÊñáÂ≠¶ÁöÑ„Å™Êï∞Â≠ó„Å´„Å™„Çä„Åæ„Åô„ÄÇÁèæÂÆüÁöÑ„Å´„ÅØ„ÄÅÂ≠¶Áøí„Éá„Éº„Çø„Å´Â≠òÂú®„Åô„ÇãN-gram„ÅÆ„Åø„Çí‰øùÂ≠ò„Åó„Åæ„Åô„Åå„ÄÅ„Åù„Çå„Åß„ÇÇ„Åù„ÅÆÊï∞„ÅØËÜ®Â§ß„Å´„Å™„Çä„ÄÅ‰ª•‰∏ã„ÅÆÂïèÈ°å„ÇíÂºï„ÅçËµ∑„Åì„Åó„Åæ„Åô„ÄÇ
        * **„É°„É¢„É™„Å®„Çπ„Éà„É¨„Éº„Ç∏„ÅÆÊ∂àË≤ª:** ËÜ®Â§ß„Å™Êï∞„ÅÆN-gram„Å®„Åù„ÅÆ„Ç´„Ç¶„É≥„Éà„Çí‰øùÂ≠ò„Åô„Çã„Åü„ÇÅ„Å´„ÄÅÈÄîÊñπ„ÇÇ„Å™„ÅÑÈáè„ÅÆ„É°„É¢„É™„Å®„Çπ„Éà„É¨„Éº„Ç∏„ÅåÂøÖË¶Å„Å´„Å™„Çä„Åæ„Åô„ÄÇ
        * **Ë®àÁÆóÊôÇÈñì:** Á¢∫Áéá„ÅÆË®àÁÆó„ÇÑÊõ¥Êñ∞„Å´ÊôÇÈñì„Åå„Åã„Åã„Çä„Åæ„Åô„ÄÇ
        * **„Éá„Éº„Çø„Çπ„Éë„Éº„ÇπÊÄß (Data Sparsity):** „Åì„Çå„ÅåÊúÄ„ÇÇÂ§ß„Åç„Å™ÂïèÈ°å„Åß„Åô„ÄÇ„Åü„Å®„ÅàÂ≠¶Áøí„Éá„Éº„Çø„ÅåÂ§ß„Åç„Åè„Å¶„ÇÇ„ÄÅ$N$„ÅåÂ§ß„Åç„Åè„Å™„Çã„Å®„ÄÅ„Åª„Å®„Çì„Å©„ÅÆN-gram„ÅÆÁµÑ„ÅøÂêà„Çè„Åõ„ÅØ‰∏ÄÂ∫¶„ÇÇÂá∫Áèæ„Åó„Å™„ÅÑÔºà„Ç´„Ç¶„É≥„Éà„Åå„Çº„É≠„Å´„Å™„ÇãÔºâÂèØËÉΩÊÄß„ÅåÈ´ò„Åæ„Çä„Åæ„Åô„ÄÇ„Åì„ÅÆÂïèÈ°å„ÅØ„Äå„Çº„É≠È†ªÂ∫¶ÂïèÈ°å„Äç„Å®Âëº„Å∞„Çå„ÄÅN-gram„É¢„Éá„É´„ÅÆÊ±éÂåñËÉΩÂäõ„ÇíËëó„Åó„Åè‰Ωé‰∏ã„Åï„Åõ„Åæ„Åô„ÄÇ‰æã„Åà„Å∞„ÄÅ„Äåunusual blue car„Äç„Å®„ÅÑ„ÅÜ„Éï„É¨„Éº„Ç∫„ÅåË®ìÁ∑¥„Éá„Éº„Çø„Å´„Å™„Åè„Å¶„ÇÇ„ÄÅÂÄã„ÄÖ„ÅÆÂçòË™û„ÅØÈ†ªÁπÅ„Å´Áèæ„Çå„Çã„Åã„ÇÇ„Åó„Çå„Åæ„Åõ„Çì„ÄÇ„Åó„Åã„Åó„ÄÅ3-gram„É¢„Éá„É´„Åß„ÅØ„Åù„ÅÆ„Éï„É¨„Éº„Ç∫„ÅÆÁ¢∫Áéá„ÅØ„Çº„É≠„Å®Ë©ï‰æ°„Åï„Çå„Å¶„Åó„Åæ„ÅÑ„Åæ„Åô„ÄÇ„Åì„Çå„ÇíÁ∑©Âíå„Åô„Çã„Åü„ÇÅ„Å´„ÄÅ„Çπ„É†„Éº„Ç∏„É≥„Ç∞ÔºàÂπ≥ÊªëÂåñÔºâ„Å™„Å©„ÅÆÊâãÊ≥ï„ÅåÁî®„ÅÑ„Çâ„Çå„Åæ„Åô„Åå„ÄÅÊ†πÊú¨ÁöÑ„Å™Ëß£Ê±∫„Å´„ÅØ„Å™„Çä„Åæ„Åõ„Çì„ÄÇ

- ‚ö†Ô∏è Ê±éÂåñËÉΩÂäõ„ÅÆ‰Ωé„Åï (Low generalization)
    - **Ë®ÄË™û„ÅØ„Åó„Å∞„Åó„Å∞„ÄÅÂ§ö„Åè„ÅÆÂçòË™û„Å´„Åæ„Åü„Åå„Çã‰æùÂ≠òÈñ¢‰øÇ„ÇíÊåÅ„Å§ÔºöÂõ∫ÂÆö„Çµ„Ç§„Ç∫„ÅÆÊñáËÑà„ÅØÊ†πÊú¨ÁöÑ„Å™Âà∂Èôê„Åß„ÅÇ„Çã„ÄÇ**
        N-gram„É¢„Éá„É´„ÅØ„Éû„É´„Ç≥„Éï‰ªÆÂÆö„Å´Âü∫„Å•„ÅÑ„Å¶„ÅÑ„Çã„Åü„ÇÅ„ÄÅ$N-1$ÂÄã„ÅÆÁõ¥Ââç„ÅÆÂçòË™û„Åó„ÅãËÄÉÊÖÆ„Åó„Åæ„Åõ„Çì„ÄÇ„Åì„Çå„ÅØ„ÄÅÊñáÁ´†ÂÖ®‰Ωì„ÅÆÊÑèÂë≥„ÇÑÊñáÊ≥ïÊßãÈÄ†„ÇíÁêÜËß£„Åô„Çã‰∏ä„ÅßËá¥ÂëΩÁöÑ„Å™Âà∂Á¥Ñ„Å®„Å™„Çä„Åæ„Åô„ÄÇ‰æã„Åà„Å∞„ÄÅ‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å™Êñá„ÇíËÄÉ„Åà„Å¶„Åø„Åæ„Åó„Çá„ÅÜ„ÄÇ
        „ÄåÂ§™ÈÉé„Åå„ÄÅÊò®Êó•ÂÖ¨Âúí„ÅßÈÅä„Çì„Åß„ÅÑ„ÅüËä±Â≠ê„Å´„ÄÅË©±„Åó„Åã„Åë„Åü„ÄÇ„Äç
        „Åì„ÅÆÊñá„Åß„ÅØ„ÄÅ„ÄåÂ§™ÈÉé„Äç„Å®„ÅÑ„ÅÜ‰∏ªË™û„Åå„ÄåË©±„Åó„Åã„Åë„Åü„Äç„Å®„ÅÑ„ÅÜÂãïË©û„Å´ÂØæÂøú„Åó„Å¶„Åä„Çä„ÄÅ‰∏°ËÄÖ„ÅÆÈñì„Å´„ÅØÂ§ö„Åè„ÅÆÂçòË™û„ÅåÊåü„Åæ„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åó„Åã„Åó„ÄÅN-gram„É¢„Éá„É´„ÅØ $N-1$ „ÅÆÁ™ì„ÇíË∂Ö„Åà„Åü‰æùÂ≠òÈñ¢‰øÇ„ÇíÊçâ„Åà„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åõ„Çì„ÄÇ
    - **Èï∑Ë∑ùÈõ¢‰æùÂ≠òÈñ¢‰øÇ„ÇíÊçâ„Åà„ÇãËÉΩÂäõ„Åå„Å™„ÅÑ„Åì„Å®„ÅØ„ÄÅN-gram„É¢„Éá„É´„ÅåÊñá‰∏≠„ÅÆÊ∑±„ÅÑÁµ±Ë™ûÁöÑÔºàÊßãÊñáÁöÑÔºâ„Åæ„Åü„ÅØÊÑèÂë≥ÁöÑÈñ¢‰øÇ„ÇíÁúü„Å´ÁêÜËß£„Åß„Åç„Å™„ÅÑ„Åì„Å®„ÇíÊÑèÂë≥„Åô„Çã„ÄÇ**
        „Åì„Çå„ÅØ‰∏äË®ò„ÅÆÁÇπ„Å®Èñ¢ÈÄ£„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ‰∫∫Èñì„ÅåË®ÄË™û„ÇíÁêÜËß£„Åô„ÇãÈöõ„Å´„ÅØ„ÄÅÊñáÈ†≠„Åã„ÇâÊñáÊú´„Åæ„Åß„ÄÅ„ÅÇ„Çã„ÅÑ„ÅØÊÆµËêΩÂÖ®‰Ωì„Å®„ÅÑ„Å£„ÅüÂ∫ÉÁØÑÂõ≤„ÅÆÊñáËÑà„ÇíËÄÉÊÖÆ„Åó„Å¶ÊÑèÂë≥„ÇíËß£Èáà„Åó„Åæ„Åô„ÄÇN-gram„É¢„Éá„É´„ÅØ„Åì„ÅÆ„Çà„ÅÜ„Å™Èï∑Ë∑ùÈõ¢„ÅÆ‰æùÂ≠òÈñ¢‰øÇÔºàlong-range dependenciesÔºâ„ÇíÊâ±„ÅÜ„Åì„Å®„Åå„Åß„Åç„Å™„ÅÑ„Åü„ÇÅ„ÄÅÊñáÊ≥ïÁöÑ„Å´Ê≠£„Åó„ÅÑ„ÅåÊÑèÂë≥ÁöÑ„Å´‰∏çËá™ÁÑ∂„Å™ÊñáÁ´†„ÇíÁîüÊàê„Åó„Åü„Çä„ÄÅË§áÈõë„Å™Ë≥™ÂïèÂøúÁ≠î„ÇÑË¶ÅÁ¥Ñ„Å®„ÅÑ„Å£„Åü„Çø„Çπ„ÇØ„ÅßËã¶Êà¶„Åó„Åü„Çä„Åó„Åæ„Åô„ÄÇ
-->


---
transition: slide-up
level: 2
---

# Language Model with RNNs

Recurrent Neural Networks (RNNs)

<div grid="~ cols-2 gap-4 items-start">

<div>

<v-clicks depth="2">

- Compute an output $y_t$ for an input $x_t$ requires activation value for the hidden layer $h_t$.
    - $h_t$ is calculated based on the input $x_t$ and the hidden layer from the previous time step $h_{t-1}$.
</v-clicks>

</div>

<div class="flex justify-center">
  <img src="./image/rnn_component.png" alt="„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÂõ≥" width="800" />
</div>

</div>


<div grid="~ cols-2 gap-4 items-start">

<div>

<v-clicks depth="2">

- Computation at time $t$ requires the value of the hidden layer from time $t ‚àí 1$ mandates an incremental inference algorithm that proceeds from the start of the sequence to the end. 
</v-clicks>

</div>

<div class="flex justify-center">
  <img src="./image/rnn_sequence.png" alt="„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÂõ≥" width="800" />
</div>

</div>


<!--

„É™„Ç´„É¨„É≥„Éà„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÔºàRNNsÔºâ„ÅØ„ÄÅ„Ç∑„Éº„Ç±„É≥„Çπ„Éá„Éº„Çø„ÇíÊâ±„ÅÜ„Åü„ÇÅ„Å´Ë®≠Ë®à„Åï„Çå„Åü„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆ‰∏ÄÁ®Æ„Åß„ÄÅË®ÄË™û„É¢„Éá„É´„ÅÆÂàÜÈáé„ÅßÂ§ß„Åç„Å™ÈÄ≤Ê≠©„Çí„ÇÇ„Åü„Çâ„Åó„Åæ„Åó„Åü„ÄÇ

* **ÂÖ•Âäõ $x_t$ „Å´ÂØæ„Åó„Å¶Âá∫Âäõ $y_t$ „ÇíË®àÁÆó„Åô„Çã„Å´„ÅØ„ÄÅÈö†„ÇåÂ±§„ÅÆÊ¥ªÊÄßÂåñÂÄ§ $h_t$ „ÅåÂøÖË¶Å„Åß„ÅÇ„Çã„ÄÇ**
    * **$h_t$ „ÅØ„ÄÅÂÖ•Âäõ $x_t$ „Å®Ââç„ÅÆ„Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„Éó„ÅÆÈö†„ÇåÂ±§ $h_{t-1}$ „Å´Âü∫„Å•„ÅÑ„Å¶Ë®àÁÆó„Åï„Çå„Çã„ÄÇ**

    „Åì„ÅÆ2„Å§„ÅÆÁÇπ„ÅØ„ÄÅRNN„ÅÆÊúÄ„ÇÇÈáçË¶Å„Å™ÁâπÂæ¥„Åß„ÅÇ„Çã„ÄåÂÜçÂ∏∞ÊÄßÔºàRecurrenceÔºâ„Äç„Å®„ÄåË®òÊÜ∂ÔºàMemoryÔºâ„Äç„ÇíË™¨Êòé„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ

    * **$x_t$**: ÁèæÂú®„ÅÆ„Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„ÉóÔºàÊôÇÂàª $t$Ôºâ„Å´„Åä„Åë„ÇãÂÖ•Âäõ„Éá„Éº„Çø„Åß„Åô„ÄÇË®ÄË™û„É¢„Éá„É´„ÅÆÊñáËÑà„Åß„ÅØ„ÄÅ„Åì„Çå„ÅØÈÄöÂ∏∏„ÄÅÁèæÂú®„ÅÆÂçòË™û„ÅÆÂüã„ÇÅËæº„ÅøÔºàÂçòË™û„Éô„ÇØ„Éà„É´Ôºâ„Å´Áõ∏ÂΩì„Åó„Åæ„Åô„ÄÇ
    * **$y_t$**: ÁèæÂú®„ÅÆ„Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„Éó„Å´„Åä„Åë„ÇãÂá∫Âäõ„Åß„Åô„ÄÇË®ÄË™û„É¢„Éá„É´„Åß„ÅØ„ÄÅÊ¨°„Å´Á∂ö„ÅèÂçòË™û„ÅÆÁ¢∫ÁéáÂàÜÂ∏É„ÇÑ„ÄÅÁèæÂú®„ÅÆÂçòË™û„Åå‰Ωï„Åß„ÅÇ„Çã„Åã„ÅÆ‰∫àÊ∏¨„Å™„Å©„Å´„Å™„Çä„Åæ„Åô„ÄÇ
    * **$h_t$**: ÁèæÂú®„ÅÆ„Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„Éó„Å´„Åä„Åë„Çã„ÄåÈö†„ÇåÁä∂ÊÖãÔºàhidden stateÔºâ„Äç„Åæ„Åü„ÅØ„ÄåÈö†„ÇåÂ±§„ÅÆÊ¥ªÊÄßÂåñÂÄ§„Äç„Åß„Åô„ÄÇ„Åì„Çå„ÅØ„ÄÅ**„Åì„Çå„Åæ„Åß„ÅÆ„Åô„Åπ„Å¶„ÅÆÂÖ•Âäõ„Ç∑„Éº„Ç±„É≥„ÇπÔºà$x_1, \ldots, x_t$Ôºâ„ÅÆÊÉÖÂ†±„ÇíÈõÜÁ¥Ñ„Åó„Åü„ÄåË®òÊÜ∂„Äç** „ÅÆ„Çà„ÅÜ„Å™ÂΩπÂâ≤„ÇíÊûú„Åü„Åó„Åæ„Åô„ÄÇ
    * **$h_{t-1}$**: Ââç„ÅÆ„Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„ÉóÔºàÊôÇÂàª $t-1$Ôºâ„ÅÆÈö†„ÇåÁä∂ÊÖã„Åß„Åô„ÄÇ$h_t$ „ÅÆË®àÁÆó„Å´ $h_{t-1}$ „ÅåÂê´„Åæ„Çå„Çã„Åì„Å®„Åß„ÄÅ„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅØÈÅéÂéª„ÅÆÊÉÖÂ†±„ÇíÁèæÂú®„ÅÆÂá¶ÁêÜ„Å´„ÄåÂÜçÂ∏∞ÁöÑ„Å´„ÄçÂà©Áî®„Åß„Åç„Çã„Çà„ÅÜ„Å´„Å™„Çä„Åæ„Åô„ÄÇ

* **RNN„ÅÆÂà©ÁÇπ:** RNN„ÅØ„ÄÅÈö†„ÇåÁä∂ÊÖã $h_t$ „Çí‰ªã„Åó„Å¶ÈÅéÂéª„ÅÆ„Åô„Åπ„Å¶„ÅÆÊÉÖÂ†±„Çí„ÄåË®òÊÜ∂„Äç„Åó„ÄÅ„Åù„Çå„ÇíÁèæÂú®„ÅÆË®àÁÆó„Å´Âà©Áî®„Åó„Åæ„Åô„ÄÇÁêÜË´ñÁöÑ„Å´„ÅØ„ÄÅ„Ç∑„Éº„Ç±„É≥„Çπ„ÅÆÊúÄÂàù„Åã„Çâ„ÅÆÊÉÖÂ†±„Çí‰øùÊåÅ„ÅóÁ∂ö„Åë„Çã„Åì„Å®„Åå„Åß„Åç„ÄÅ„Åì„Çå„Å´„Çà„ÇäN-gram„Åß„ÅØ‰∏çÂèØËÉΩ„Å†„Å£„Åü**Èï∑Ë∑ùÈõ¢‰æùÂ≠òÈñ¢‰øÇ**„ÇíÂ≠¶Áøí„Åó„ÄÅ„Çà„ÇäÊ∑±„ÅÑÊñáËÑà„ÇíÁêÜËß£„Åô„Çã„Åì„Å®„ÅåÂèØËÉΩ„Å´„Å™„Çä„Åæ„Åô„ÄÇ„Åì„Çå„Å´„Çà„Çä„ÄÅ„Çà„ÇäÊñáÊ≥ïÁöÑ„Å´Ê≠£„Åó„Åè„ÄÅÊÑèÂë≥ÁöÑ„Å´‰∏ÄË≤´„Åó„ÅüÊñáÁ´†„ÇíÁîüÊàê„Åó„Åü„Çä„ÄÅË§áÈõë„Å™ÊÑèÂë≥ÁêÜËß£„Çí‰º¥„ÅÜ„Çø„Çπ„ÇØ„ÇíÂá¶ÁêÜ„Åó„Åü„Çä„Åß„Åç„Çã„Çà„ÅÜ„Å´„Å™„Çä„Åæ„Åô„ÄÇ

 **N-gram„ÅÆÈôêÁïå:** N-gram„ÅØÂõ∫ÂÆöÈï∑„ÅÆ„Ç¶„Ç£„É≥„Éâ„Ç¶„Åß„Åó„ÅãÊñáËÑà„ÇíË°®Áèæ„Åß„Åç„Åæ„Åõ„Çì„ÄÇN„ÇíÂ§ß„Åç„Åè„Åô„Çã„Å®„ÄÅË®àÁÆóÈáè„Å®„Éá„Éº„Çø„Çπ„Éë„Éº„ÇπÊÄß„ÅÆÂïèÈ°å„ÅåÁàÜÁô∫ÁöÑ„Å´Â¢óÂä†„Åó„Åæ„Åô„ÄÇ
    * **RNN„ÅÆÂà©ÁÇπ:** RNN„ÅØ„ÄÅÈö†„ÇåÁä∂ÊÖã„ÇíÈÄö„Åò„Å¶ÂèØÂ§âÈï∑„ÅÆÊñáËÑà„ÇíÂãïÁöÑ„Å´Ë°®Áèæ„Åó„Åæ„Åô„ÄÇ„Åì„Çå„Å´„Çà„Çä„ÄÅÂçòË™û„ÅÆÂá∫Áèæ„Éë„Çø„Éº„É≥„Å†„Åë„Åß„Å™„Åè„ÄÅÊñá„ÅÆÊßãÈÄ†„ÇÑÊÑèÂë≥„Å®„ÅÑ„Å£„Åü„Çà„ÇäÊäΩË±°ÁöÑ„Å™ÁâπÂæ¥„ÇíÂ≠¶Áøí„Åß„Åç„Åæ„Åô„ÄÇN-gram„ÅÆ„Çà„ÅÜ„Å´ $N$ „ÇíÂ¢ó„ÇÑ„Åô„Åì„Å®„Å´„Çà„ÇãË®àÁÆóÈáè„ÅÆÁàÜÁô∫ÁöÑ„Å™Â¢óÂä†„ÇÑ„Éá„Éº„Çø„Çπ„Éë„Éº„ÇπÊÄß„ÅÆÂïèÈ°å„Å´ÊÇ©„Åæ„Åï„Çå„Çã„Åì„Å®„ÅåÂ∞ë„Å™„Åè„Å™„Çä„Åæ„Åô„ÄÇ

* **ÊôÇÂàª $t$ „Å´„Åä„Åë„ÇãË®àÁÆó„Å´„ÅØ„ÄÅÊôÇÂàª $t-1$ „Å´„Åä„Åë„ÇãÈö†„ÇåÂ±§„ÅÆÂÄ§„ÅåÂøÖË¶Å„Åß„ÅÇ„Çã„Åü„ÇÅ„ÄÅ„Ç∑„Éº„Ç±„É≥„Çπ„ÅÆÂÖàÈ†≠„Åã„ÇâÊúÄÂæå„Åæ„ÅßÂá¶ÁêÜ„ÇíÈÄ≤„ÇÅ„ÇãÈÄêÊ¨°ÁöÑ„Å™Êé®Ë´ñ„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÅåÂøÖÈ†à„Å®„Å™„Çã„ÄÇ**

    „Åì„Çå„ÅØRNN„ÅÆ„ÇÇ„ÅÜ‰∏Ä„Å§„ÅÆÈáçË¶Å„Å™ÁâπÊÄß„Åß„ÅÇ„Çä„ÄÅÂêåÊôÇ„Å´Â§ß„Åç„Å™Âà∂Á¥Ñ„Åß„ÇÇ„ÅÇ„Çä„Åæ„Åô„ÄÇ
    * **ÈÄêÊ¨°ÁöÑ„Å™Ë®àÁÆó (Sequential Computation):** ÂêÑ„Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„Éó„ÅÆË®àÁÆó„ÅØ„ÄÅÂâç„ÅÆ„Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„Éó„ÅÆË®àÁÆóÁµêÊûú„Å´‰æùÂ≠ò„Åô„Çã„Åü„ÇÅ„ÄÅ‰∏¶ÂàóÂåñ„ÅåÈùûÂ∏∏„Å´Âõ∞Èõ£„Åß„Åô„ÄÇ„Åì„Çå„ÅØ„ÄÅÁâπ„Å´Èï∑„ÅÑ„Ç∑„Éº„Ç±„É≥„Çπ„ÇíÊâ±„ÅÜÂ†¥Âêà„Å´„ÄÅË®ìÁ∑¥„ÇÑÊé®Ë´ñ„ÅÆÈÄüÂ∫¶„ÅåÈÅÖ„Åè„Å™„ÇãÂéüÂõ†„Å®„Å™„Çä„Åæ„Åô„ÄÇ
    * **ÊÉÖÂ†±„ÅÆ„Éï„É≠„Éº:** Èö†„ÇåÁä∂ÊÖã„ÅåÊ¨°„ÄÖ„Å´Êõ¥Êñ∞„Åï„Çå„Å¶„ÅÑ„Åè„Åì„Å®„Åß„ÄÅÁêÜË´ñÁöÑ„Å´„ÅØ„Ç∑„Éº„Ç±„É≥„Çπ„ÅÆÊúÄÂàù„Åã„Çâ„ÅÆÊÉÖÂ†±„Çí„ÄåË®òÊÜ∂„Äç„ÅóÁ∂ö„Åë„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇ„Åì„Çå„Å´„Çà„Çä„ÄÅN-gram„É¢„Éá„É´„ÅÆÂõ∫ÂÆöÈï∑„ÅÆÊñáËÑà„ÅÆÂà∂Á¥Ñ„Çí‰πó„ÇäË∂ä„Åà„ÄÅÈï∑Ë∑ùÈõ¢‰æùÂ≠òÈñ¢‰øÇ„Çí„ÅÇ„ÇãÁ®ãÂ∫¶Êçâ„Åà„Çã„Åì„Å®„ÅåÂèØËÉΩ„Å´„Å™„Çä„Åæ„Åó„Åü„ÄÇ„Åó„Åã„Åó„ÄÅÂÆüÈöõ„Å´„ÅØ„ÄÅÂãæÈÖçÊ∂àÂ§±„ÉªÁàÜÁô∫ÂïèÈ°å„Å´„Çà„Çä„ÄÅÈùûÂ∏∏„Å´Èï∑„ÅÑ„Ç∑„Éº„Ç±„É≥„Çπ„ÅÆÂàù„ÇÅ„ÅÆÊÉÖÂ†±„Çí‰øùÊåÅ„ÅóÁ∂ö„Åë„Çã„Åì„Å®„ÅØÂõ∞Èõ£„Åß„Åó„ÅüÔºà„Åì„ÅÆÂïèÈ°å„ÅØLSTM„ÇÑGRU„Å´„Çà„Å£„Å¶Á∑©Âíå„Åï„Çå„Åæ„Åó„ÅüÔºâ„ÄÇ

„Åæ„Å®„ÇÅ„Çã„Å®„ÄÅRNN„ÅØ„ÄåË®òÊÜ∂„Äç„ÇíÊåÅ„Å§„Åì„Å®„ÅßN-gram„É¢„Éá„É´„ÅÆË™≤È°å„ÇíÂÖãÊúç„Åó„ÄÅ„Çà„ÇäËá™ÁÑ∂„Å™Ë®ÄË™ûÂá¶ÁêÜ„ÇíÂèØËÉΩ„Å´„Åó„Åæ„Åó„Åü„Åå„ÄÅ„Åù„ÅÆÈÄêÊ¨°ÁöÑ„Å™ÊÄßË≥™„ÇÜ„Åà„Å´Ë®àÁÆóÂäπÁéá„Å®ÈùûÂ∏∏„Å´Èï∑„ÅÑÊñáËÑà„ÅÆÊâ±„ÅÑ„Å´ÈôêÁïå„Åå„ÅÇ„Çä„Åæ„Åó„Åü„ÄÇ„Åì„Çå„Åå„ÄÅAttentionÊ©üÊßã„ÇÑTransformer„Å®„ÅÑ„Å£„Åü„ÄÅ„Çà„Çä‰∏¶ÂàóÂåñ„Å´ÈÅ©„Åó„Åü„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÅåÁôªÂ†¥„Åô„ÇãËÉåÊôØ„Å®„Å™„Çä„Åæ„Åô„ÄÇ
-->


---
transition: slide-up
level: 2
---

# Language Model with RNNs

Recurrent Neural Networks (RNNs)

<v-switch>
      <template #1>
        <img src="./image/1-min.png" alt="Image 1" style="max-width: 650px;">
      </template>
      <template #2>
        <img src="./image/2-min.png" alt="Image 2" style="max-width: 650px;">
      </template>
      <template #3>
        <img src="./image/3-min.png" alt="Image 3" style="max-width: 650px;">
      </template>
      <template #4>
        <img src="./image/4-min.png" alt="Image 4" style="max-width: 650px;">
      </template>
      <template #5>
        <img src="./image/5-min.png" alt="Image 5" style="max-width: 650px;">
      </template>
      <template #6>
        <img src="./image/6-min.png" alt="Image 6" style="max-width: 650px;">
      </template>
      <template #7>
        <img src="./image/7-min.png" alt="Image 7" style="max-width: 650px;">
      </template>
      <template #8>
        <img src="./image/8-min.png" alt="Image 8" style="max-width: 650px;">
      </template>
    </v-switch>

---
transition: slide-up
level: 2
---

# Language Model with RNNs

Recurrent Neural Networks (RNNs)

<v-clicks depth="2">

- Advantages of RNN LMs
    - **Contextual Understanding**: RNNs can capture longer dependencies in the text when making predictions.
    - **Dynamic Computation**: RNNs can handle input sequences of varying lengths without needing to predefine a fixed size.
- Disadvantages of RNN LMs: In a encoder-decoder model with RNNs LMs, the hidden state of the last time step represents absolutely everything about the meaning of the source text.
    - Sequential nature of RNNs means recurrent computation is slow.
    - Information at the beginning of the sentence, especially for long sentences, may not be equally well represented in the context vector.

</v-clicks>


<div class="flex justify-center">
  <img src="./image/rnn_problem.png" alt="„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÂõ≥" width="500" />
</div>



<!--



**RNN„ÇíÁî®„ÅÑ„ÅüË®ÄË™û„É¢„Éá„É´„ÅÆÂà©ÁÇπ (Advantages of RNN LMs)**

1.  **ÊñáËÑàÁêÜËß£ (Contextual Understanding):**
    * **RNNs„ÅØ„ÄÅ‰∫àÊ∏¨„ÇíË°å„ÅÜÈöõ„Å´„ÉÜ„Ç≠„Çπ„Éà‰∏≠„ÅÆ„Çà„ÇäÈï∑„ÅÑ‰æùÂ≠òÈñ¢‰øÇ„ÇíÊçâ„Åà„Çã„Åì„Å®„Åå„Åß„Åç„Çã„ÄÇ**
        „Åì„Çå„ÅØN-gram„É¢„Éá„É´„Å®„ÅÆÊúÄÂ§ß„ÅÆÂ∑ÆÂà•Âåñ„Éù„Ç§„É≥„Éà„Åß„Åô„ÄÇRNN„ÅØ„ÄÅÈö†„ÇåÁä∂ÊÖãÔºà$h_t$Ôºâ„ÇíÈÄö„Åò„Å¶ÈÅéÂéª„ÅÆÊÉÖÂ†±„Çí„ÄåË®òÊÜ∂„Äç„Åó„ÄÅ„Åù„Çå„ÇíÁèæÂú®„ÅÆÂçòË™û„ÅÆ‰∫àÊ∏¨„Å´Âà©Áî®„Åó„Åæ„Åô„ÄÇÁêÜË´ñ‰∏ä„ÅØ„ÄÅ„Ç∑„Éº„Ç±„É≥„Çπ„ÅÆ‰ªªÊÑè„ÅÆÈï∑„Åï„ÅÆÊÉÖÂ†±„Çí‰øùÊåÅ„Åß„Åç„Çã„Åü„ÇÅ„ÄÅÊñá„ÅÆÊúÄÂàù„Å´Âá∫„Å¶„Åç„ÅüÂçòË™û„ÅåÊñá„ÅÆÂæåÂçä„ÅÆÂçòË™û„ÅÆÊÑèÂë≥„Å´ÂΩ±Èüø„Çí‰∏é„Åà„Çã„Çà„ÅÜ„Å™„ÄåÈï∑Ë∑ùÈõ¢‰æùÂ≠òÈñ¢‰øÇ„Äç„ÇíN-gram„Çà„Çä„ÇÇ„ÅØ„Çã„Åã„Å´„ÅÜ„Åæ„ÅèÊçâ„Åà„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇ„Åì„Çå„Å´„Çà„Çä„ÄÅ„Çà„ÇäÊñáËÑà„Å´Âç≥„Åó„Åü„ÄÅËá™ÁÑ∂„Åß‰∏ÄË≤´ÊÄß„ÅÆ„ÅÇ„Çã„ÉÜ„Ç≠„Çπ„Éà„ÇíÁîüÊàê„Åó„Åü„Çä„ÄÅË§áÈõë„Å™ÊÑèÂë≥„ÇíÊåÅ„Å§ÊñáÁ´†„ÇíÁêÜËß£„Åó„Åü„Çä„Åô„Çã„Åì„Å®„ÅåÂèØËÉΩ„Å´„Å™„Çä„Åæ„Åô„ÄÇ

2.  **ÂãïÁöÑ„Å™Ë®àÁÆó (Dynamic Computation):**
    * **RNNs„ÅØ„ÄÅÂõ∫ÂÆö„Çµ„Ç§„Ç∫„Çí‰∫ãÂâç„Å´ÂÆöÁæ©„Åô„Çã„Åì„Å®„Å™„Åè„ÄÅÂèØÂ§âÈï∑„ÅÆÂÖ•Âäõ„Ç∑„Éº„Ç±„É≥„Çπ„ÇíÂá¶ÁêÜ„Åß„Åç„Çã„ÄÇ**
        N-gram„É¢„Éá„É´„ÅØ„ÄÅÁâπÂÆö„ÅÆ $N$ „ÅÆÂÄ§Ôºà‰æãÔºö3-gram„Å™„Çâ3ÂçòË™ûÔºâ„Å´Âõ∫ÂÆö„Åï„Çå„Åæ„Åô„ÄÇ„Åó„Åã„Åó„ÄÅRNN„ÅØÈÄêÊ¨°ÁöÑ„Å´Âá¶ÁêÜ„ÇíÈÄ≤„ÇÅ„Çã„Åü„ÇÅ„ÄÅÂÖ•Âäõ„Åï„Çå„Çã„Ç∑„Éº„Ç±„É≥„Çπ„ÅÆÈï∑„Åï„Åå„ÅÑ„Åè„Çâ„Åß„ÅÇ„Å£„Å¶„ÇÇ„ÄÅ„Åù„ÅÆÈï∑„Åï„Å´Âøú„Åò„Å¶Ë®àÁÆó„ÇíÁ∂ôÁ∂ö„Åß„Åç„Åæ„Åô„ÄÇ„Åì„Çå„Å´„Çà„Çä„ÄÅÊñáÁ´†„ÅÆÈï∑„Åï„Åå‰∏çÊèÉ„ÅÑ„Å™ÂÆüÈöõ„ÅÆË®ÄË™û„Éá„Éº„Çø„Å´ÊüîËªü„Å´ÂØæÂøú„Åß„Åç„ÄÅÊ©üÊ¢∞ÁøªË®≥„ÅÆ„Çà„ÅÜ„Å´Áï∞„Å™„ÇãÈï∑„Åï„ÅÆÊñá„ÇíÊâ±„ÅÜ„Çø„Çπ„ÇØ„Å´„ÇÇËá™ÁÑ∂„Å´ÈÅ©Áî®„Åß„Åç„Åæ„Åô„ÄÇ


**RNN„ÇíÁî®„ÅÑ„ÅüË®ÄË™û„É¢„Éá„É´„ÅÆÊ¨†ÁÇπ (Disadvantages of RNN LMs)**

1.  **ÊÉÖÂ†±„ÅÆÂúßÁ∏Æ„Å®„Éú„Éà„É´„Éç„ÉÉ„ÇØ (Information Compression and Bottleneck):**
    * **RNN„ÇíÁî®„ÅÑ„Åü„Ç®„É≥„Ç≥„Éº„ÉÄ„Éª„Éá„Ç≥„Éº„ÉÄ„É¢„Éá„É´„Å´„Åä„ÅÑ„Å¶„ÄÅÊúÄÂæå„ÅÆ„Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„Éó„ÅÆÈö†„ÇåÁä∂ÊÖã„Åå„ÇΩ„Éº„Çπ„ÉÜ„Ç≠„Çπ„Éà„ÅÆ„ÅÇ„Çâ„ÇÜ„ÇãÊÑèÂë≥„ÇíÂÆåÂÖ®„Å´Ë°®Áèæ„Åô„Çã„Åì„Å®„Å´„Å™„Çã„ÄÇ**
        „Åì„Çå„ÅØ„ÄÅRNN„Éô„Éº„Çπ„ÅÆSeq2Seq„É¢„Éá„É´„Å´„Åä„Åë„Çã**„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„Éô„ÇØ„Éà„É´Ôºà„Åæ„Åü„ÅØÂõ∫ÂÆö„Çµ„Ç§„Ç∫Ë°®ÁèæÔºâ„ÅÆ„Éú„Éà„É´„Éç„ÉÉ„ÇØÂïèÈ°å**„ÇíÊåá„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Ç®„É≥„Ç≥„Éº„ÉÄ„ÅØÂÖ•Âäõ„Ç∑„Éº„Ç±„É≥„ÇπÂÖ®‰Ωì„ÇíË™≠„ÅøËæº„Åø„ÄÅ„Åù„ÅÆÊÉÖÂ†±„ÇíÂçò‰∏Ä„ÅÆÈö†„ÇåÁä∂ÊÖã„Éô„ÇØ„Éà„É´ÔºàÈÄöÂ∏∏„ÅØÊúÄÂæå„ÅÆÈö†„ÇåÁä∂ÊÖãÔºâ„Å´„ÄåÂúßÁ∏Æ„Äç„Åó„Åæ„Åô„ÄÇ„Åì„ÅÆÂçò‰∏Ä„Éô„ÇØ„Éà„É´„Åå„ÄÅ„Éá„Ç≥„Éº„ÉÄ„Åå„Çø„Éº„Ç≤„ÉÉ„Éà„Ç∑„Éº„Ç±„É≥„Çπ„ÇíÁîüÊàê„Åô„Çã„Åü„ÇÅ„Å´‰ΩøÁî®„Åß„Åç„ÇãÂîØ‰∏Ä„ÅÆÊÉÖÂ†±Ê∫ê„Å®„Å™„Çä„Åæ„Åô„ÄÇ
        * **ÂïèÈ°åÁÇπ:** Áâπ„Å´Èï∑„ÅÑÂÖ•Âäõ„Ç∑„Éº„Ç±„É≥„Çπ„ÅÆÂ†¥Âêà„ÄÅ„Åô„Åπ„Å¶„ÅÆÈáçË¶Å„Å™ÊÉÖÂ†±„Çí‰∏Ä„Å§„ÅÆÂõ∫ÂÆö„Çµ„Ç§„Ç∫„ÅÆ„Éô„ÇØ„Éà„É´„Å´ÂáùÁ∏Æ„Åô„Çã„Åì„Å®„ÅØÈùûÂ∏∏„Å´Âõ∞Èõ£„Åß„Åô„ÄÇÊÉÖÂ†±„ÅåÂ§±„Çè„Çå„Åü„Çä„ÄÅÈáçË¶Å„Å™ÊÉÖÂ†±„ÅåËñÑ„Åæ„Å£„Åü„Çä„Åô„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ„Åì„Çå„ÅØ„ÄÅÈï∑„ÅÑÊñá„ÇíÁêÜËß£„Åó„Åü„Çä„ÄÅË©≥Á¥∞„Å™ÊÉÖÂ†±„Çí‰øùÊåÅ„Åó„Åü„Çä„Åô„ÇãËÉΩÂäõ„ÇíÂà∂Èôê„Åó„Åæ„Åô„ÄÇ

2.  **ÈÄêÊ¨°ÁöÑ„Å™ÊÄßË≥™„Å´„Çà„ÇãË®àÁÆó„ÅÆÈÅÖ„Åï (Slow Computation due to Sequential Nature):**
    * **RNN„ÅÆÈÄêÊ¨°ÁöÑ„Å™ÊÄßË≥™„ÅØ„ÄÅÂÜçÂ∏∞ÁöÑ„Å™Ë®àÁÆó„ÅåÈÅÖ„ÅÑ„Åì„Å®„ÇíÊÑèÂë≥„Åô„Çã„ÄÇ**
        RNN„ÅÆÂêÑ„Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„Éó„ÅÆË®àÁÆó„ÅØ„ÄÅÂâç„ÅÆ„Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„Éó„ÅÆË®àÁÆóÁµêÊûú„Å´Âº∑„Åè‰æùÂ≠ò„Åó„Åæ„Åô„ÄÇ„Å§„Åæ„Çä„ÄÅ$h_t$ „ÇíË®àÁÆó„Åô„Çã„Å´„ÅØ $h_{t-1}$ „ÅåÂøÖË¶Å„Åß„ÅÇ„Çä„ÄÅ$h_{t-1}$ „ÇíË®àÁÆó„Åô„Çã„Å´„ÅØ $h_{t-2}$ „ÅåÂøÖË¶Å‚Ä¶„Å®„ÅÑ„ÅÜ„Çà„ÅÜ„Å´„ÄÅÂá¶ÁêÜ„ÇíÂÖàÈ†≠„Åã„ÇâÈ†Ü„Å´„Åó„ÅãÈÄ≤„ÇÅ„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åõ„Çì„ÄÇ„Åì„ÅÆ**Êú¨Ë≥™ÁöÑ„Å™ÈÄêÊ¨°ÊÄß**„ÅØ„ÄÅÊúÄÊñ∞„ÅÆGPU„Å™„Å©„ÅÆ‰∏¶ÂàóË®àÁÆóËÉΩÂäõ„ÇíÂçÅÂàÜ„Å´Ê¥ªÁî®„Åô„Çã„Åì„Å®„ÇíÂ¶®„Åí„ÄÅÁâπ„Å´ÈùûÂ∏∏„Å´Èï∑„ÅÑ„Ç∑„Éº„Ç±„É≥„Çπ„ÇÑÂ§ßË¶èÊ®°„Å™„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÇíÊâ±„ÅÜÈöõ„ÅÆË®ìÁ∑¥„Å®Êé®Ë´ñ„ÅÆÈÄüÂ∫¶„ÇíËëó„Åó„Åè‰Ωé‰∏ã„Åï„Åõ„Åæ„Åô„ÄÇ

3.  **Èï∑Ë∑ùÈõ¢‰æùÂ≠òÈñ¢‰øÇ„ÅÆÂ≠¶Áøí„ÅÆÂõ∞Èõ£„Åï (Difficulty in learning very long-range dependencies):**
    * **Áâπ„Å´Èï∑„ÅÑÊñáÁ´†„ÅÆÂ†¥Âêà„ÄÅÊñá„ÅÆÂÜíÈ†≠„ÅÆÊÉÖÂ†±„Åå„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„Éô„ÇØ„Éà„É´„Å´ÂêåÁ≠â„Å´„ÅÜ„Åæ„ÅèË°®Áèæ„Åï„Çå„Å™„ÅÑÂèØËÉΩÊÄß„Åå„ÅÇ„Çã„ÄÇ**
        ÁêÜË´ñ‰∏ä„ÅØRNN„ÅåÈï∑Ë∑ùÈõ¢‰æùÂ≠òÈñ¢‰øÇ„ÇíÊçâ„Åà„Çâ„Çå„Åæ„Åô„Åå„ÄÅÂÆüÈöõ„Å´„ÅØ**ÂãæÈÖçÊ∂àÂ§±ÂïèÈ°å (vanishing gradient problem)** „ÇÑ**ÂãæÈÖçÁàÜÁô∫ÂïèÈ°å (exploding gradient problem)** „Å®„ÅÑ„ÅÜË™≤È°å„Å´Áõ¥Èù¢„Åó„Åæ„Åô„ÄÇ
        * **ÂãæÈÖçÊ∂àÂ§±:** Â§ö„Åè„ÅÆ„Çø„Ç§„É†„Çπ„ÉÜ„ÉÉ„Éó„ÇíÈÅ°„Çã„Å´„Å§„Çå„Å¶„ÄÅÂãæÈÖç„ÅåÈùûÂ∏∏„Å´Â∞è„Åï„Åè„Å™„Çä„ÄÅ„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆÂàùÊúüÂ±§„ÅÆÈáç„Åø„Åå„Åª„Å®„Çì„Å©Êõ¥Êñ∞„Åï„Çå„Å™„Åè„Å™„Çä„Åæ„Åô„ÄÇ„Åì„Çå„Å´„Çà„Çä„ÄÅ„É¢„Éá„É´„ÅØ„Ç∑„Éº„Ç±„É≥„Çπ„ÅÆÊúÄÂàù„ÅÆÈÉ®ÂàÜ„ÅÆÊÉÖÂ†±„Çí„ÄåÂøò„Çå„Å¶„Äç„Åó„Åæ„ÅÑ„ÄÅÈùûÂ∏∏„Å´Èï∑„ÅÑË∑ùÈõ¢„ÅÆ‰æùÂ≠òÈñ¢‰øÇ„ÇíÂ≠¶Áøí„Åô„Çã„Åì„Å®„ÅåÂõ∞Èõ£„Å´„Å™„Çä„Åæ„Åô„ÄÇ
        * **ÂãæÈÖçÁàÜÁô∫:** ÈÄÜ„Å´ÂãæÈÖç„ÅåÈùûÂ∏∏„Å´Â§ß„Åç„Åè„Å™„Çä„ÄÅË®ìÁ∑¥„Åå‰∏çÂÆâÂÆö„Å´„Å™„Çã„Åì„Å®„Åå„ÅÇ„Çä„Åæ„ÅôÔºà„Åì„Å°„Çâ„ÅØÂãæÈÖç„ÇØ„É™„ÉÉ„Éî„É≥„Ç∞„ÅßÂØæÂá¶ÂèØËÉΩÔºâ„ÄÇ
        „Åì„Çå„Çâ„ÅÆÂïèÈ°å„Å´„Çà„Çä„ÄÅRNN„ÅØ„ÄåÁêÜË´ñ‰∏ä„ÅØ„ÄçÈï∑Ë∑ùÈõ¢‰æùÂ≠òÈñ¢‰øÇ„ÇíÊâ±„Åà„Çã„ÇÇ„ÅÆ„ÅÆ„ÄÅ**ÂÆüÈöõ„Å´„ÅØ**Êï∞„Çπ„ÉÜ„ÉÉ„ÉóÂÖà„Åæ„Åß„ÅÆ‰æùÂ≠òÈñ¢‰øÇ„Åó„ÅãÂäπÊûúÁöÑ„Å´Â≠¶Áøí„Åß„Åç„Å™„ÅÑ„Åì„Å®„ÅåÂ§ö„Åã„Å£„Åü„ÅÆ„Åß„Åô„ÄÇLSTM (Long Short-Term Memory) „ÇÑ GRU (Gated Recurrent Unit) „Å®„ÅÑ„Å£„ÅüÊîπËâØÁâàRNN„ÅåÁôªÂ†¥„Åó„ÄÅ„Åì„Çå„Çâ„ÅÆÂïèÈ°å„ÇíÁ∑©Âíå„Åó„Åæ„Åó„Åü„Åå„ÄÅÊ†πÊú¨ÁöÑ„Å™Ëß£Ê±∫„Å´„ÅØËá≥„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ

„Åì„Çå„Çâ„ÅÆRNN„ÅÆÊ¨†ÁÇπ„ÄÅÁâπ„Å´‰∏¶ÂàóÂåñ„ÅÆÂõ∞Èõ£„Åï„Å®Èï∑Ë∑ùÈõ¢‰æùÂ≠òÈñ¢‰øÇ„ÅÆÂ≠¶ÁøíÈôêÁïå„Åå„ÄÅAttentionÊ©üÊßã„ÅÆÂ∞éÂÖ•„ÄÅ„Åù„Åó„Å¶ÊúÄÁµÇÁöÑ„Å´Transformer„É¢„Éá„É´„Å∏„Å®Áπã„Åå„ÇãÂ§ß„Åç„Å™ÂãïÊ©ü„Å®„Å™„Çä„Åæ„Åó„Åü„ÄÇ
-->


---
transition: slide-up
level: 2
---

# The Principle of Transformer

Attention Mechanism

<div class="flex justify-center">
  <img src="./image/attention_example.png" alt="„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÂõ≥" width="500" />
</div>

<v-clicks depth="2">

- Key vectors are generated using a weight matrix 
    - $$k_i = W_K \cdot h_i, \quad \text{for } i = 1 \text{ to } m$$
- Query $q_t$ is generated using another weight matrix
    -  $$q_t = W_Q \cdot s_t$$
</v-clicks>

<!--
Attention„É°„Ç´„Éã„Ç∫„É†„ÅØ„ÄÅ„Åì„Çå„Çâ„ÅÆË™≤È°å„ÇíËß£Ê±∫„Åô„Çã„Åü„ÇÅ„Å´Â∞éÂÖ•„Åï„Çå„Åæ„Åó„Åü„ÄÇ„Åù„ÅÆÂü∫Êú¨ÁöÑ„Å™ËÄÉ„ÅàÊñπ„ÅØ„ÄÅ‰∫∫Èñì„Åå‰Ωï„Åã„ÇíÁêÜËß£„Åô„ÇãÈöõ„Å´„ÄÅÊñáÂÖ®‰Ωì„ÇíÊº´ÁÑ∂„Å®Ë™≠„ÇÄ„ÅÆ„Åß„ÅØ„Å™„Åè„ÄÅÈáçË¶Å„Å™ÈÉ®ÂàÜ„Å´„ÄåÊ≥®ÊÑè„ÇíÂêë„Åë„Çã„Äç„Å®„ÅÑ„ÅÜË™çÁü•„Éó„É≠„Çª„Çπ„ÇíÊ®°ÂÄ£„Åô„Çã„Åì„Å®„Åß„Åô„ÄÇ

ÂÖ∑‰ΩìÁöÑ„Å™‰ªïÁµÑ„Åø„ÅØ„ÄÅ‰∏ª„Å´„ÇØ„Ç®„É™ÔºàQueryÔºâ„ÄÅ„Ç≠„ÉºÔºàKeyÔºâ„ÄÅ„Éê„É™„É•„ÉºÔºàValueÔºâ „ÅÆ3„Å§„ÅÆÊ¶ÇÂøµ„Çí‰Ωø„Å£„Å¶Ë™¨Êòé„Åï„Çå„Åæ„Åô„ÄÇ

-->


---
transition: slide-up
level: 2
---

# The Principle of Transformer

Attention Mechanism

<div class="flex justify-center">
  <img src="./image/attention_example.png" alt="„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÂõ≥" width="500" />
</div>

<v-clicks depth="2">

- The attention score $a_i^t$ between each key $k_i$ and the query $q_t$ is computed
    - $$a_i^t = k_i^T \cdot q_t$$
- $Softmax([a_1,...,a_m])$: Normalize with a softmax to create a vector of weights
</v-clicks>

<!--
„Éá„Ç≥„Éº„ÉÄ„ÅåÁèæÂú®„ÅÆÂçòË™û„ÇíÁîüÊàê„Åó„Çà„ÅÜ„Å®„Åô„Çã„Å®„Åç„ÄÅÁèæÂú®„ÅÆÁä∂ÊÖã„ÇíË°®„Åô„Äå„ÇØ„Ç®„É™„Äç„Å®„ÄÅÂÖ•Âäõ„Ç∑„Éº„Ç±„É≥„ÇπÂÜÖ„ÅÆÂêÑÂçòË™û„ÇíË°®„Åô„Äå„Ç≠„Éº„Äç„Å®„ÅÆÈ°û‰ººÂ∫¶ÔºàÈñ¢ÈÄ£Â∫¶Ôºâ„ÇíË®àÁÆó„Åó„Åæ„Åô„ÄÇ
„Åì„ÅÆÈ°û‰ººÂ∫¶„ÅØ„ÄÅÈÄöÂ∏∏„ÄÅÂÜÖÁ©ç„ÇÑÂä†Ê≥ï„Å™„Å©„ÅÆÈñ¢Êï∞„Çí‰Ωø„Å£„Å¶Ë®àÁÆó„Åï„Çå„ÄÅ„Äå„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„Çπ„Ç≥„Ç¢„Äç„Å®Âëº„Å∞„Çå„Åæ„Åô„ÄÇ„Çπ„Ç≥„Ç¢„ÅåÈ´ò„ÅÑ„Åª„Å©„ÄÅ„Åù„ÅÆÂçòË™û„ÅÆÈñ¢ÈÄ£ÊÄß„ÅåÈ´ò„ÅÑ„Åì„Å®„ÇíÁ§∫„Åó„Åæ„Åô„ÄÇ

Ë®àÁÆó„Åï„Çå„Åü„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„Çπ„Ç≥„Ç¢„ÅØ„ÄÅSoftmaxÈñ¢Êï∞„ÇíÈÄö„Åó„Å¶Ê≠£Ë¶èÂåñ„Åï„Çå„ÄÅÂêàË®à„Åå1„Å´„Å™„Çã„Çà„ÅÜ„Å™„Äå„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥Èáç„ÅøÔºà„Åæ„Åü„ÅØ„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥ÂàÜÂ∏ÉÔºâ„Äç„Å´Â§âÊèõ„Åï„Çå„Åæ„Åô„ÄÇ„Åì„Çå„Å´„Çà„Çä„ÄÅÂêÑÂçòË™û„Åå„Å©„Çå„Åè„Çâ„ÅÑÈáçË¶Å„Åß„ÅÇ„Çã„Åã„ÇíÁ§∫„ÅôÁ¢∫ÁéáÁöÑ„Å™Èáç„Åø„ÅåÂæó„Çâ„Çå„Åæ„Åô„ÄÇÈáç„Åø„ÅåÂ§ß„Åç„ÅÑ„Åª„Å©„ÄÅ„Åù„ÅÆÂçòË™û„Å´Âº∑„Åè„ÄåÊ≥®ÁõÆ„Äç„Åó„Å¶„ÅÑ„Çã„Åì„Å®„ÇíÊÑèÂë≥„Åó„Åæ„Åô„ÄÇ
-->

---
transition: slide-up
level: 2
---

# The Principle of Transformer

Attention Mechanism

<div class="flex justify-center">
  <img src="./image/attention_example.png" alt="„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÂõ≥" width="500" />
</div>

<v-clicks depth="2">

- Create a fixed-length vector by taking a weighted sum of all the encoder hidden states
    - $$c_t=a_{1}^t h_1+...+a_{m}^t h_m$$

</v-clicks>


<!--
- ÂêÑÂçòË™û„Å´ÂØæÂøú„Åô„Çã„Äå„Éê„É™„É•„Éº„ÄçÔºàÈÄöÂ∏∏„ÅØÂçòË™û„ÅÆË°®Áèæ„Éô„ÇØ„Éà„É´Ëá™‰ΩìÔºâ„Çí„ÄÅ„Çπ„ÉÜ„ÉÉ„Éó2„ÅßÂæó„Çâ„Çå„Åü„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥Èáç„Åø„ÅßÈáç„Åø‰ªò„Åë„Åó„Å¶ÂêàË®à„Åó„Åæ„Åô„ÄÇ
- „Åì„Çå„Å´„Çà„Çä„ÄÅÁèæÂú®„ÅÆ„Çø„Çπ„ÇØÔºà‰æãÔºöÊ¨°„ÅÆÂçòË™ûÁîüÊàêÔºâ„Å´„Å®„Å£„Å¶ÊúÄ„ÇÇÈñ¢ÈÄ£ÊÄß„ÅÆÈ´ò„ÅÑÊÉÖÂ†±„ÅåÂº∑Ë™ø„Åï„Çå„Åü„ÄåÊñáËÑà„Éô„ÇØ„Éà„É´„Äç„ÅåÁîüÊàê„Åï„Çå„Åæ„Åô„ÄÇ
-->


---
transition: slide-up
level: 2
---

# The Principle of Transformer

Self-Attention

<div grid="~ cols-2 gap-4 items-start">

<div>

<v-clicks depth="2">

- Create three vectors query, key, and value from each of the embeddings of each word
- $$W^Q, W^K, W^V \in R^{d_{model} \times d_k}$$

- Calculate attention scores between each word of the input sentence against a specific word

- $$Attention(\mathbf{Q},\mathbf{K},\mathbf{V})=softmax(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}})V$$
</v-clicks>

</div>

<div class="flex justify-center">
  <img src="./image/key_query.png" alt="„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÂõ≥" width="300" />
</div>

</div>



---
transition: slide-up
level: 2
---

# The Principle of Transformer

Attention Mechanism

<div class="flex justify-center">
  <img src="./image/self-attention-matrix-calculation-2.png" alt="„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÂõ≥" width="500" />
</div>

<v-clicks depth="2">

- Scale the attention scores, take the softmax, and then multiply the result by $V$ resulting in a matrix of shape $N \times d$: a vector embedding representation for each token in the input.

</v-clicks>

<!--
„Åì„ÅÆÊñ∞„Åó„ÅÑÊñáËÑà„Éô„ÇØ„Éà„É´„ÅØ„ÄÅÂçò‰∏Ä„ÅÆÂõ∫ÂÆö„Éô„ÇØ„Éà„É´„Åß„ÅØ„Å™„Åè„ÄÅ„Çø„Çπ„ÇØ„ÇÑÁèæÂú®„ÅÆÂá∫Âäõ„ÅÆÁä∂Ê≥Å„Å´Âøú„Åò„Å¶„ÄÅÂÖ•Âäõ„Ç∑„Éº„Ç±„É≥„Çπ„ÅÆ„Å©„Åì„Å´„ÄåÊ≥®ÁõÆ„Äç„Åô„Åπ„Åç„Åã„ÇíÂãïÁöÑ„Å´Â§âÂåñ„Åï„Åõ„Å™„Åå„ÇâÁîüÊàê„Åï„Çå„ÇãÁÇπ„ÅåÈáçË¶Å„Åß„Åô„ÄÇ
-->


---
transition: slide-up
level: 2
---

# The Principle of Transformer

Attention Mechanism

<div class="flex justify-center">
  <img src="./image/general_scheme-min.png" alt="„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÂõ≥" width="600" />
</div>


<!--
„Åì„ÅÆÊñ∞„Åó„ÅÑÊñáËÑà„Éô„ÇØ„Éà„É´„ÅØ„ÄÅÂçò‰∏Ä„ÅÆÂõ∫ÂÆö„Éô„ÇØ„Éà„É´„Åß„ÅØ„Å™„Åè„ÄÅ„Çø„Çπ„ÇØ„ÇÑÁèæÂú®„ÅÆÂá∫Âäõ„ÅÆÁä∂Ê≥Å„Å´Âøú„Åò„Å¶„ÄÅÂÖ•Âäõ„Ç∑„Éº„Ç±„É≥„Çπ„ÅÆ„Å©„Åì„Å´„ÄåÊ≥®ÁõÆ„Äç„Åô„Åπ„Åç„Åã„ÇíÂãïÁöÑ„Å´Â§âÂåñ„Åï„Åõ„Å™„Åå„ÇâÁîüÊàê„Åï„Çå„ÇãÁÇπ„ÅåÈáçË¶Å„Åß„Åô„ÄÇ
-->





---
transition: slide-up
level: 2
---

# The Principle of Transformer

Self-Attention

<div grid="~ cols-2 gap-4">
<div>

- Within a single sequence, each word evaluates how it relates to every other word in that sequence and learns its representation based on these relationships.
   - Capturing long-range dependencies: Since each word directly computes its relevance to all other words in the sequence, relationships between distant words can be efficiently learned.
   - Parallel computation: The self-attention calculation for each word can be performed independently of the calculations for other words.
</div>

<div>

<video controls width="700" style="display: block; margin-left: auto; margin-right: auto;">
  <source src="./image/encoder_self_attention.mp4" type="video/mp4">
</video>
</div>
</div>

<!--
Self-Attention„ÅØ„ÄÅAttention„É°„Ç´„Éã„Ç∫„É†„ÅÆ‰∏ÄËà¨ÁöÑ„Å™Ê¶ÇÂøµÔºà„ÇØ„Ç®„É™„ÄÅ„Ç≠„Éº„ÄÅ„Éê„É™„É•„ÉºÔºâ„Çí„ÄÅÂçò‰∏Ä„ÅÆ„Ç∑„Éº„Ç±„É≥„ÇπÂÜÖ„ÅÆË¶ÅÁ¥†Èñì„ÅßÈÅ©Áî®„Åô„Çã„Åì„Å®„ÅßÂÆüÁèæ„Åï„Çå„Åæ„Åô„ÄÇ

„ÅÇ„ÇãÂÖ•Âäõ„Ç∑„Éº„Ç±„É≥„Çπ $X = (x_1, x_2, \ldots, x_n)$ „Åå„ÅÇ„Çã„Å®„Åç„ÄÅÂêÑÂçòË™û $x_i$ „Å´„Å§„ÅÑ„Å¶„ÄÅ‰ª•‰∏ã„ÅÆ3„Å§„ÅÆ„Éô„ÇØ„Éà„É´„ÅåÁîüÊàê„Åï„Çå„Åæ„Åô„ÄÇ

1.  **„ÇØ„Ç®„É™ (Query: $Q$)**: ÁèæÂú®Ê≥®ÁõÆ„Åó„Å¶„ÅÑ„ÇãÂçòË™û„Åå„ÄÅ‰ªñ„ÅÆÂçòË™û„Å´ÂØæ„Åó„Å¶„Äå‰Ωï„ÇíÊé¢„Åó„Å¶„ÅÑ„Çã„Åã„Äç„ÇíË°®Áèæ„Åó„Åæ„Åô„ÄÇ
2.  **„Ç≠„Éº (Key: $K$)**: ‰ªñ„ÅÆÂçòË™û„Åå„Äå„Å©„ÅÆ„Çà„ÅÜ„Å™ÊÉÖÂ†±„ÇíÊåÅ„Å£„Å¶„ÅÑ„Çã„Åã„Äç„ÇíË°®Áèæ„Åó„Åæ„Åô„ÄÇ
3.  **„Éê„É™„É•„Éº (Value: $V$)**: ‰ªñ„ÅÆÂçòË™û„ÅåÂÆüÈöõ„Å´Êèê‰æõ„Åô„Çã„ÄåÊÉÖÂ†±„Åù„ÅÆ„ÇÇ„ÅÆ„Äç„ÇíË°®Áèæ„Åó„Åæ„Åô„ÄÇ

„Åì„Çå„Çâ„ÅÆ $Q, K, V$ „ÅØ„ÄÅÂÖ•ÂäõÂçòË™û„ÅÆÂüã„ÇÅËæº„Åø„Éô„ÇØ„Éà„É´ $x_i$ „Çí„Åù„Çå„Åû„ÇåÁï∞„Å™„ÇãÂ≠¶ÁøíÂèØËÉΩ„Å™Èáç„ÅøË°åÂàó $W_Q, W_K, W_V$ „ÅßÁ∑öÂΩ¢Â§âÊèõ„Åô„Çã„Åì„Å®„ÅßÁîüÊàê„Åï„Çå„Åæ„Åô„ÄÇ

$Q_i = x_i W_Q$
$K_i = x_i W_K$
$V_i = x_i W_V$

Self-Attention„ÅÆË®àÁÆó„ÅØ„ÄÅ‰∏ª„Å´‰ª•‰∏ã„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó„ÅßË°å„Çè„Çå„Åæ„Åô„ÄÇ

1.  **„Çπ„Ç≥„Ç¢„ÅÆË®àÁÆó**:
    * ÂêÑÂçòË™û„ÅÆ„ÇØ„Ç®„É™ $Q_i$ „Å®„ÄÅ„Ç∑„Éº„Ç±„É≥„ÇπÂÜÖ„ÅÆ‰ªñ„ÅÆ„Åô„Åπ„Å¶„ÅÆÂçòË™û„ÅÆ„Ç≠„Éº $K_j$ „Å®„ÅÆÂÜÖÁ©çÔºà„Éâ„ÉÉ„ÉàÁ©çÔºâ„ÇíË®àÁÆó„Åó„Åæ„Åô„ÄÇ
    * „Åì„ÅÆÂÜÖÁ©ç„Åå„ÄÅÂçòË™û $x_i$ „ÅåÂçòË™û $x_j$ „Å´„Å©„Çå„Åè„Çâ„ÅÑ„ÄåÊ≥®ÁõÆ„Äç„Åô„Åπ„Åç„Åã„ÅÆÈñ¢ÈÄ£Â∫¶„Çπ„Ç≥„Ç¢„Å®„Å™„Çä„Åæ„Åô„ÄÇ
    * „Çπ„Ç±„Éº„É´„Åï„Çå„Åü„Éâ„ÉÉ„ÉàÁ©çÔºàÈÄöÂ∏∏„ÅØ„Ç≠„Éº„Éô„ÇØ„Éà„É´„ÅÆÊ¨°ÂÖÉ„ÅÆÂπ≥ÊñπÊ†π„ÅßÂâ≤„ÇãÔºâ„Çí‰ΩøÁî®„Åô„Çã„Åì„Å®„Åå‰∏ÄËà¨ÁöÑ„Åß„Åô„ÄÇ
    $Score(Q_i, K_j) = \frac{Q_i \cdot K_j}{\sqrt{d_k}}$ Ôºà$d_k$ „ÅØ„Ç≠„Éº„Éô„ÇØ„Éà„É´„ÅÆÊ¨°ÂÖÉÔºâ

2.  **Èáç„Åø„ÅÆÊ≠£Ë¶èÂåñ**:
    * Ë®àÁÆó„Åï„Çå„Åü„Åô„Åπ„Å¶„ÅÆ„Çπ„Ç≥„Ç¢„Å´ÂØæ„Åó„ÄÅSoftmaxÈñ¢Êï∞„ÇíÈÅ©Áî®„Åó„Åæ„Åô„ÄÇ„Åì„Çå„Å´„Çà„Çä„ÄÅÂêÑÂçòË™û $x_j$ „Å∏„ÅÆÊ≥®ÁõÆÂ∫¶„ÇíË°®„Åô**„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥Èáç„Åø**ÔºàÂêàË®à„Åå1„Å´„Å™„ÇãÁ¢∫ÁéáÂàÜÂ∏ÉÔºâ„ÅåÂæó„Çâ„Çå„Åæ„Åô„ÄÇ
    $Attention\_Weight_{ij} = \text{Softmax}(Score(Q_i, K_j))$

3.  **Èáç„Åø‰ªò„Åë„Åï„Çå„Åü„Éê„É™„É•„Éº„ÅÆÂêàË®à**:
    * ÂêÑÂçòË™û $x_j$ „Å´ÂØæÂøú„Åô„Çã„Éê„É™„É•„Éº $V_j$ „Çí„ÄÅË®àÁÆó„Åï„Çå„Åü„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥Èáç„Åø $Attention\_Weight_{ij}$ „ÅßÈáç„Åø‰ªò„Åë„Åó„ÄÅ„Åù„Çå„Çâ„Çí„Åô„Åπ„Å¶ÂêàË®à„Åó„Åæ„Åô„ÄÇ
    * „Åì„Çå„Åå„ÄÅÂçòË™û $x_i$ „ÅÆÊñ∞„Åó„ÅÑË°®ÁèæÔºàÊñáËÑà„ÇíËÄÉÊÖÆ„Åó„ÅüË°®ÁèæÔºâ„Åß„ÅÇ„Çã**Âá∫Âäõ„Éô„ÇØ„Éà„É´ $Z_i$** „Å®„Å™„Çä„Åæ„Åô„ÄÇ
    $Z_i = \sum_{j=1}^{n} Attention\_Weight_{ij} \cdot V_j$

„Åì„ÅÆ„Éó„É≠„Çª„Çπ„ÅØ„ÄÅ„Ç∑„Éº„Ç±„É≥„ÇπÂÜÖ„ÅÆÂêÑÂçòË™û $x_i$ „Åî„Å®„Å´Ë°å„Çè„Çå„Åæ„Åô„ÄÇÁµêÊûú„Å®„Åó„Å¶„ÄÅÂêÑÂçòË™û„ÅØ„ÄÅ„Ç∑„Éº„Ç±„É≥„ÇπÂÜÖ„ÅÆ‰ªñ„ÅÆÂçòË™û„Åô„Åπ„Å¶„Å®„ÅÆÈñ¢‰øÇÊÄß„ÇíËÄÉÊÖÆ„Åó„Åü„ÄÅ„Çà„ÇäË±ä„Åã„Å™ÊñáËÑàÁöÑ„Å™Ë°®Áèæ„ÇíÁç≤Âæó„Åó„Åæ„Åô„ÄÇ
-->



---
transition: slide-up
level: 2
---

# The Principle of Transformer

Self-Attention

<div grid="~ cols-2 gap-4 items-start">

<div>

<v-clicks depth="2">

- Each attention score quantifies how relevant every other word (or token) in the sequence is to a given word.

- If an attention mechanism assigns a high weight to a particular word when processing another word or making a prediction, it suggests that those highly-weighted words are important for the model's decision. 

</v-clicks>

</div>

<div class="flex justify-center">
  <img src="./image/self-attention-score.png" alt="„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÂõ≥" width="300" />
</div>

</div>

---
transition: slide-up
level: 2
---

# The Principle of Transformer

Components in Transformer Architecture


<div grid="~ cols-2 gap-4 items-start">

<div>

<v-clicks depth="2">

- **Multi-Head Attention**: Allows the model to jointly attend to information from different representation subspaces (e.g., syntactic, semantic, and discourse relationships).

- **Positional Encoding**: Injects some information about the order of the sequence into the model.

- **Add & Norm Layer**: Encourages training deeper models by ensuring that backpropagation through many layers does not result in vanishing or exploding gradients.

</v-clicks>
</div>

<div class="flex justify-center">
  <img src="./image/transformer_frame.png" alt="„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÂõ≥" width="500" />
</div>

</div>

<!--

### Multi-Head Attention („Éû„É´„ÉÅ„Éò„ÉÉ„Éâ„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥)

* **„ÄåAllows the model to jointly attend to information from different representation subspaces (e.g., syntactic, semantic, and discourse relationships).„Äç**

    * **Áï∞„Å™„ÇãË°®ÁèæÈÉ®ÂàÜÁ©∫Èñì„Å∏„ÅÆÂÖ±ÂêåÊ≥®ÁõÆ:**
        * ÈÄöÂ∏∏„ÅÆSelf-Attention„Åß„ÅØ„ÄÅÂçòË™ûÈñì„ÅÆÈñ¢ÈÄ£ÊÄß„ÇíË®àÁÆó„Åô„Çã„Åü„ÇÅ„ÅÆÈáç„ÅøË°åÂàó„ÅØ1„Çª„ÉÉ„Éà„Åó„Åã„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ„Åó„Åã„Åó„ÄÅË®ÄË™û„Å´„ÅØÊßò„ÄÖ„Å™Á®ÆÈ°û„ÅÆÈñ¢‰øÇÊÄß„ÅåÂ≠òÂú®„Åó„Åæ„Åô„ÄÇ‰æã„Åà„Å∞„ÄÅÊñáÊ≥ïÁöÑ„Å™Èñ¢‰øÇÔºà‰∏ªË™û„Å®ÂãïË©ûÔºâ„ÄÅÊÑèÂë≥ÁöÑ„Å™Èñ¢‰øÇÔºàÂêåÁæ©Ë™û„ÄÅÈ°ûÁæ©Ë™ûÔºâ„ÄÅË´áË©±ÁöÑ„Å™Èñ¢‰øÇÔºà‰ª£ÂêçË©û„Å®ÂÖàË°åË©ûÔºâ„Å™„Å©„Åß„Åô„ÄÇ
        * Multi-Head Attention„ÅØ„ÄÅ„Åì„Çå„Çâ„ÅÆÁï∞„Å™„ÇãÁ®ÆÈ°û„ÅÆÈñ¢‰øÇÊÄß„ÇíÂêåÊôÇ„Å´Êçâ„Åà„Çã„Åì„Å®„ÇíÂèØËÉΩ„Å´„Åó„Åæ„Åô„ÄÇÂÖ∑‰ΩìÁöÑ„Å´„ÅØ„ÄÅSelf-Attention„ÅÆË®àÁÆó„Çí„ÄÅ**Áï∞„Å™„ÇãÈáç„ÅøË°åÂàó $W_Q, W_K, W_V$ „ÅÆ„Çª„ÉÉ„Éà„Çí‰Ωø„Å£„Å¶Ë§áÊï∞ÂõûÔºà‰æãÔºö8ÂõûÔºâ‰∏¶Ë°å„Åó„Å¶ÂÆüË°å**„Åó„Åæ„Åô„ÄÇ„Åù„Çå„Åû„Çå„ÅÆË®àÁÆóÂçò‰Ωç„Çí„Äå„Éò„ÉÉ„Éâ„Äç„Å®Âëº„Å≥„Åæ„Åô„ÄÇ
        * ÂêÑ„Éò„ÉÉ„Éâ„ÅØ„ÄÅË®ìÁ∑¥‰∏≠„Å´Áï∞„Å™„Çã„ÄåÂÅ¥Èù¢„Äç„Å´ÁâπÂåñ„Åô„Çã„Åì„Å®„ÇíÂ≠¶Áøí„Åó„Åæ„Åô„ÄÇ„ÅÇ„Çã„Éò„ÉÉ„Éâ„ÅØÊñáÊ≥ïÁöÑ„Å™„Éë„Çø„Éº„É≥„Å´Ê≥®ÁõÆ„Åó„ÄÅÂà•„ÅÆ„Éò„ÉÉ„Éâ„ÅØÊÑèÂë≥ÁöÑ„Å™Ëøë„Åï„Å´Ê≥®ÁõÆ„Åô„Çã„Å®„ÅÑ„Å£„ÅüÂÖ∑Âêà„Åß„Åô„ÄÇ
        * „Åù„Çå„Åû„Çå„ÅÆ„Éò„ÉÉ„Éâ„Åã„Çâ„ÅÆÂá∫Âäõ„ÅØÁµêÂêà„Åï„Çå„ÄÅÊúÄÁµÇÁöÑ„Å´Âçò‰∏Ä„ÅÆ„Éô„ÇØ„Éà„É´„Å´Â§âÊèõ„Åï„Çå„Å¶Ê¨°„ÅÆÂ±§„Å´Ê∏°„Åï„Çå„Åæ„Åô„ÄÇ„Åì„Çå„Å´„Çà„Çä„ÄÅ„É¢„Éá„É´„ÅØÂçò‰∏Ä„ÅÆSelf-Attention„Çà„Çä„ÇÇÈÅ•„Åã„Å´Ë±ä„Åã„ÅßÂ§öËßíÁöÑ„Å™ÊñáËÑàÊÉÖÂ†±„ÇíÂèñÂæó„Åó„ÄÅ„Çà„ÇäÊ∑±„ÅÑË®ÄË™ûÁêÜËß£„ÇíÂÆüÁèæ„Åó„Åæ„Åô„ÄÇ


### Positional Encoding (‰ΩçÁΩÆ„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞)

* **„ÄåInjects some information about the order of the sequence into the model.„Äç**

    * **„Ç∑„Éº„Ç±„É≥„Çπ„ÅÆÈ†ÜÂ∫è„Å´Èñ¢„Åô„ÇãÊÉÖÂ†±„Çí„É¢„Éá„É´„Å´Ê≥®ÂÖ•„Åô„Çã:**
        * Transformer„É¢„Éá„É´„ÅØ„ÄÅRNN„Å®„ÅØÁï∞„Å™„Çä„ÄÅÂÜçÂ∏∞ÁöÑ„Å™ÊßãÈÄ†„ÇíÊåÅ„Åü„Åö„ÄÅÁï≥„ÅøËæº„Åø„ÇÇ‰ΩøÁî®„Åó„Åæ„Åõ„Çì„ÄÇSelf-Attention„ÅØ„ÄÅ„Ç∑„Éº„Ç±„É≥„ÇπÂÜÖ„ÅÆ„Å©„ÅÆÂçòË™û„ÇÇ‰∏¶Âàó„Å´Âá¶ÁêÜ„Åó„Åæ„Åô„ÄÇ„Åì„Çå„ÅØÈ´òÈÄüÂåñ„Å´Ë≤¢ÁåÆ„Åó„Åæ„Åô„Åå„ÄÅ„Åù„ÅÆÁµêÊûú„ÄÅ**ÂçòË™û„ÅÆÈ†ÜÂ∫è„Å´Èñ¢„Åô„ÇãÊÉÖÂ†±„ÅåÂ§±„Çè„Çå„Åæ„Åô**„ÄÇ‰æã„Åà„Å∞„ÄÅ„ÄåÁä¨„ÅåÁå´„ÇíËøΩ„ÅÑ„Åã„Åë„Çã„Äç„Å®„ÄåÁå´„ÅåÁä¨„ÇíËøΩ„ÅÑ„Åã„Åë„Çã„Äç„ÅØ„ÄÅÂçòË™û„ÅÆÁ®ÆÈ°û„ÅØÂêå„Åò„Åß„ÇÇÈ†ÜÂ∫è„ÅåÁï∞„Å™„Çã„Åü„ÇÅÊÑèÂë≥„ÅåÂÖ®„ÅèÁï∞„Å™„Çä„Åæ„Åô„Åå„ÄÅSelf-AttentionÂçò‰Ωì„Åß„ÅØ„Åì„ÅÆÈÅï„ÅÑ„ÇíË™çË≠ò„Åß„Åç„Åæ„Åõ„Çì„ÄÇ
        * Positional Encoding„ÅØ„ÄÅ„Åì„ÅÆÊ¨†ÁÇπ„ÇíË£ú„ÅÜ„Åü„ÇÅ„Å´Â∞éÂÖ•„Åï„Çå„Åæ„Åô„ÄÇ„Åì„Çå„ÅØ„ÄÅÂçòË™û„ÅÆÂüã„ÇÅËæº„Åø„Éô„ÇØ„Éà„É´„Å´„ÄÅ**„Åù„ÅÆÂçòË™û„Åå„Ç∑„Éº„Ç±„É≥„ÇπÂÜÖ„ÅÆ„Å©„ÅÆ‰ΩçÁΩÆ„Å´„ÅÇ„Çã„Åã„ÇíÁ§∫„ÅôÁâπÂà•„Å™„Éô„ÇØ„Éà„É´„ÇíÂä†ÁÆó**„Åô„Çã„ÇÇ„ÅÆ„Åß„Åô„ÄÇ
        * „Åì„ÅÆ‰ΩçÁΩÆ„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞„ÅØ„ÄÅÈÄöÂ∏∏„ÄÅÂõ∫ÂÆö„Åï„Çå„ÅüÔºàÂ≠¶Áøí„Åï„Çå„Å™„ÅÑÔºâÂë®Ê≥¢Êï∞„Éô„Éº„Çπ„ÅÆÈñ¢Êï∞Ôºà‰æãÔºö„Çµ„Ç§„É≥Èñ¢Êï∞„Å®„Ç≥„Çµ„Ç§„É≥Èñ¢Êï∞Ôºâ„ÇíÁî®„ÅÑ„Å¶ÁîüÊàê„Åï„Çå„Åæ„Åô„ÄÇ„Åì„Çå„Å´„Çà„Çä„ÄÅ„É¢„Éá„É´„ÅØÂçòË™û„ÅÆÊÑèÂë≥ÊÉÖÂ†±„Å®ÂêåÊôÇ„Å´„ÄÅ„Åù„ÅÆÂçòË™û„ÅÆÁõ∏ÂØæÁöÑ„ÉªÁµ∂ÂØæÁöÑ„Å™‰ΩçÁΩÆÊÉÖÂ†±„ÇÇÂæó„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇ
        * ÁµêÊûú„Å®„Åó„Å¶„ÄÅTransformer„ÅØÂçòË™û„ÅÆÈ†ÜÂ∫è„ÇíËÄÉÊÖÆ„Åó„Åü‰∏ä„ÅßÊñáËÑà„ÇíÁêÜËß£„Åó„ÄÅÊÑèÂë≥„ÇíÊ≠£Á¢∫„Å´ÊääÊè°„Åß„Åç„Çã„Çà„ÅÜ„Å´„Å™„Çä„Åæ„Åô„ÄÇ

### Add & Norm Layer (Add & NormÂ±§)

* **„ÄåEncourages training deeper models by ensuring that backpropagation through many layers does not result in vanishing or exploding gradients.„Äç**

    * **ÂãæÈÖçÊ∂àÂ§±„ÉªÁàÜÁô∫„ÇíÈò≤„Åé„ÄÅÊ∑±„ÅÑ„É¢„Éá„É´„ÅÆË®ìÁ∑¥„Çí‰øÉÈÄ≤„Åô„Çã:**
        * „Åì„ÅÆÂ±§„ÅØ„ÄÅTransformer„ÅÆÂêÑ„Çµ„Éñ„É¨„Ç§„É§„ÉºÔºàMulti-Head Attention„ÇÑFeed-ForwardÂ±§„Å™„Å©Ôºâ„ÅÆÂæå„Å´ÈÖçÁΩÆ„Åï„Çå„Åæ„Åô„ÄÇ
        * **„ÄåAdd„ÄçÈÉ®ÂàÜÔºàÊÆãÂ∑ÆÊé•Á∂ö / Residual ConnectionÔºâ:** „Çµ„Éñ„É¨„Ç§„É§„Éº„Å∏„ÅÆÂÖ•Âäõ„Åå„ÄÅ„Åù„ÅÆ„Çµ„Éñ„É¨„Ç§„É§„Éº„ÅÆÂá∫Âäõ„Å´Áõ¥Êé•Âä†ÁÆó„Åï„Çå„Åæ„Åô„ÄÇ
            * $Output = Input + Sublayer(Input)$
            * „Åì„Çå„Å´„Çà„Çä„ÄÅÂãæÈÖç„ÅåÊ∑±Â±§„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÇíÈÄÜ‰ºùÊí≠„Åô„ÇãÈöõ„Å´„ÄÅÁâπÂÆö„ÅÆ„Éë„Çπ„ÇíÈÄö„Å£„Å¶„Éê„Ç§„Éë„Çπ„Åï„Çå„Çã„Åü„ÇÅ„ÄÅÂãæÈÖç„ÅåÊ∂àÂ§±„Åó„Åü„ÇäÁàÜÁô∫„Åó„Åü„Çä„Åô„Çã„ÅÆ„ÇíÈò≤„Åé„ÇÑ„Åô„Åè„Å™„Çä„Åæ„Åô„ÄÇ„Åæ„Åü„ÄÅÂêÑÂ±§„ÅåÂçò„Å™„ÇãÂÖ•Âá∫Âäõ„ÅÆÂ§âÊèõ„Åß„ÅØ„Å™„Åè„ÄÅÂÖ•Âäõ„Å´ÂØæ„Åô„Çã„ÄåÊÆãÂ∑ÆÔºàÂ∑ÆÂàÜÔºâ„Äç„ÇíÂ≠¶Áøí„Åô„Çã„Çà„ÅÜ„Å´‰øÉ„Åô„Åì„Å®„Åß„ÄÅË®ìÁ∑¥„ÅåÂÆâÂÆö„Åó„ÇÑ„Åô„Åè„Å™„Çä„Åæ„Åô„ÄÇ
        * **„ÄåNorm„ÄçÈÉ®ÂàÜÔºà„É¨„Ç§„É§„ÉºÊ≠£Ë¶èÂåñ / Layer NormalizationÔºâ:** ÊÆãÂ∑ÆÊé•Á∂ö„ÅÆÂá∫Âäõ„ÅØ„ÄÅLayer Normalization„Å´Ê∏°„Åï„Çå„Åæ„Åô„ÄÇ
            * Layer Normalization„ÅØ„ÄÅ„Éê„ÉÉ„ÉÅÊ≠£Ë¶èÂåñ (Batch Normalization) „Å®„ÅØÁï∞„Å™„Çä„ÄÅÂêÑ„Çµ„É≥„Éó„É´„ÅÆÁâπÂæ¥ÈáèÊ¨°ÂÖÉ„Åî„Å®„Å´Ê≠£Ë¶èÂåñ„ÇíË°å„ÅÑ„Åæ„Åô„ÄÇ„Åì„Çå„Å´„Çà„Çä„ÄÅÂêÑÂ±§„ÅÆÂÖ•Âäõ„ÅÆÂàÜÂ∏É„ÇíÂÆâÂÆö„Åï„Åõ„ÄÅÂãæÈÖç„ÅÆ„Çπ„Ç±„Éº„É´„ÇíÈÅ©Âàá„Å´‰øù„Å°„Åæ„Åô„ÄÇ
            * ÁµêÊûú„Å®„Åó„Å¶„ÄÅ„É¢„Éá„É´„ÅÆÂ≠¶Áøí„ÅåÂÆâÂÆö„Åó„ÄÅ„Çà„ÇäÂ§ö„Åè„ÅÆÂ±§„ÇíÊåÅ„Å§**Ê∑±„ÅÑ„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Åß„ÇÇÂäπÊûúÁöÑ„Å´Ë®ìÁ∑¥„Åß„Åç„Çã„Çà„ÅÜ„Å´„Å™„Çä„Åæ„Åô„ÄÇ**

„Åì„Çå„Çâ„ÅÆË¶ÅÁ¥†„ÅåÁµÑ„ÅøÂêà„Çè„Åï„Çã„Åì„Å®„Åß„ÄÅTransformer„É¢„Éá„É´„ÅØ„Åì„Çå„Åæ„Åß„ÅÆ„É¢„Éá„É´„ÅåËã¶Êâã„Å®„Åó„Å¶„ÅÑ„ÅüË™≤È°å„ÇíÂÖãÊúç„Åó„ÄÅËá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ„ÅÆÂàÜÈáé„ÅßÈù©ÂëΩÁöÑ„Å™ÊÄßËÉΩ„ÇíÁô∫ÊèÆ„Åô„Çã„Åì„Å®„Å´„Å™„Çä„Åæ„Åó„Åü„ÄÇ
-->



---
transition: slide-up
level: 2
---

# LLMs with Transformer


<div class="flex justify-center">
  <img src="./image/tr18.png" alt="„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÂõ≥" width="800" />
</div>


<!--
„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£: Transformer„ÅÆ„Ç®„É≥„Ç≥„Éº„ÉÄÔºàEncoderÔºâÈÉ®ÂàÜ„Çí„Éô„Éº„Çπ„Å´„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Ç®„É≥„Ç≥„Éº„ÉÄ„ÅØ„ÄÅÂÖ•Âäõ„Ç∑„Éº„Ç±„É≥„ÇπÂÖ®‰Ωì„ÇíÂèåÊñπÂêë„Å´Âá¶ÁêÜ„Åó„ÄÅÂêÑ„Éà„Éº„ÇØ„É≥„Å´ÂØæ„Åô„ÇãÊñáËÑàÂåñ„Åï„Çå„ÅüË°®Áèæ„ÇíÁîüÊàê„Åô„Çã„ÅÆ„Å´ÂÑ™„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ

„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£: Transformer„ÅÆ„Éá„Ç≥„Éº„ÉÄÔºàDecoderÔºâÈÉ®ÂàÜ„Çí„Éô„Éº„Çπ„Å´„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Éá„Ç≥„Éº„ÉÄ„ÅØ„ÄÅÂâç„ÅÆ„Éà„Éº„ÇØ„É≥„Å´Âü∫„Å•„ÅÑ„Å¶Ê¨°„ÅÆ„Éà„Éº„ÇØ„É≥„ÇíÁîüÊàê„Åô„Çã„ÅÆ„Å´ÁâπÂåñ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
-->

---
transition: slide-up
level: 2
---

# Practive

- [Hugging Face Transformers Tutorial](https://colab.research.google.com/github/lvzeyu/Tohoku_AIE_PBL/blob/main/lecture1/notebook/Hugging%20Face%20Transformers%20Tutorial.ipynb)

- [Assignment](https://colab.research.google.com/github/lvzeyu/Tohoku_AIE_PBL/blob/main/lecture1/notebook/assignment.ipynb)