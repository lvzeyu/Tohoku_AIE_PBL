---
# You can also start simply with 'default'
theme: seriph
# random image from a curated Unsplash collection by Anthony
# like them? see https://unsplash.com/collections/94734566/slidev
background: https://cover.sli.dev
# some information about your slides (markdown enabled)
title: Welcome to Slidev
info: |
  ## Slidev Starter Template
  Presentation slides for developers.

  Learn more at [Sli.dev](https://sli.dev)
# apply unocss classes to the current slide
class: text-center
# https://sli.dev/features/drawing
drawings:
  persist: false
# slide transition: https://sli.dev/guide/animations.html#slide-transitions
transition: slide-left
# enable MDC Syntax: https://sli.dev/features/mdc
mdc: true

fonts:
  # basically the text
  sans: Robot
  # use with `font-serif` css class from UnoCSS
  serif: Robot Slab
  # for code blocks, inline code, etc.
  mono: Fira Code
---

# Introduction to LLMs and Their Applications

ZEYU LYU

<div @click="$slidev.nav.next" class="mt-12 py-1" hover:bg="white op-10">
  Press Space for next page <carbon:arrow-right />
</div>

<div class="abs-br m-6 text-xl"> 
  <button @click="$slidev.nav.openInEditor" title="Open in Editor" class="slidev-icon-btn">
    <carbon:edit />
  </button>
  <a href="https://github.com/lvzeyu/Tohoku_AIE_PBL" target="_blank" class="slidev-icon-btn">
    <carbon:logo-github />
  </a>
</div>

<!--
The last comment block of each slide will be treated as slide notes. It will be visible and editable in Presenter Mode along with the slide. [Read more in the docs](https://sli.dev/guide/syntax.html#notes)
-->

---
transition: fade-out
---

# Overview of Lectures

LLMs Basics

<v-clicks depth="2">

- **Lecture 1**

    - ğŸ’¡ High-level explanations of the fundamental concepts behind LLMs
    - â­ï¸ Insights into the transformer architecture
    - ğŸ§‘â€ğŸ’» Application of LLMs using Transformers library and HuggingFace Hub

- **Lecture 2**

    - ğŸ“ Prompting and In-context Learning of LLMs
    - ğŸ”§ Fine-tuning LLMs

- **Lecture 3** 
   
    - ğŸ¤– Applications of LLMs Agents

</v-clicks>

 

---
transition: slide-up
level: 2
---

# LLMs Basics

Journey of Language Models

<div class="flex justify-center">
  <img src="./image/NLP_history.png" width="800" />

</div>

<!--
è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNLPï¼‰ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒäººé–“ã®è¨€èªã‚’ç†è§£ã€ç”Ÿæˆã€åˆ†æã™ã‚‹ãŸã‚ã®ä¸€é€£ã®æŠ€è¡“ã§ã™ã€‚æ–‡ç« ãªã©ã®ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯ã™ã¹ã¦éæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã«åˆ†é¡ã•ã‚Œã¾ã™ã€‚ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«ã¨ã£ã¦ã€ã“ã®ç¨®ã®ãƒ‡ãƒ¼ã‚¿ã¯è¡¨ã‚„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚ˆã†ã«æ˜ç¢ºãªæ§‹é€ ã‚„è¦å‰‡ã‚’æŒãŸãªã„ãŸã‚ã€å‡¦ç†ãŒéå¸¸ã«å›°é›£ã§ã™ã€‚

è‡ªç„¶è¨€èªæŠ€è¡“ã®ç™ºå±•ã®åˆæœŸæ®µéšã§ã¯ã€éš ã‚Œãƒãƒ«ã‚³ãƒ•ãƒ¢ãƒ‡ãƒ«ï¼ˆHMMï¼‰ã€ç·šå½¢ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³ï¼ˆSVMï¼‰ã€ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãªã©ã®çµ±è¨ˆçš„æ©Ÿæ¢°å­¦ç¿’ã®æ‰‹æ³•ãŒä¸»æµã§ã—ãŸã€‚

ã“ã‚Œã‚‰ã®æ‰‹æ³•ã®ç‰¹å¾´ã¯ã€äººé–“ãŒæ‰‹å‹•ã§ç‰¹å¾´é‡ã‚’è¨­è¨ˆã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã“ã¨ã§ã™ã€‚ã¤ã¾ã‚Šã€ã¾ãšãƒ¢ãƒ‡ãƒ«ã«ã€Œã©ã®å˜èªãŒé‡è¦ã‹ã€ã€Œã©ã®ã‚ˆã†ãªæ§‹é€ ãŒæ„å‘³ã‚’æŒã¤ã‹ã€ã‚’æ•™ãˆã€ãã®ä¸Šã§ãƒ¢ãƒ‡ãƒ«ã«è¦å‰‡æ€§ã‚’å­¦ç¿’ã•ã›ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã—ãŸã€‚

ã“ã®ç¨®ã®æ‰‹æ³•ã¯å°è¦æ¨¡ãªã‚¿ã‚¹ã‚¯ã§ã¯éå¸¸ã«æœ‰åŠ¹ã§ã—ãŸãŒã€ä»Šæ—¥ã®ã‚ˆã†ãªå¤§è¦æ¨¡ã§è¤‡é›‘æ€§ã®é«˜ã„è¨€èªãƒ‡ãƒ¼ã‚¿ã«ç›´é¢ã™ã‚‹ã¨ã€æŠ€è¡“çš„ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãŒéœ²å‘ˆã™ã‚‹ã“ã¨ãŒå¤šãã‚ã‚Šã¾ã—ãŸã€‚

2010å¹´ä»£ã«å…¥ã‚‹ã¨ã€ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°æŠ€è¡“ã®ãƒ–ãƒ¬ãƒ¼ã‚¯ã‚¹ãƒ«ãƒ¼ãŒè‡ªç„¶è¨€èªå‡¦ç†ã®åˆ†é‡ã«æ–°ãŸãªç™ºå±•ã®å¥‘æ©Ÿã‚’ã‚‚ãŸã‚‰ã—ã¾ã—ãŸã€‚ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã€äººé–“ã«ã‚ˆã‚‹ç‰¹å¾´é‡è¨­è¨ˆã«ä¾å­˜ã›ãšã€ç”Ÿã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç›´æ¥è¡¨ç¾ï¼ˆrepresentationï¼‰ã‚’è‡ªå‹•çš„ã«å­¦ç¿’ã™ã‚‹ãŸã‚ã€è¤‡é›‘ãªè¨€èªã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†ã™ã‚‹èƒ½åŠ›ãŒè‘—ã—ãå‘ä¸Šã—ã¾ã—ãŸã€‚

2018å¹´ã€Googleã¯ç”»æœŸçš„ãªãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹BERTã‚’ç™ºè¡¨ã—ã¾ã—ãŸã€‚ã“ã‚Œã¯ã€ã¾ãšå¤§è¦æ¨¡ãªã‚³ãƒ¼ãƒ‘ã‚¹ã§æ±ç”¨çš„ãªè¨€èªèƒ½åŠ›ã‚’ã€Œäº‹å‰å­¦ç¿’ã€ã—ã€ãã®å¾Œã€å…·ä½“çš„ãªã‚¿ã‚¹ã‚¯ã«å¿œã˜ã¦ã€Œãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€ã‚’è¡Œã†ã‚‚ã®ã§ã™ã€‚ã“ã®ã€Œäº‹å‰å­¦ç¿’-ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€ã¨ã„ã†é–‹ç™ºãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã¯ã€NLPãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰æ–¹æ³•ã‚’æ ¹æœ¬çš„ã«å¤‰ãˆã€è¨“ç·´ã‚³ã‚¹ãƒˆã‚’å¤§å¹…ã«å‰Šæ¸›ã—ã€ã‚¿ã‚¹ã‚¯é–“ã®è»¢ç§»èƒ½åŠ›ã‚‚å‘ä¸Šã•ã›ã¾ã—ãŸã€‚ã“ã®æ®µéšã‹ã‚‰ã€NLPãƒ¢ãƒ‡ãƒ«ã¯æ–‡è„ˆã‚’ç†è§£ã—ã€å‰å¾Œã®æ„å‘³ã‚’æ‰ãˆã‚‹èƒ½åŠ›ã‚’å‚™ãˆã‚‹ã‚ˆã†ã«ãªã‚Šã€ã‚ˆã‚Šè¤‡é›‘ãªæ¨è«–ã€è³ªç–‘å¿œç­”ã€æ„Ÿæƒ…èªè­˜ãªã©ã®ã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚

2019å¹´ä»¥é™ã¯GPTã€T5ãªã©ã€æ§˜ã€…ãªãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãŒç™»å ´ã—ã€ã‚ˆã‚Šå¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§äº‹å‰å­¦ç¿’ã•ã‚Œã€æ€§èƒ½ãŒé£›èºçš„ã«å‘ä¸Šã—ã¾ã—ãŸã€‚LLMã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¨å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®è¦æ¨¡ã‚’æ‹¡å¤§ã™ã‚‹ã“ã¨ã§ã€é©šãã¹ãèƒ½åŠ›ã‚’ç™ºæ®ã™ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚æ•°åƒå„„ã€ã•ã‚‰ã«ã¯å…†å˜ä½ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ãŒç™»å ´ã—ã€ã“ã‚Œã«ã‚ˆã‚Šã€ã‚ˆã‚Šè¤‡é›‘ãªæ¨è«–ã€å¤šè¨€èªå¯¾å¿œã€ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚³ãƒ¼ãƒ‰ã®ç”Ÿæˆã€å‰µé€ çš„ãªæ–‡ç« ä½œæˆãªã©ã€å¹…åºƒã„ã‚¿ã‚¹ã‚¯ã§äººé–“ãƒ¬ãƒ™ãƒ«ã€ã‚ã‚‹ã„ã¯ãã‚Œã‚’è¶…ãˆã‚‹æ€§èƒ½ã‚’ç¤ºã™ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚
-->



---
transition: slide-up
level: 2
---

# LLMs Basics

What is LLMs?

<v-clicks depth="2">

- LLMs are deep neural networks trained on massive amounts of text data, designed to understand, generate, and respond to human-like text.
    - ***Large* refers to both the model's size in terms of parameters and the dataset**
        - LLMs often involves billions of parameters
        - LLMs were trained on a large amount of texts
    - **LLMs are capable of *generating text***
        - LLMs are often referred to as a form of generative AI.
        - LLMs can handle various task including answering questions, writing essays, translating languages through text generation
    - **LLMs utilize an architecture called the *Transformer***
        - Transformer  enables efficient processing of language by capturing complex patterns and relationships across long text sequences

</v-clicks>

<!--
å¤§è¦æ¨¡ãªãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã•ã‚ŒãŸæ·±å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯: LLMã¯ã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆä¸Šã®è†¨å¤§ãªæ›¸ç±ã€è¨˜äº‹ã€ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆãªã©ã€ã‚ã‚‰ã‚†ã‚‹ç¨®é¡ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã—ã¦ã„ã¾ã™ã€‚ã“ã®ã€Œå¤§è¦æ¨¡ã•ã€ãŒã€äººé–“ã®ã‚ˆã†ãªãƒ†ã‚­ã‚¹ãƒˆã‚’ç†è§£ã—ã€ç”Ÿæˆã—ã€å¿œç­”ã™ã‚‹èƒ½åŠ›ã®åŸºç›¤ã¨ãªã£ã¦ã„ã¾ã™ã€‚

ã€Œå¤§è¦æ¨¡ï¼ˆLargeï¼‰ã€ã®æ„å‘³:

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: ãƒ¢ãƒ‡ãƒ«ã®å†…éƒ¨æ§‹é€ ã‚’æ§‹æˆã™ã‚‹ã€Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ã®æ•°ãŒéå¸¸ã«å¤šã„ã“ã¨ã‚’æŒ‡ã—ã¾ã™ã€‚æ•°å„„ã€æ•°åå„„ã€ã•ã‚‰ã«ã¯æ•°å…†ã«åŠã¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã‚‚å­˜åœ¨ã—ã¾ã™ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¤šã„ã»ã©ã€ã‚ˆã‚Šè¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚„é–¢ä¿‚æ€§ã‚’å­¦ç¿’ã§ãã¾ã™ã€‚
è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: å­¦ç¿’ã«ç”¨ã„ã‚‰ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®é‡ãŒè†¨å¤§ã§ã‚ã‚‹ã“ã¨ã‚’æŒ‡ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å¤šæ§˜ãªè¨€èªè¡¨ç¾ã‚„çŸ¥è­˜ã‚’ç²å¾—ã—ã¾ã™ã€‚
ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆèƒ½åŠ›ï¼ˆGenerative AIï¼‰: LLMã¯ã€ä¸ãˆã‚‰ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚„æ–‡è„ˆã«åŸºã¥ã„ã¦ã€æ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹èƒ½åŠ›ã«å„ªã‚Œã¦ã„ã¾ã™ã€‚ãã®ãŸã‚ã€ã€Œç”ŸæˆAIï¼ˆGenerative AIï¼‰ã€ã®ä¸€ç¨®ã¨ã—ã¦åºƒãèªè­˜ã•ã‚Œã¦ã„ã¾ã™ã€‚æ–‡ç« ä½œæˆã€è¦ç´„ã€ç¿»è¨³ã€ã‚³ãƒ¼ãƒ‰ç”Ÿæˆãªã©ã€å¤šå²ã«ã‚ãŸã‚‹ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚

Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®åˆ©ç”¨: LLMã®ã»ã¨ã‚“ã©ã¯ã€ã€ŒTransformerï¼ˆãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ï¼‰ã€ã¨å‘¼ã°ã‚Œã‚‹ç‰¹åˆ¥ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’åŸºç›¤ã¨ã—ã¦ã„ã¾ã™ã€‚Transformerã¯ã€ãƒ†ã‚­ã‚¹ãƒˆå†…ã®å˜èªé–“ã®é–¢ä¿‚æ€§ï¼ˆæ–‡è„ˆï¼‰ã‚’åŠ¹ç‡çš„ã«æ‰ãˆã‚‹ã“ã¨ãŒã§ãã‚‹ãŸã‚ã€é•·æ–‡ã®ç†è§£ã‚„ç”Ÿæˆã«ãŠã„ã¦éå¸¸ã«é«˜ã„æ€§èƒ½ã‚’ç™ºæ®ã—ã¾ã™ã€‚BERTã‚„GPTã¨ã„ã£ãŸæœ‰åãªãƒ¢ãƒ‡ãƒ«ã‚‚ã€ã“ã®Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚

è¦ã™ã‚‹ã«ã€LLMã¯è†¨å¤§ãªãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã—ã€Transformerã¨ã„ã†åŠ¹ç‡çš„ãªæ§‹é€ ã‚’ä½¿ã£ã¦ã€äººé–“ã®ã‚ˆã†ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’ç†è§£ã—ã€ãã—ã¦å‰µé€ çš„ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹èƒ½åŠ›ã‚’æŒã¤ã€éå¸¸ã«å¤§è¦æ¨¡ãªAIãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ã¨è¨€ãˆã¾ã™ã€‚ 
-->


---
transition: slide-up
level: 2
---

# LLMs Basics

What is GPT?

<v-clicks depth="2">

- G: **G**enerative model
    - GPT are trained to predict the next word in a sequence
- P: **P**re-trained
    - GPT models undergo an extensive training phase on a massive dataset of text
    - Pre-trained allows model to learn a vast amount of linguistic patterns, facts, reasoning abilities, and general knowledge.
- T: **T**ransformer
    - GPT models are built on the Transformer architecture


</v-clicks>

---
transition: slide-up
level: 2
---

# LLMs Basics

Language Models(LMs)

- LMs  designed to understand and generate human language by assigning probabilities to sequences of words
    - **Setup**: Assume a vocabulary of words  
  $$
  V = \{W_1, W_2, W_3, \ldots, W_n\}
  $$

    - **Data**: Given a training set of example sentences

    - **Goal**: Estimate a probability distribution  
  $$
  \sum_{x \in V^{*}} p(x) = 1
  $$

<!--
Language Models (LMs)ã¯ã€äººé–“ã®è¨€èªã‚’ç†è§£ã—ç”Ÿæˆã™ã‚‹ãŸã‚ã«ã€å˜èªã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ç¢ºç‡ã‚’å‰²ã‚Šå½“ã¦ã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚

ã‚ã‚‹å˜èªã®å¾Œã«ã©ã®ã‚ˆã†ãªå˜èªãŒç¶šãã‹ã€ã‚ã‚‹ã„ã¯ã‚ã‚‹å˜èªã®ä¸¦ã³å…¨ä½“ãŒã©ã‚Œãã‚‰ã„ã®ç¢ºç‡ã§ç™ºç”Ÿã™ã‚‹ã‹ã‚’æ•°å­¦çš„ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è¨€èªã®è¦å‰‡æ€§ã‚„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã€æ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ãŸã‚Šã€æ—¢å­˜ã®ãƒ†ã‚­ã‚¹ãƒˆã®å¦¥å½“æ€§ã‚’è©•ä¾¡ã—ãŸã‚Šã™ã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

- è¨€èªãƒ¢ãƒ‡ãƒ«ãŒæ‰±ã†ã“ã¨ã®ã§ãã‚‹ã™ã¹ã¦ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªã®é›†åˆã‚’ç”¨æ„ã—ã¾ã™ã€‚
- è¨“ç·´ã‚»ãƒƒãƒˆã®ä¾‹æ–‡: ã“ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€ã©ã®å˜èªãŒã©ã®å˜èªã®å¾Œã«ç¾ã‚Œã‚‹ã“ã¨ãŒå¤šã„ã‹ã€ã©ã®ã‚ˆã†ãªå˜èªã®ä¸¦ã³ãŒè‡ªç„¶ã‹ã€ã¨ã„ã£ãŸæƒ…å ±ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
- ç›®æ¨™ã¯ç¢ºç‡åˆ†å¸ƒã®æ¨å®š: è¨€èªãƒ¢ãƒ‡ãƒ«ã®ç©¶æ¥µã®ç›®æ¨™ã¯ã€å¯èƒ½ãªã™ã¹ã¦ã®å˜èªã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã™ã‚‹ç¢ºç‡åˆ†å¸ƒ$p(x)$ã‚’æ¨å®šã™ã‚‹ã“ã¨ã§ã™ã€‚èªå½™ V ã‹ã‚‰æ§‹æˆã•ã‚Œã‚‹ã™ã¹ã¦ã®å¯èƒ½ãªå˜èªã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®é›†åˆã‚’è¡¨ã—ã¾ã™ã€‚ã“ã®å¼ã¯ã€ã™ã¹ã¦ã®å¯èƒ½ãªå˜èªã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ç¢ºç‡ã‚’åˆè¨ˆã™ã‚‹ã¨1ã«ãªã‚‹ã€‚ã¤ã¾ã‚Šã€ãƒ¢ãƒ‡ãƒ«ã¯ã‚ã‚‰ã‚†ã‚‹å˜èªã®ä¸¦ã³ã«å¯¾ã—ã¦ã€ãã‚ŒãŒã©ã‚Œã ã‘ã€Œã‚ã‚Šãã†ã‹ã€ã¨ã„ã†ç¢ºç‡ã‚’å‰²ã‚Šå½“ã¦ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¾ã™ã€‚
-->


---
transition: slide-up
level: 2
---

# LLMs Basics

Next-word Prediction Task

<div class="flex justify-center">
  <img src="./image/next.png" width="800" />
</div>


---
transition: slide-up
level: 2
---

# LLMs Basics

Text Generation

<div class="flex justify-center">
  <img src="./image/generative.png" width="550" />
</div>


---
transition: slide-up
level: 2
---

# LLMs Basics

Transformer

<div grid="~ cols-2 gap-4 items-start">

<div>

<v-clicks depth="2">

- The Transformer has significantly advanced the field of NLP and is applied across a wide range of large language models.
- **Self-attention** mechanism allows the model to compute the relevance of each element in a sequence and use this information to understand the context. 
    - Handling of **Long-Range Dependencies**
    - **Parallelization** for training truly "large" language models
</v-clicks>

</div>

<div class="flex justify-center">
  <img src="./image/transformer_frame.png" alt="ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³" width="800" />
</div>

</div>

---
transition: slide-up
level: 2
---

# LLMs Basics

Seq2seq

<div grid="~ cols-2 gap-4 items-start">

<div>

<v-clicks depth="2">

- The Transformer is a type of **Seq2seq (Sequence-to-Sequence)** model
- A Seq2Seq model is an architecture designed to transform an input sequence into an output sequence

    - **Encoder**: Process the input sequence and compress its information into a fixed-size representation called the context vector
    
    - **Decoder**: Use encoded information (context vector) to generate the output sequence.

</v-clicks>

</div>

<div class="flex justify-center">
  <img src="./image/encoder-and-decoder-transformers-blocks.png" alt="ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³" width="180" />
</div>

</div>


<!--
Transformerã¯Seq2Seq (Sequence-to-Sequence) ãƒ¢ãƒ‡ãƒ«ã®ä¸€ç¨®ã§ã‚ã‚‹.

Seq2Seqãƒ¢ãƒ‡ãƒ«ã¯ã€å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å‡ºåŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¤‰æ›ã™ã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚ŒãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã‚ã‚‹

- å¾“æ¥ã®Seq2Seqãƒ¢ãƒ‡ãƒ«ï¼ˆãƒªã‚«ãƒ¬ãƒ³ãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (RNN) ã‚„ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (CNN) ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ãŸã‚‚ã®ï¼‰ã¨ã¯ç•°ãªã‚‹ã€ã‚ˆã‚ŠåŠ¹ç‡çš„ã§é«˜æ€§èƒ½ãªãƒ¡ã‚«ãƒ‹ã‚ºãƒ ï¼ˆè‡ªå·±æ³¨æ„æ©Ÿæ§‹ãªã©ï¼‰ã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ç‚¹ãŒç‰¹å¾´ã§ã™ã€‚

- Encoder (ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€):å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å‡¦ç†ã—ã€ãã®æƒ…å ±ã‚’å›ºå®šã‚µã‚¤ã‚ºã®è¡¨ç¾ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã«åœ§ç¸®ã™ã‚‹ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å½¹å‰²ã¯ã€å…¥åŠ›ã•ã‚ŒãŸæ–‡ç« ã‚„ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã®æ„å‘³å†…å®¹ã‚’æ‰ãˆã€ãã‚Œã‚’ãƒ‡ã‚³ãƒ¼ãƒ€ãŒåˆ©ç”¨ã§ãã‚‹å½¢ã«å¤‰æ›ã™ã‚‹ã“ã¨ã§ã™ã€‚ã“ã®ã€Œå›ºå®šã‚µã‚¤ã‚ºã®è¡¨ç¾ã€ã¯ã€å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ã™ã¹ã¦ã®é‡è¦ãªæƒ…å ±ãŒå‡ç¸®ã•ã‚ŒãŸã‚‚ã®ã¨è¦‹ãªã•ã‚Œã¾ã™ã€‚å¾“æ¥ã®Seq2Seqãƒ¢ãƒ‡ãƒ«ã§ã¯ã€ãƒªã‚«ãƒ¬ãƒ³ãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æœ€å¾Œã®éš ã‚ŒçŠ¶æ…‹ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦ä½¿ã‚ã‚Œã‚‹ã“ã¨ãŒå¤šã‹ã£ãŸã§ã™ã€‚
Decoder (ãƒ‡ã‚³ãƒ¼ãƒ€):

- ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸæƒ…å ±ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã‚’ä½¿ç”¨ã—ã¦å‡ºåŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ ãƒ‡ã‚³ãƒ¼ãƒ€ã¯ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãŒä½œæˆã—ãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ«ã‚’åŸºã«ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨€èªã‚„ç›®çš„ã¨ã™ã‚‹å‡ºåŠ›å½¢å¼ã§å˜èªã‚’ä¸€ã¤ãšã¤ç”Ÿæˆã—ã¦ã„ãã¾ã™ã€‚ç”Ÿæˆã®éš›ã«ã¯ã€ãã‚Œã¾ã§ã«ç”Ÿæˆã•ã‚ŒãŸå˜èªã‚‚è€ƒæ…®ã«å…¥ã‚Œã€æ–‡è„ˆã«åˆã£ãŸæ¬¡ã®å˜èªã‚’é¸æŠã—ã¾ã™ã€‚
-->


---
transition: slide-up
level: 2
---

# LLMs Basics

Seq2seq

<video controls width="700" style="display: block; margin-left: auto; margin-right: auto;">
  <source src="./image/seq2seq_training_with_target.mp4" type="video/mp4">
</video>



---
transition: slide-up
level: 2
---

# LMs before the Transformer

N-gram Language Model

- **ğŸ” Markov assumption**: The probability of a word depends only on the previous \( N-1 \) words.  
  $$
  P(w_n \mid w_{1:n-1}) = P(w_n \mid w_{n-N+1:n-1})
  $$

- For a sentence *students opened their __[blank]__*  
  $$
  P(w) = \frac{\text{count(opened their } w)}{\text{count(opened their)}}
  $$

<div grid="~ cols-2 gap-4 items-start">

<div>

<v-clicks depth="2">

- An N-gram is a contiguous sequence of $N$ items from a given text.
- N-gram language model refers to a probabilistic model that can estimate the probability of a word given the $n-1$ previous words.
</v-clicks>

</div>

<div class="flex justify-center">
  <img src="./image/n-gram.png" alt="ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³" width="800" />
</div>

</div>



<!--
ãƒãƒ«ã‚³ãƒ•ä»®å®š (Markov assumption): 

$$P(w_n \mid w_{1:n-1}) = P(w_n \mid w_{n-N+1:n-1})$$
ã“ã®ä»®å®šã¯ã€N-gramãƒ¢ãƒ‡ãƒ«ã®æ ¹å¹¹ã‚’ãªã™ã‚‚ã®ã§ã™ã€‚
- å·¦è¾º $P(w_n \mid w_{1:n-1})$ ã¯ã€ã€Œç¾åœ¨ã®å˜èª $w_n$ ã®ç¢ºç‡ãŒã€ãã®å‰ã«ç¾ã‚ŒãŸã™ã¹ã¦ã®å˜èª $w_1, \ldots, w_{n-1}$ ã«ä¾å­˜ã™ã‚‹ã€ã¨ã„ã†ç†æƒ³çš„ãªï¼ˆã—ã‹ã—è¨ˆç®—ãŒéå¸¸ã«å›°é›£ãªï¼‰çŠ¶æ³ã‚’ç¤ºã—ã¾ã™ã€‚
- å³è¾º $P(w_n \mid w_{n-N+1:n-1})$ ã¯ã€ã€Œç¾åœ¨ã®å˜èª $w_n$ ã®ç¢ºç‡ã¯ã€ãã®ç›´å‰ã® $N-1$ å€‹ã®å˜èª $w_{n-N+1}, \ldots, w_{n-1}$ ã®ã¿ã«ä¾å­˜ã™ã‚‹ã€ã¨ã„ã†ç°¡ç•¥åŒ–ã•ã‚ŒãŸä»®å®šã§ã™ã€‚

ã¤ã¾ã‚Šã€**éå»ã®ã™ã¹ã¦ã®æ–‡è„ˆã‚’è€ƒæ…®ã™ã‚‹ã®ã§ã¯ãªãã€ç›´è¿‘ã®ä¸€å®šæ•°ã®å˜èªã®ã¿ã‚’æ–‡è„ˆã¨ã—ã¦è€ƒæ…®ã™ã‚‹**ã¨ã„ã†è€ƒãˆæ–¹ã§ã™ã€‚ã“ã®ã€Œä¸€å®šã®æ•°ã€ãŒ $N-1$ ã§ã™ã€‚$N$ ã®å€¤ãŒå¤§ãã„ã»ã©ã€ã‚ˆã‚Šå¤šãã®æ–‡è„ˆã‚’è€ƒæ…®ã§ãã¾ã™ãŒã€ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘æ€§ã¨ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã®å•é¡ŒãŒå¢—å¤§ã—ã¾ã™ã€‚

$$P(w) = \frac{\text{count(opened their } w)}{\text{count(opened their)}}$$

ã“ã‚Œã¯ã€**3-gramï¼ˆã¾ãŸã¯Trigramï¼‰ãƒ¢ãƒ‡ãƒ«** ã®å ´åˆã®ä¾‹ã§ã™ã€‚ã“ã®å¼ã¯ã€ç©ºç™½ã«å½“ã¦ã¯ã¾ã‚‹å˜èª $w$ ãŒæ¥ã‚‹ç¢ºç‡ã‚’è¨ˆç®—ã—ã¦ã„ã¾ã™ã€‚

- $\text{count(opened their } w)$ ã¯ã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä¸­ã«ã€Œopened theirã€ã®å¾Œã«ç‰¹å®šã®å˜èª $w$ ãŒç¶šãå›æ•°ã‚’ç¤ºã—ã¾ã™ã€‚ä¾‹ãˆã°ã€ã€Œopened their booksã€ã¨ã„ã†ãƒ•ãƒ¬ãƒ¼ã‚ºãŒä½•å›å‡ºç¾ã—ãŸã‹ã€‚
- $\text{count(opened their)}$ ã¯ã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä¸­ã«ã€Œopened theirã€ã¨ã„ã†ãƒ•ãƒ¬ãƒ¼ã‚ºãŒä½•å›å‡ºç¾ã—ãŸã‹ã‚’ç¤ºã—ã¾ã™ã€‚

ã—ãŸãŒã£ã¦ã€ã“ã®å¼ã¯ã€**ã€Œopened theirã€ã¨ã„ã†æ–‡è„ˆãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã«ã€æ¬¡ã«å˜èª $w$ ãŒç¶šãæ¡ä»¶ä»˜ãç¢ºç‡** ã‚’æ¨å®šã—ã¦ã„ã¾ã™ã€‚
-->



---
transition: slide-up
level: 2
---

# LMs before the Transformer

N-gram Language Model

<v-clicks depth="2">

- âš ï¸Computational complexity 
    - As $N$ increases (to capture more context), the number of possible unique N-grams grows exponentially with the size of the vocabulary. 
    - Example: For a vocabulary of 50,000 words, a 5-gram model would theoretically need to store $(50,000)^5$ counts

- âš ï¸Low generalization 
    - Language often has dependencies that span many words: The fixed-size context is a fundamental limitation.
    - Inability to capture long-range dependencies means N-gram models cannot truly understand the deep syntactic or semantic relationships in sentences.
</v-clicks>

<!--

- **âš ï¸ è¨ˆç®—ä¸Šã®è¤‡é›‘æ€§ (Computational complexity)**
    - **$N$ãŒå¢—åŠ ã™ã‚‹ã¨ï¼ˆã‚ˆã‚Šå¤šãã®æ–‡è„ˆã‚’æ‰ãˆã‚‹ãŸã‚ï¼‰ã€å¯èƒ½ãªãƒ¦ãƒ‹ãƒ¼ã‚¯ãªN-gramã®æ•°ãŒèªå½™ã‚µã‚¤ã‚ºã«å¯¾ã—ã¦æŒ‡æ•°é–¢æ•°çš„ã«å¢—åŠ ã™ã‚‹ã€‚**
        ã“ã‚Œã¯N-gramãƒ¢ãƒ‡ãƒ«ã®æœ€ã‚‚æ·±åˆ»ãªå•é¡Œã®ä¸€ã¤ã§ã™ã€‚N-gramã¯åŸºæœ¬çš„ã«ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä¸­ã«ç¾ã‚Œã‚‹é€£ç¶šã™ã‚‹å˜èªã®ä¸¦ã³ï¼ˆN-gramï¼‰ã®é »åº¦ã‚’æ•°ãˆä¸Šã’ã¦ç¢ºç‡ã‚’æ¨å®šã—ã¾ã™ã€‚$N$ãŒå¤§ãããªã‚‹ã¨ã€è€ƒãˆã‚‰ã‚Œã‚‹å˜èªã®çµ„ã¿åˆã‚ã›ãŒçˆ†ç™ºçš„ã«å¢—ãˆã‚‹ãŸã‚ã€æ¬¡ã®å•é¡ŒãŒç”Ÿã˜ã¾ã™ã€‚
    - **ä¾‹ï¼šèªå½™ã‚µã‚¤ã‚ºãŒ50,000èªã®å ´åˆã€5-gramãƒ¢ãƒ‡ãƒ«ã¯ç†è«–ä¸Š $(50,000)^5$ ã®ã‚«ã‚¦ãƒ³ãƒˆã‚’ä¿å­˜ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚**
        ã“ã‚Œã¯ $3.125 \times 10^{23}$ ã¨ã„ã†å¤©æ–‡å­¦çš„ãªæ•°å­—ã«ãªã‚Šã¾ã™ã€‚ç¾å®Ÿçš„ã«ã¯ã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã™ã‚‹N-gramã®ã¿ã‚’ä¿å­˜ã—ã¾ã™ãŒã€ãã‚Œã§ã‚‚ãã®æ•°ã¯è†¨å¤§ã«ãªã‚Šã€ä»¥ä¸‹ã®å•é¡Œã‚’å¼•ãèµ·ã“ã—ã¾ã™ã€‚
        * **ãƒ¡ãƒ¢ãƒªã¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã®æ¶ˆè²»:** è†¨å¤§ãªæ•°ã®N-gramã¨ãã®ã‚«ã‚¦ãƒ³ãƒˆã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã«ã€é€”æ–¹ã‚‚ãªã„é‡ã®ãƒ¡ãƒ¢ãƒªã¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚
        * **è¨ˆç®—æ™‚é–“:** ç¢ºç‡ã®è¨ˆç®—ã‚„æ›´æ–°ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚
        * **ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ (Data Sparsity):** ã“ã‚ŒãŒæœ€ã‚‚å¤§ããªå•é¡Œã§ã™ã€‚ãŸã¨ãˆå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒå¤§ããã¦ã‚‚ã€$N$ãŒå¤§ãããªã‚‹ã¨ã€ã»ã¨ã‚“ã©ã®N-gramã®çµ„ã¿åˆã‚ã›ã¯ä¸€åº¦ã‚‚å‡ºç¾ã—ãªã„ï¼ˆã‚«ã‚¦ãƒ³ãƒˆãŒã‚¼ãƒ­ã«ãªã‚‹ï¼‰å¯èƒ½æ€§ãŒé«˜ã¾ã‚Šã¾ã™ã€‚ã“ã®å•é¡Œã¯ã€Œã‚¼ãƒ­é »åº¦å•é¡Œã€ã¨å‘¼ã°ã‚Œã€N-gramãƒ¢ãƒ‡ãƒ«ã®æ±åŒ–èƒ½åŠ›ã‚’è‘—ã—ãä½ä¸‹ã•ã›ã¾ã™ã€‚ä¾‹ãˆã°ã€ã€Œunusual blue carã€ã¨ã„ã†ãƒ•ãƒ¬ãƒ¼ã‚ºãŒè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«ãªãã¦ã‚‚ã€å€‹ã€…ã®å˜èªã¯é »ç¹ã«ç¾ã‚Œã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ã—ã‹ã—ã€3-gramãƒ¢ãƒ‡ãƒ«ã§ã¯ãã®ãƒ•ãƒ¬ãƒ¼ã‚ºã®ç¢ºç‡ã¯ã‚¼ãƒ­ã¨è©•ä¾¡ã•ã‚Œã¦ã—ã¾ã„ã¾ã™ã€‚ã“ã‚Œã‚’ç·©å’Œã™ã‚‹ãŸã‚ã«ã€ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ï¼ˆå¹³æ»‘åŒ–ï¼‰ãªã©ã®æ‰‹æ³•ãŒç”¨ã„ã‚‰ã‚Œã¾ã™ãŒã€æ ¹æœ¬çš„ãªè§£æ±ºã«ã¯ãªã‚Šã¾ã›ã‚“ã€‚

- âš ï¸ æ±åŒ–èƒ½åŠ›ã®ä½ã• (Low generalization)
    - **è¨€èªã¯ã—ã°ã—ã°ã€å¤šãã®å˜èªã«ã¾ãŸãŒã‚‹ä¾å­˜é–¢ä¿‚ã‚’æŒã¤ï¼šå›ºå®šã‚µã‚¤ã‚ºã®æ–‡è„ˆã¯æ ¹æœ¬çš„ãªåˆ¶é™ã§ã‚ã‚‹ã€‚**
        N-gramãƒ¢ãƒ‡ãƒ«ã¯ãƒãƒ«ã‚³ãƒ•ä»®å®šã«åŸºã¥ã„ã¦ã„ã‚‹ãŸã‚ã€$N-1$å€‹ã®ç›´å‰ã®å˜èªã—ã‹è€ƒæ…®ã—ã¾ã›ã‚“ã€‚ã“ã‚Œã¯ã€æ–‡ç« å…¨ä½“ã®æ„å‘³ã‚„æ–‡æ³•æ§‹é€ ã‚’ç†è§£ã™ã‚‹ä¸Šã§è‡´å‘½çš„ãªåˆ¶ç´„ã¨ãªã‚Šã¾ã™ã€‚ä¾‹ãˆã°ã€ä»¥ä¸‹ã®ã‚ˆã†ãªæ–‡ã‚’è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚
        ã€Œå¤ªéƒãŒã€æ˜¨æ—¥å…¬åœ’ã§éŠã‚“ã§ã„ãŸèŠ±å­ã«ã€è©±ã—ã‹ã‘ãŸã€‚ã€
        ã“ã®æ–‡ã§ã¯ã€ã€Œå¤ªéƒã€ã¨ã„ã†ä¸»èªãŒã€Œè©±ã—ã‹ã‘ãŸã€ã¨ã„ã†å‹•è©ã«å¯¾å¿œã—ã¦ãŠã‚Šã€ä¸¡è€…ã®é–“ã«ã¯å¤šãã®å˜èªãŒæŒŸã¾ã£ã¦ã„ã¾ã™ã€‚ã—ã‹ã—ã€N-gramãƒ¢ãƒ‡ãƒ«ã¯ $N-1$ ã®çª“ã‚’è¶…ãˆãŸä¾å­˜é–¢ä¿‚ã‚’æ‰ãˆã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã€‚
    - **é•·è·é›¢ä¾å­˜é–¢ä¿‚ã‚’æ‰ãˆã‚‹èƒ½åŠ›ãŒãªã„ã“ã¨ã¯ã€N-gramãƒ¢ãƒ‡ãƒ«ãŒæ–‡ä¸­ã®æ·±ã„çµ±èªçš„ï¼ˆæ§‹æ–‡çš„ï¼‰ã¾ãŸã¯æ„å‘³çš„é–¢ä¿‚ã‚’çœŸã«ç†è§£ã§ããªã„ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚**
        ã“ã‚Œã¯ä¸Šè¨˜ã®ç‚¹ã¨é–¢é€£ã—ã¦ã„ã¾ã™ã€‚äººé–“ãŒè¨€èªã‚’ç†è§£ã™ã‚‹éš›ã«ã¯ã€æ–‡é ­ã‹ã‚‰æ–‡æœ«ã¾ã§ã€ã‚ã‚‹ã„ã¯æ®µè½å…¨ä½“ã¨ã„ã£ãŸåºƒç¯„å›²ã®æ–‡è„ˆã‚’è€ƒæ…®ã—ã¦æ„å‘³ã‚’è§£é‡ˆã—ã¾ã™ã€‚N-gramãƒ¢ãƒ‡ãƒ«ã¯ã“ã®ã‚ˆã†ãªé•·è·é›¢ã®ä¾å­˜é–¢ä¿‚ï¼ˆlong-range dependenciesï¼‰ã‚’æ‰±ã†ã“ã¨ãŒã§ããªã„ãŸã‚ã€æ–‡æ³•çš„ã«æ­£ã—ã„ãŒæ„å‘³çš„ã«ä¸è‡ªç„¶ãªæ–‡ç« ã‚’ç”Ÿæˆã—ãŸã‚Šã€è¤‡é›‘ãªè³ªå•å¿œç­”ã‚„è¦ç´„ã¨ã„ã£ãŸã‚¿ã‚¹ã‚¯ã§è‹¦æˆ¦ã—ãŸã‚Šã—ã¾ã™ã€‚
-->


---
transition: slide-up
level: 2
---

# Language Model with RNNs

Recurrent Neural Networks (RNNs)

<div grid="~ cols-2 gap-4 items-start">

<div>

<v-clicks depth="2">

- Compute an output $y_t$ for an input $x_t$ requires activation value for the hidden layer $h_t$.
    - $h_t$ is calculated based on the input $x_t$ and the hidden layer from the previous time step $h_{t-1}$.
</v-clicks>

</div>

<div class="flex justify-center">
  <img src="./image/rnn_component.png" alt="ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³" width="800" />
</div>

</div>


<div grid="~ cols-2 gap-4 items-start">

<div>

<v-clicks depth="2">

- Computation at time $t$ requires the value of the hidden layer from time $t âˆ’ 1$ mandates an incremental inference algorithm that proceeds from the start of the sequence to the end. 
</v-clicks>

</div>

<div class="flex justify-center">
  <img src="./image/rnn_sequence.png" alt="ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³" width="800" />
</div>

</div>


<!--

ãƒªã‚«ãƒ¬ãƒ³ãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆRNNsï¼‰ã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ã†ãŸã‚ã«è¨­è¨ˆã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ä¸€ç¨®ã§ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã®åˆ†é‡ã§å¤§ããªé€²æ­©ã‚’ã‚‚ãŸã‚‰ã—ã¾ã—ãŸã€‚

* **å…¥åŠ› $x_t$ ã«å¯¾ã—ã¦å‡ºåŠ› $y_t$ ã‚’è¨ˆç®—ã™ã‚‹ã«ã¯ã€éš ã‚Œå±¤ã®æ´»æ€§åŒ–å€¤ $h_t$ ãŒå¿…è¦ã§ã‚ã‚‹ã€‚**
    * **$h_t$ ã¯ã€å…¥åŠ› $x_t$ ã¨å‰ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®éš ã‚Œå±¤ $h_{t-1}$ ã«åŸºã¥ã„ã¦è¨ˆç®—ã•ã‚Œã‚‹ã€‚**

    ã“ã®2ã¤ã®ç‚¹ã¯ã€RNNã®æœ€ã‚‚é‡è¦ãªç‰¹å¾´ã§ã‚ã‚‹ã€Œå†å¸°æ€§ï¼ˆRecurrenceï¼‰ã€ã¨ã€Œè¨˜æ†¶ï¼ˆMemoryï¼‰ã€ã‚’èª¬æ˜ã—ã¦ã„ã¾ã™ã€‚

    * **$x_t$**: ç¾åœ¨ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆæ™‚åˆ» $t$ï¼‰ã«ãŠã‘ã‚‹å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚è¨€èªãƒ¢ãƒ‡ãƒ«ã®æ–‡è„ˆã§ã¯ã€ã“ã‚Œã¯é€šå¸¸ã€ç¾åœ¨ã®å˜èªã®åŸ‹ã‚è¾¼ã¿ï¼ˆå˜èªãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã«ç›¸å½“ã—ã¾ã™ã€‚
    * **$y_t$**: ç¾åœ¨ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã«ãŠã‘ã‚‹å‡ºåŠ›ã§ã™ã€‚è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã¯ã€æ¬¡ã«ç¶šãå˜èªã®ç¢ºç‡åˆ†å¸ƒã‚„ã€ç¾åœ¨ã®å˜èªãŒä½•ã§ã‚ã‚‹ã‹ã®äºˆæ¸¬ãªã©ã«ãªã‚Šã¾ã™ã€‚
    * **$h_t$**: ç¾åœ¨ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã«ãŠã‘ã‚‹ã€Œéš ã‚ŒçŠ¶æ…‹ï¼ˆhidden stateï¼‰ã€ã¾ãŸã¯ã€Œéš ã‚Œå±¤ã®æ´»æ€§åŒ–å€¤ã€ã§ã™ã€‚ã“ã‚Œã¯ã€**ã“ã‚Œã¾ã§ã®ã™ã¹ã¦ã®å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ï¼ˆ$x_1, \ldots, x_t$ï¼‰ã®æƒ…å ±ã‚’é›†ç´„ã—ãŸã€Œè¨˜æ†¶ã€** ã®ã‚ˆã†ãªå½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚
    * **$h_{t-1}$**: å‰ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆæ™‚åˆ» $t-1$ï¼‰ã®éš ã‚ŒçŠ¶æ…‹ã§ã™ã€‚$h_t$ ã®è¨ˆç®—ã« $h_{t-1}$ ãŒå«ã¾ã‚Œã‚‹ã“ã¨ã§ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯éå»ã®æƒ…å ±ã‚’ç¾åœ¨ã®å‡¦ç†ã«ã€Œå†å¸°çš„ã«ã€åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

* **RNNã®åˆ©ç‚¹:** RNNã¯ã€éš ã‚ŒçŠ¶æ…‹ $h_t$ ã‚’ä»‹ã—ã¦éå»ã®ã™ã¹ã¦ã®æƒ…å ±ã‚’ã€Œè¨˜æ†¶ã€ã—ã€ãã‚Œã‚’ç¾åœ¨ã®è¨ˆç®—ã«åˆ©ç”¨ã—ã¾ã™ã€‚ç†è«–çš„ã«ã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æœ€åˆã‹ã‚‰ã®æƒ…å ±ã‚’ä¿æŒã—ç¶šã‘ã‚‹ã“ã¨ãŒã§ãã€ã“ã‚Œã«ã‚ˆã‚ŠN-gramã§ã¯ä¸å¯èƒ½ã ã£ãŸ**é•·è·é›¢ä¾å­˜é–¢ä¿‚**ã‚’å­¦ç¿’ã—ã€ã‚ˆã‚Šæ·±ã„æ–‡è„ˆã‚’ç†è§£ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚ˆã‚Šæ–‡æ³•çš„ã«æ­£ã—ãã€æ„å‘³çš„ã«ä¸€è²«ã—ãŸæ–‡ç« ã‚’ç”Ÿæˆã—ãŸã‚Šã€è¤‡é›‘ãªæ„å‘³ç†è§£ã‚’ä¼´ã†ã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†ã—ãŸã‚Šã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

 **N-gramã®é™ç•Œ:** N-gramã¯å›ºå®šé•·ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§ã—ã‹æ–‡è„ˆã‚’è¡¨ç¾ã§ãã¾ã›ã‚“ã€‚Nã‚’å¤§ããã™ã‚‹ã¨ã€è¨ˆç®—é‡ã¨ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã®å•é¡ŒãŒçˆ†ç™ºçš„ã«å¢—åŠ ã—ã¾ã™ã€‚
    * **RNNã®åˆ©ç‚¹:** RNNã¯ã€éš ã‚ŒçŠ¶æ…‹ã‚’é€šã˜ã¦å¯å¤‰é•·ã®æ–‡è„ˆã‚’å‹•çš„ã«è¡¨ç¾ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å˜èªã®å‡ºç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ã ã‘ã§ãªãã€æ–‡ã®æ§‹é€ ã‚„æ„å‘³ã¨ã„ã£ãŸã‚ˆã‚ŠæŠ½è±¡çš„ãªç‰¹å¾´ã‚’å­¦ç¿’ã§ãã¾ã™ã€‚N-gramã®ã‚ˆã†ã« $N$ ã‚’å¢—ã‚„ã™ã“ã¨ã«ã‚ˆã‚‹è¨ˆç®—é‡ã®çˆ†ç™ºçš„ãªå¢—åŠ ã‚„ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã®å•é¡Œã«æ‚©ã¾ã•ã‚Œã‚‹ã“ã¨ãŒå°‘ãªããªã‚Šã¾ã™ã€‚

* **æ™‚åˆ» $t$ ã«ãŠã‘ã‚‹è¨ˆç®—ã«ã¯ã€æ™‚åˆ» $t-1$ ã«ãŠã‘ã‚‹éš ã‚Œå±¤ã®å€¤ãŒå¿…è¦ã§ã‚ã‚‹ãŸã‚ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å…ˆé ­ã‹ã‚‰æœ€å¾Œã¾ã§å‡¦ç†ã‚’é€²ã‚ã‚‹é€æ¬¡çš„ãªæ¨è«–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒå¿…é ˆã¨ãªã‚‹ã€‚**

    ã“ã‚Œã¯RNNã®ã‚‚ã†ä¸€ã¤ã®é‡è¦ãªç‰¹æ€§ã§ã‚ã‚Šã€åŒæ™‚ã«å¤§ããªåˆ¶ç´„ã§ã‚‚ã‚ã‚Šã¾ã™ã€‚
    * **é€æ¬¡çš„ãªè¨ˆç®— (Sequential Computation):** å„ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®è¨ˆç®—ã¯ã€å‰ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®è¨ˆç®—çµæœã«ä¾å­˜ã™ã‚‹ãŸã‚ã€ä¸¦åˆ—åŒ–ãŒéå¸¸ã«å›°é›£ã§ã™ã€‚ã“ã‚Œã¯ã€ç‰¹ã«é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æ‰±ã†å ´åˆã«ã€è¨“ç·´ã‚„æ¨è«–ã®é€Ÿåº¦ãŒé…ããªã‚‹åŸå› ã¨ãªã‚Šã¾ã™ã€‚
    * **æƒ…å ±ã®ãƒ•ãƒ­ãƒ¼:** éš ã‚ŒçŠ¶æ…‹ãŒæ¬¡ã€…ã«æ›´æ–°ã•ã‚Œã¦ã„ãã“ã¨ã§ã€ç†è«–çš„ã«ã¯ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æœ€åˆã‹ã‚‰ã®æƒ…å ±ã‚’ã€Œè¨˜æ†¶ã€ã—ç¶šã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€N-gramãƒ¢ãƒ‡ãƒ«ã®å›ºå®šé•·ã®æ–‡è„ˆã®åˆ¶ç´„ã‚’ä¹—ã‚Šè¶Šãˆã€é•·è·é›¢ä¾å­˜é–¢ä¿‚ã‚’ã‚ã‚‹ç¨‹åº¦æ‰ãˆã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã—ãŸã€‚ã—ã‹ã—ã€å®Ÿéš›ã«ã¯ã€å‹¾é…æ¶ˆå¤±ãƒ»çˆ†ç™ºå•é¡Œã«ã‚ˆã‚Šã€éå¸¸ã«é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®åˆã‚ã®æƒ…å ±ã‚’ä¿æŒã—ç¶šã‘ã‚‹ã“ã¨ã¯å›°é›£ã§ã—ãŸï¼ˆã“ã®å•é¡Œã¯LSTMã‚„GRUã«ã‚ˆã£ã¦ç·©å’Œã•ã‚Œã¾ã—ãŸï¼‰ã€‚

ã¾ã¨ã‚ã‚‹ã¨ã€RNNã¯ã€Œè¨˜æ†¶ã€ã‚’æŒã¤ã“ã¨ã§N-gramãƒ¢ãƒ‡ãƒ«ã®èª²é¡Œã‚’å…‹æœã—ã€ã‚ˆã‚Šè‡ªç„¶ãªè¨€èªå‡¦ç†ã‚’å¯èƒ½ã«ã—ã¾ã—ãŸãŒã€ãã®é€æ¬¡çš„ãªæ€§è³ªã‚†ãˆã«è¨ˆç®—åŠ¹ç‡ã¨éå¸¸ã«é•·ã„æ–‡è„ˆã®æ‰±ã„ã«é™ç•ŒãŒã‚ã‚Šã¾ã—ãŸã€‚ã“ã‚ŒãŒã€Attentionæ©Ÿæ§‹ã‚„Transformerã¨ã„ã£ãŸã€ã‚ˆã‚Šä¸¦åˆ—åŒ–ã«é©ã—ãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒç™»å ´ã™ã‚‹èƒŒæ™¯ã¨ãªã‚Šã¾ã™ã€‚
-->


---
transition: slide-up
level: 2
---

# Language Model with RNNs

Recurrent Neural Networks (RNNs)

<v-switch>
      <template #1>
        <img src="./image/1-min.png" alt="Image 1" style="max-width: 650px;">
      </template>
      <template #2>
        <img src="./image/2-min.png" alt="Image 2" style="max-width: 650px;">
      </template>
      <template #3>
        <img src="./image/3-min.png" alt="Image 3" style="max-width: 650px;">
      </template>
      <template #4>
        <img src="./image/4-min.png" alt="Image 4" style="max-width: 650px;">
      </template>
      <template #5>
        <img src="./image/5-min.png" alt="Image 5" style="max-width: 650px;">
      </template>
      <template #6>
        <img src="./image/6-min.png" alt="Image 6" style="max-width: 650px;">
      </template>
      <template #7>
        <img src="./image/7-min.png" alt="Image 7" style="max-width: 650px;">
      </template>
      <template #8>
        <img src="./image/8-min.png" alt="Image 8" style="max-width: 650px;">
      </template>
    </v-switch>

---
transition: slide-up
level: 2
---

# Language Model with RNNs

Recurrent Neural Networks (RNNs)

<v-clicks depth="2">

- Advantages of RNN LMs
    - **Contextual Understanding**: RNNs can capture longer dependencies in the text when making predictions.
    - **Dynamic Computation**: RNNs can handle input sequences of varying lengths without needing to predefine a fixed size.
- Disadvantages of RNN LMs: In a encoder-decoder model with RNNs LMs, the hidden state of the last time step represents absolutely everything about the meaning of the source text.
    - Sequential nature of RNNs means recurrent computation is slow.
    - Information at the beginning of the sentence, especially for long sentences, may not be equally well represented in the context vector.

</v-clicks>


<div class="flex justify-center">
  <img src="./image/rnn_problem.png" alt="ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³" width="500" />
</div>



<!--



**RNNã‚’ç”¨ã„ãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã®åˆ©ç‚¹ (Advantages of RNN LMs)**

1.  **æ–‡è„ˆç†è§£ (Contextual Understanding):**
    * **RNNsã¯ã€äºˆæ¸¬ã‚’è¡Œã†éš›ã«ãƒ†ã‚­ã‚¹ãƒˆä¸­ã®ã‚ˆã‚Šé•·ã„ä¾å­˜é–¢ä¿‚ã‚’æ‰ãˆã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚**
        ã“ã‚Œã¯N-gramãƒ¢ãƒ‡ãƒ«ã¨ã®æœ€å¤§ã®å·®åˆ¥åŒ–ãƒã‚¤ãƒ³ãƒˆã§ã™ã€‚RNNã¯ã€éš ã‚ŒçŠ¶æ…‹ï¼ˆ$h_t$ï¼‰ã‚’é€šã˜ã¦éå»ã®æƒ…å ±ã‚’ã€Œè¨˜æ†¶ã€ã—ã€ãã‚Œã‚’ç¾åœ¨ã®å˜èªã®äºˆæ¸¬ã«åˆ©ç”¨ã—ã¾ã™ã€‚ç†è«–ä¸Šã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ä»»æ„ã®é•·ã•ã®æƒ…å ±ã‚’ä¿æŒã§ãã‚‹ãŸã‚ã€æ–‡ã®æœ€åˆã«å‡ºã¦ããŸå˜èªãŒæ–‡ã®å¾ŒåŠã®å˜èªã®æ„å‘³ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ã‚ˆã†ãªã€Œé•·è·é›¢ä¾å­˜é–¢ä¿‚ã€ã‚’N-gramã‚ˆã‚Šã‚‚ã¯ã‚‹ã‹ã«ã†ã¾ãæ‰ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚ˆã‚Šæ–‡è„ˆã«å³ã—ãŸã€è‡ªç„¶ã§ä¸€è²«æ€§ã®ã‚ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ãŸã‚Šã€è¤‡é›‘ãªæ„å‘³ã‚’æŒã¤æ–‡ç« ã‚’ç†è§£ã—ãŸã‚Šã™ã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

2.  **å‹•çš„ãªè¨ˆç®— (Dynamic Computation):**
    * **RNNsã¯ã€å›ºå®šã‚µã‚¤ã‚ºã‚’äº‹å‰ã«å®šç¾©ã™ã‚‹ã“ã¨ãªãã€å¯å¤‰é•·ã®å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å‡¦ç†ã§ãã‚‹ã€‚**
        N-gramãƒ¢ãƒ‡ãƒ«ã¯ã€ç‰¹å®šã® $N$ ã®å€¤ï¼ˆä¾‹ï¼š3-gramãªã‚‰3å˜èªï¼‰ã«å›ºå®šã•ã‚Œã¾ã™ã€‚ã—ã‹ã—ã€RNNã¯é€æ¬¡çš„ã«å‡¦ç†ã‚’é€²ã‚ã‚‹ãŸã‚ã€å…¥åŠ›ã•ã‚Œã‚‹ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®é•·ã•ãŒã„ãã‚‰ã§ã‚ã£ã¦ã‚‚ã€ãã®é•·ã•ã«å¿œã˜ã¦è¨ˆç®—ã‚’ç¶™ç¶šã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æ–‡ç« ã®é•·ã•ãŒä¸æƒã„ãªå®Ÿéš›ã®è¨€èªãƒ‡ãƒ¼ã‚¿ã«æŸ”è»Ÿã«å¯¾å¿œã§ãã€æ©Ÿæ¢°ç¿»è¨³ã®ã‚ˆã†ã«ç•°ãªã‚‹é•·ã•ã®æ–‡ã‚’æ‰±ã†ã‚¿ã‚¹ã‚¯ã«ã‚‚è‡ªç„¶ã«é©ç”¨ã§ãã¾ã™ã€‚


**RNNã‚’ç”¨ã„ãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã®æ¬ ç‚¹ (Disadvantages of RNN LMs)**

1.  **æƒ…å ±ã®åœ§ç¸®ã¨ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ (Information Compression and Bottleneck):**
    * **RNNã‚’ç”¨ã„ãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ã€æœ€å¾Œã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®éš ã‚ŒçŠ¶æ…‹ãŒã‚½ãƒ¼ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã®ã‚ã‚‰ã‚†ã‚‹æ„å‘³ã‚’å®Œå…¨ã«è¡¨ç¾ã™ã‚‹ã“ã¨ã«ãªã‚‹ã€‚**
        ã“ã‚Œã¯ã€RNNãƒ™ãƒ¼ã‚¹ã®Seq2Seqãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹**ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ«ï¼ˆã¾ãŸã¯å›ºå®šã‚µã‚¤ã‚ºè¡¨ç¾ï¼‰ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯å•é¡Œ**ã‚’æŒ‡ã—ã¦ã„ã¾ã™ã€‚ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¯å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¨ä½“ã‚’èª­ã¿è¾¼ã¿ã€ãã®æƒ…å ±ã‚’å˜ä¸€ã®éš ã‚ŒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆé€šå¸¸ã¯æœ€å¾Œã®éš ã‚ŒçŠ¶æ…‹ï¼‰ã«ã€Œåœ§ç¸®ã€ã—ã¾ã™ã€‚ã“ã®å˜ä¸€ãƒ™ã‚¯ãƒˆãƒ«ãŒã€ãƒ‡ã‚³ãƒ¼ãƒ€ãŒã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã§ãã‚‹å”¯ä¸€ã®æƒ…å ±æºã¨ãªã‚Šã¾ã™ã€‚
        * **å•é¡Œç‚¹:** ç‰¹ã«é•·ã„å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å ´åˆã€ã™ã¹ã¦ã®é‡è¦ãªæƒ…å ±ã‚’ä¸€ã¤ã®å›ºå®šã‚µã‚¤ã‚ºã®ãƒ™ã‚¯ãƒˆãƒ«ã«å‡ç¸®ã™ã‚‹ã“ã¨ã¯éå¸¸ã«å›°é›£ã§ã™ã€‚æƒ…å ±ãŒå¤±ã‚ã‚ŒãŸã‚Šã€é‡è¦ãªæƒ…å ±ãŒè–„ã¾ã£ãŸã‚Šã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã¯ã€é•·ã„æ–‡ã‚’ç†è§£ã—ãŸã‚Šã€è©³ç´°ãªæƒ…å ±ã‚’ä¿æŒã—ãŸã‚Šã™ã‚‹èƒ½åŠ›ã‚’åˆ¶é™ã—ã¾ã™ã€‚

2.  **é€æ¬¡çš„ãªæ€§è³ªã«ã‚ˆã‚‹è¨ˆç®—ã®é…ã• (Slow Computation due to Sequential Nature):**
    * **RNNã®é€æ¬¡çš„ãªæ€§è³ªã¯ã€å†å¸°çš„ãªè¨ˆç®—ãŒé…ã„ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚**
        RNNã®å„ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®è¨ˆç®—ã¯ã€å‰ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®è¨ˆç®—çµæœã«å¼·ãä¾å­˜ã—ã¾ã™ã€‚ã¤ã¾ã‚Šã€$h_t$ ã‚’è¨ˆç®—ã™ã‚‹ã«ã¯ $h_{t-1}$ ãŒå¿…è¦ã§ã‚ã‚Šã€$h_{t-1}$ ã‚’è¨ˆç®—ã™ã‚‹ã«ã¯ $h_{t-2}$ ãŒå¿…è¦â€¦ã¨ã„ã†ã‚ˆã†ã«ã€å‡¦ç†ã‚’å…ˆé ­ã‹ã‚‰é †ã«ã—ã‹é€²ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã€‚ã“ã®**æœ¬è³ªçš„ãªé€æ¬¡æ€§**ã¯ã€æœ€æ–°ã®GPUãªã©ã®ä¸¦åˆ—è¨ˆç®—èƒ½åŠ›ã‚’ååˆ†ã«æ´»ç”¨ã™ã‚‹ã“ã¨ã‚’å¦¨ã’ã€ç‰¹ã«éå¸¸ã«é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚„å¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ‰±ã†éš›ã®è¨“ç·´ã¨æ¨è«–ã®é€Ÿåº¦ã‚’è‘—ã—ãä½ä¸‹ã•ã›ã¾ã™ã€‚

3.  **é•·è·é›¢ä¾å­˜é–¢ä¿‚ã®å­¦ç¿’ã®å›°é›£ã• (Difficulty in learning very long-range dependencies):**
    * **ç‰¹ã«é•·ã„æ–‡ç« ã®å ´åˆã€æ–‡ã®å†’é ­ã®æƒ…å ±ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ«ã«åŒç­‰ã«ã†ã¾ãè¡¨ç¾ã•ã‚Œãªã„å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚**
        ç†è«–ä¸Šã¯RNNãŒé•·è·é›¢ä¾å­˜é–¢ä¿‚ã‚’æ‰ãˆã‚‰ã‚Œã¾ã™ãŒã€å®Ÿéš›ã«ã¯**å‹¾é…æ¶ˆå¤±å•é¡Œ (vanishing gradient problem)** ã‚„**å‹¾é…çˆ†ç™ºå•é¡Œ (exploding gradient problem)** ã¨ã„ã†èª²é¡Œã«ç›´é¢ã—ã¾ã™ã€‚
        * **å‹¾é…æ¶ˆå¤±:** å¤šãã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’é¡ã‚‹ã«ã¤ã‚Œã¦ã€å‹¾é…ãŒéå¸¸ã«å°ã•ããªã‚Šã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®åˆæœŸå±¤ã®é‡ã¿ãŒã»ã¨ã‚“ã©æ›´æ–°ã•ã‚Œãªããªã‚Šã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã¯ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æœ€åˆã®éƒ¨åˆ†ã®æƒ…å ±ã‚’ã€Œå¿˜ã‚Œã¦ã€ã—ã¾ã„ã€éå¸¸ã«é•·ã„è·é›¢ã®ä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ãŒå›°é›£ã«ãªã‚Šã¾ã™ã€‚
        * **å‹¾é…çˆ†ç™º:** é€†ã«å‹¾é…ãŒéå¸¸ã«å¤§ãããªã‚Šã€è¨“ç·´ãŒä¸å®‰å®šã«ãªã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ï¼ˆã“ã¡ã‚‰ã¯å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã§å¯¾å‡¦å¯èƒ½ï¼‰ã€‚
        ã“ã‚Œã‚‰ã®å•é¡Œã«ã‚ˆã‚Šã€RNNã¯ã€Œç†è«–ä¸Šã¯ã€é•·è·é›¢ä¾å­˜é–¢ä¿‚ã‚’æ‰±ãˆã‚‹ã‚‚ã®ã®ã€**å®Ÿéš›ã«ã¯**æ•°ã‚¹ãƒ†ãƒƒãƒ—å…ˆã¾ã§ã®ä¾å­˜é–¢ä¿‚ã—ã‹åŠ¹æœçš„ã«å­¦ç¿’ã§ããªã„ã“ã¨ãŒå¤šã‹ã£ãŸã®ã§ã™ã€‚LSTM (Long Short-Term Memory) ã‚„ GRU (Gated Recurrent Unit) ã¨ã„ã£ãŸæ”¹è‰¯ç‰ˆRNNãŒç™»å ´ã—ã€ã“ã‚Œã‚‰ã®å•é¡Œã‚’ç·©å’Œã—ã¾ã—ãŸãŒã€æ ¹æœ¬çš„ãªè§£æ±ºã«ã¯è‡³ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚

ã“ã‚Œã‚‰ã®RNNã®æ¬ ç‚¹ã€ç‰¹ã«ä¸¦åˆ—åŒ–ã®å›°é›£ã•ã¨é•·è·é›¢ä¾å­˜é–¢ä¿‚ã®å­¦ç¿’é™ç•ŒãŒã€Attentionæ©Ÿæ§‹ã®å°å…¥ã€ãã—ã¦æœ€çµ‚çš„ã«Transformerãƒ¢ãƒ‡ãƒ«ã¸ã¨ç¹‹ãŒã‚‹å¤§ããªå‹•æ©Ÿã¨ãªã‚Šã¾ã—ãŸã€‚
-->


---
transition: slide-up
level: 2
---

# The Principle of Transformer

Attention Mechanism

<div class="flex justify-center">
  <img src="./image/attention_example.png" alt="ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³" width="500" />
</div>

<v-clicks depth="2">

- Key vectors are generated using a weight matrix 
    - $$k_i = W_K \cdot h_i, \quad \text{for } i = 1 \text{ to } m$$
- Query $q_t$ is generated using another weight matrix
    -  $$q_t = W_Q \cdot s_t$$
</v-clicks>

<!--
Attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¯ã€ã“ã‚Œã‚‰ã®èª²é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«å°å…¥ã•ã‚Œã¾ã—ãŸã€‚ãã®åŸºæœ¬çš„ãªè€ƒãˆæ–¹ã¯ã€äººé–“ãŒä½•ã‹ã‚’ç†è§£ã™ã‚‹éš›ã«ã€æ–‡å…¨ä½“ã‚’æ¼«ç„¶ã¨èª­ã‚€ã®ã§ã¯ãªãã€é‡è¦ãªéƒ¨åˆ†ã«ã€Œæ³¨æ„ã‚’å‘ã‘ã‚‹ã€ã¨ã„ã†èªçŸ¥ãƒ—ãƒ­ã‚»ã‚¹ã‚’æ¨¡å€£ã™ã‚‹ã“ã¨ã§ã™ã€‚

å…·ä½“çš„ãªä»•çµ„ã¿ã¯ã€ä¸»ã«ã‚¯ã‚¨ãƒªï¼ˆQueryï¼‰ã€ã‚­ãƒ¼ï¼ˆKeyï¼‰ã€ãƒãƒªãƒ¥ãƒ¼ï¼ˆValueï¼‰ ã®3ã¤ã®æ¦‚å¿µã‚’ä½¿ã£ã¦èª¬æ˜ã•ã‚Œã¾ã™ã€‚

-->


---
transition: slide-up
level: 2
---

# The Principle of Transformer

Attention Mechanism

<div class="flex justify-center">
  <img src="./image/attention_example.png" alt="ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³" width="500" />
</div>

<v-clicks depth="2">

- The attention score $a_i^t$ between each key $k_i$ and the query $q_t$ is computed
    - $$a_i^t = k_i^T \cdot q_t$$
- $Softmax([a_1,...,a_m])$: Normalize with a softmax to create a vector of weights
</v-clicks>

<!--
ãƒ‡ã‚³ãƒ¼ãƒ€ãŒç¾åœ¨ã®å˜èªã‚’ç”Ÿæˆã—ã‚ˆã†ã¨ã™ã‚‹ã¨ãã€ç¾åœ¨ã®çŠ¶æ…‹ã‚’è¡¨ã™ã€Œã‚¯ã‚¨ãƒªã€ã¨ã€å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®å„å˜èªã‚’è¡¨ã™ã€Œã‚­ãƒ¼ã€ã¨ã®é¡ä¼¼åº¦ï¼ˆé–¢é€£åº¦ï¼‰ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
ã“ã®é¡ä¼¼åº¦ã¯ã€é€šå¸¸ã€å†…ç©ã‚„åŠ æ³•ãªã©ã®é–¢æ•°ã‚’ä½¿ã£ã¦è¨ˆç®—ã•ã‚Œã€ã€Œã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã€ã¨å‘¼ã°ã‚Œã¾ã™ã€‚ã‚¹ã‚³ã‚¢ãŒé«˜ã„ã»ã©ã€ãã®å˜èªã®é–¢é€£æ€§ãŒé«˜ã„ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚

è¨ˆç®—ã•ã‚ŒãŸã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã¯ã€Softmaxé–¢æ•°ã‚’é€šã—ã¦æ­£è¦åŒ–ã•ã‚Œã€åˆè¨ˆãŒ1ã«ãªã‚‹ã‚ˆã†ãªã€Œã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿ï¼ˆã¾ãŸã¯ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³åˆ†å¸ƒï¼‰ã€ã«å¤‰æ›ã•ã‚Œã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å„å˜èªãŒã©ã‚Œãã‚‰ã„é‡è¦ã§ã‚ã‚‹ã‹ã‚’ç¤ºã™ç¢ºç‡çš„ãªé‡ã¿ãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚é‡ã¿ãŒå¤§ãã„ã»ã©ã€ãã®å˜èªã«å¼·ãã€Œæ³¨ç›®ã€ã—ã¦ã„ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚
-->

---
transition: slide-up
level: 2
---

# The Principle of Transformer

Attention Mechanism

<div class="flex justify-center">
  <img src="./image/attention_example.png" alt="ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³" width="500" />
</div>

<v-clicks depth="2">

- Create a fixed-length vector by taking a weighted sum of all the encoder hidden states
    - $$c_t=a_{1}^t h_1+...+a_{m}^t h_m$$

</v-clicks>


<!--
- å„å˜èªã«å¯¾å¿œã™ã‚‹ã€Œãƒãƒªãƒ¥ãƒ¼ã€ï¼ˆé€šå¸¸ã¯å˜èªã®è¡¨ç¾ãƒ™ã‚¯ãƒˆãƒ«è‡ªä½“ï¼‰ã‚’ã€ã‚¹ãƒ†ãƒƒãƒ—2ã§å¾—ã‚‰ã‚ŒãŸã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿ã§é‡ã¿ä»˜ã‘ã—ã¦åˆè¨ˆã—ã¾ã™ã€‚
- ã“ã‚Œã«ã‚ˆã‚Šã€ç¾åœ¨ã®ã‚¿ã‚¹ã‚¯ï¼ˆä¾‹ï¼šæ¬¡ã®å˜èªç”Ÿæˆï¼‰ã«ã¨ã£ã¦æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„æƒ…å ±ãŒå¼·èª¿ã•ã‚ŒãŸã€Œæ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ã€ãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚
-->


---
transition: slide-up
level: 2
---

# The Principle of Transformer

Self-Attention

<div grid="~ cols-2 gap-4 items-start">

<div>

<v-clicks depth="2">

- Create three vectors query, key, and value from each of the embeddings of each word
- $$W^Q, W^K, W^V \in R^{d_{model} \times d_k}$$

- Calculate attention scores between each word of the input sentence against a specific word

- $$Attention(\mathbf{Q},\mathbf{K},\mathbf{V})=softmax(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}})V$$
</v-clicks>

</div>

<div class="flex justify-center">
  <img src="./image/key_query.png" alt="ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³" width="300" />
</div>

</div>



---
transition: slide-up
level: 2
---

# The Principle of Transformer

Attention Mechanism

<div class="flex justify-center">
  <img src="./image/self-attention-matrix-calculation-2.png" alt="ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³" width="500" />
</div>

<v-clicks depth="2">

- Scale the attention scores, take the softmax, and then multiply the result by $V$ resulting in a matrix of shape $N \times d$: a vector embedding representation for each token in the input.

</v-clicks>

<!--
ã“ã®æ–°ã—ã„æ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ã¯ã€å˜ä¸€ã®å›ºå®šãƒ™ã‚¯ãƒˆãƒ«ã§ã¯ãªãã€ã‚¿ã‚¹ã‚¯ã‚„ç¾åœ¨ã®å‡ºåŠ›ã®çŠ¶æ³ã«å¿œã˜ã¦ã€å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ã©ã“ã«ã€Œæ³¨ç›®ã€ã™ã¹ãã‹ã‚’å‹•çš„ã«å¤‰åŒ–ã•ã›ãªãŒã‚‰ç”Ÿæˆã•ã‚Œã‚‹ç‚¹ãŒé‡è¦ã§ã™ã€‚
-->


---
transition: slide-up
level: 2
---

# The Principle of Transformer

Attention Mechanism

<div class="flex justify-center">
  <img src="./image/general_scheme-min.png" alt="ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³" width="600" />
</div>


<!--
ã“ã®æ–°ã—ã„æ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ã¯ã€å˜ä¸€ã®å›ºå®šãƒ™ã‚¯ãƒˆãƒ«ã§ã¯ãªãã€ã‚¿ã‚¹ã‚¯ã‚„ç¾åœ¨ã®å‡ºåŠ›ã®çŠ¶æ³ã«å¿œã˜ã¦ã€å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ã©ã“ã«ã€Œæ³¨ç›®ã€ã™ã¹ãã‹ã‚’å‹•çš„ã«å¤‰åŒ–ã•ã›ãªãŒã‚‰ç”Ÿæˆã•ã‚Œã‚‹ç‚¹ãŒé‡è¦ã§ã™ã€‚
-->





---
transition: slide-up
level: 2
---

# The Principle of Transformer

Self-Attention

<div grid="~ cols-2 gap-4">
<div>

- å˜ä¸€ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã§ã€å„å˜èªãŒãã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®ä»–ã®ã™ã¹ã¦ã®å˜èªã¨ã©ã®ã‚ˆã†ã«é–¢é€£ã—ã¦ã„ã‚‹ã‹ã‚’è©•ä¾¡ã—ã€ãã‚Œã«åŸºã¥ã„ã¦å˜èªã®è¡¨ç¾ã‚’å­¦ç¿’ã™ã‚‹
   - é•·è·é›¢ä¾å­˜é–¢ä¿‚ã‚’åæ˜ : å„å˜èªã¯ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®ä»–ã®ã™ã¹ã¦ã®å˜èªã¨ç›´æ¥é–¢é€£åº¦ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã€å˜èªé–“ã®è·é›¢ãŒé›¢ã‚Œã¦ã„ã¦ã‚‚ã€ãã®é–¢ä¿‚æ€§ã‚’åŠ¹ç‡çš„ã«å­¦ç¿’
   - ä¸¦åˆ—è¨ˆç®—: å„å˜èªã«å¯¾ã™ã‚‹Self-Attentionã®è¨ˆç®—ã¯ã€ä»–ã®å˜èªã«å¯¾ã™ã‚‹è¨ˆç®—ã¨ã¯ç‹¬ç«‹ã—ã¦è¡Œãˆã‚‹
</div>

<div>

<video controls width="700" style="display: block; margin-left: auto; margin-right: auto;">
  <source src="./image/encoder_self_attention.mp4" type="video/mp4">
</video>
</div>
</div>

<!--
Self-Attentionã¯ã€Attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®ä¸€èˆ¬çš„ãªæ¦‚å¿µï¼ˆã‚¯ã‚¨ãƒªã€ã‚­ãƒ¼ã€ãƒãƒªãƒ¥ãƒ¼ï¼‰ã‚’ã€å˜ä¸€ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®è¦ç´ é–“ã§é©ç”¨ã™ã‚‹ã“ã¨ã§å®Ÿç¾ã•ã‚Œã¾ã™ã€‚

ã‚ã‚‹å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ $X = (x_1, x_2, \ldots, x_n)$ ãŒã‚ã‚‹ã¨ãã€å„å˜èª $x_i$ ã«ã¤ã„ã¦ã€ä»¥ä¸‹ã®3ã¤ã®ãƒ™ã‚¯ãƒˆãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚

1.  **ã‚¯ã‚¨ãƒª (Query: $Q$)**: ç¾åœ¨æ³¨ç›®ã—ã¦ã„ã‚‹å˜èªãŒã€ä»–ã®å˜èªã«å¯¾ã—ã¦ã€Œä½•ã‚’æ¢ã—ã¦ã„ã‚‹ã‹ã€ã‚’è¡¨ç¾ã—ã¾ã™ã€‚
2.  **ã‚­ãƒ¼ (Key: $K$)**: ä»–ã®å˜èªãŒã€Œã©ã®ã‚ˆã†ãªæƒ…å ±ã‚’æŒã£ã¦ã„ã‚‹ã‹ã€ã‚’è¡¨ç¾ã—ã¾ã™ã€‚
3.  **ãƒãƒªãƒ¥ãƒ¼ (Value: $V$)**: ä»–ã®å˜èªãŒå®Ÿéš›ã«æä¾›ã™ã‚‹ã€Œæƒ…å ±ãã®ã‚‚ã®ã€ã‚’è¡¨ç¾ã—ã¾ã™ã€‚

ã“ã‚Œã‚‰ã® $Q, K, V$ ã¯ã€å…¥åŠ›å˜èªã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ« $x_i$ ã‚’ãã‚Œãã‚Œç•°ãªã‚‹å­¦ç¿’å¯èƒ½ãªé‡ã¿è¡Œåˆ— $W_Q, W_K, W_V$ ã§ç·šå½¢å¤‰æ›ã™ã‚‹ã“ã¨ã§ç”Ÿæˆã•ã‚Œã¾ã™ã€‚

$Q_i = x_i W_Q$
$K_i = x_i W_K$
$V_i = x_i W_V$

Self-Attentionã®è¨ˆç®—ã¯ã€ä¸»ã«ä»¥ä¸‹ã®ã‚¹ãƒ†ãƒƒãƒ—ã§è¡Œã‚ã‚Œã¾ã™ã€‚

1.  **ã‚¹ã‚³ã‚¢ã®è¨ˆç®—**:
    * å„å˜èªã®ã‚¯ã‚¨ãƒª $Q_i$ ã¨ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®ä»–ã®ã™ã¹ã¦ã®å˜èªã®ã‚­ãƒ¼ $K_j$ ã¨ã®å†…ç©ï¼ˆãƒ‰ãƒƒãƒˆç©ï¼‰ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
    * ã“ã®å†…ç©ãŒã€å˜èª $x_i$ ãŒå˜èª $x_j$ ã«ã©ã‚Œãã‚‰ã„ã€Œæ³¨ç›®ã€ã™ã¹ãã‹ã®é–¢é€£åº¦ã‚¹ã‚³ã‚¢ã¨ãªã‚Šã¾ã™ã€‚
    * ã‚¹ã‚±ãƒ¼ãƒ«ã•ã‚ŒãŸãƒ‰ãƒƒãƒˆç©ï¼ˆé€šå¸¸ã¯ã‚­ãƒ¼ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒã®å¹³æ–¹æ ¹ã§å‰²ã‚‹ï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒä¸€èˆ¬çš„ã§ã™ã€‚
    $Score(Q_i, K_j) = \frac{Q_i \cdot K_j}{\sqrt{d_k}}$ ï¼ˆ$d_k$ ã¯ã‚­ãƒ¼ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒï¼‰

2.  **é‡ã¿ã®æ­£è¦åŒ–**:
    * è¨ˆç®—ã•ã‚ŒãŸã™ã¹ã¦ã®ã‚¹ã‚³ã‚¢ã«å¯¾ã—ã€Softmaxé–¢æ•°ã‚’é©ç”¨ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å„å˜èª $x_j$ ã¸ã®æ³¨ç›®åº¦ã‚’è¡¨ã™**ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿**ï¼ˆåˆè¨ˆãŒ1ã«ãªã‚‹ç¢ºç‡åˆ†å¸ƒï¼‰ãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚
    $Attention\_Weight_{ij} = \text{Softmax}(Score(Q_i, K_j))$

3.  **é‡ã¿ä»˜ã‘ã•ã‚ŒãŸãƒãƒªãƒ¥ãƒ¼ã®åˆè¨ˆ**:
    * å„å˜èª $x_j$ ã«å¯¾å¿œã™ã‚‹ãƒãƒªãƒ¥ãƒ¼ $V_j$ ã‚’ã€è¨ˆç®—ã•ã‚ŒãŸã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿ $Attention\_Weight_{ij}$ ã§é‡ã¿ä»˜ã‘ã—ã€ãã‚Œã‚‰ã‚’ã™ã¹ã¦åˆè¨ˆã—ã¾ã™ã€‚
    * ã“ã‚ŒãŒã€å˜èª $x_i$ ã®æ–°ã—ã„è¡¨ç¾ï¼ˆæ–‡è„ˆã‚’è€ƒæ…®ã—ãŸè¡¨ç¾ï¼‰ã§ã‚ã‚‹**å‡ºåŠ›ãƒ™ã‚¯ãƒˆãƒ« $Z_i$** ã¨ãªã‚Šã¾ã™ã€‚
    $Z_i = \sum_{j=1}^{n} Attention\_Weight_{ij} \cdot V_j$

ã“ã®ãƒ—ãƒ­ã‚»ã‚¹ã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®å„å˜èª $x_i$ ã”ã¨ã«è¡Œã‚ã‚Œã¾ã™ã€‚çµæœã¨ã—ã¦ã€å„å˜èªã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®ä»–ã®å˜èªã™ã¹ã¦ã¨ã®é–¢ä¿‚æ€§ã‚’è€ƒæ…®ã—ãŸã€ã‚ˆã‚Šè±Šã‹ãªæ–‡è„ˆçš„ãªè¡¨ç¾ã‚’ç²å¾—ã—ã¾ã™ã€‚
-->



---
transition: slide-up
level: 2
---

# The Principle of Transformer

Self-Attention

<div grid="~ cols-2 gap-4 items-start">

<div>

<v-clicks depth="2">

- Each attention score quantifies how relevant every other word (or token) in the sequence is to a given word.

- Self-attention processes all parts of the input can be parallelized. 
    - The input embedding of $N$ tokens can be packed into a single matrix:
    - $$X \in R^{N \times d}$$

</v-clicks>

</div>

<div class="flex justify-center">
  <img src="./image/self-attention-score.png" alt="ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³" width="300" />
</div>

</div>

---
transition: slide-up
level: 2
---

# The Principle of Transformer

Components in Transformer Architecture


<div grid="~ cols-2 gap-4 items-start">

<div>

<v-clicks depth="2">

- **Multi-Head Attention**: Allows the model to jointly attend to information from different representation subspaces (e.g., syntactic, semantic, and discourse relationships).

- **Positional Encoding**: Injects some information about the order of the sequence into the model.

- **Add & Norm Layer**: Encourages training deeper models by ensuring that backpropagation through many layers does not result in vanishing or exploding gradients.

</v-clicks>
</div>

<div class="flex justify-center">
  <img src="./image/transformer_frame.png" alt="ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³" width="500" />
</div>

</div>




---
transition: slide-up
level: 2
---

# LLMs with Transformer


<div class="flex justify-center">
  <img src="./image/tr18.png" alt="ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³" width="800" />
</div>